{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPTIMIZED HYBRID MOVIE RECOMMENDATION SYSTEM (alpha=0.30)\n",
      "================================================================================\n",
      "\n",
      "Loading essential data...\n",
      "Loaded content-based evaluation metrics: RMSE=0.9647\n",
      "Loaded DNN evaluation metrics: RMSE=0.9064\n",
      "Loaded content-based recommendations for 16090 users\n",
      "Loaded collaborative filtering recommendations for 20000 users\n",
      "Loaded rating counts for 500 users\n",
      "Loaded minimal movie metadata for 7359 movies\n",
      "Data loading completed in 0.21s\n",
      "Found 16090 users with both content-based and collaborative recommendations\n",
      "\n",
      "Combining recommendations with optimized adaptive alpha values...\n",
      "Processed 1000/20000 users (5.0%)\n",
      "Processed 2000/20000 users (10.0%)\n",
      "Processed 3000/20000 users (15.0%)\n",
      "Processed 4000/20000 users (20.0%)\n",
      "Processed 5000/20000 users (25.0%)\n",
      "Processed 6000/20000 users (30.0%)\n",
      "Processed 7000/20000 users (35.0%)\n",
      "Processed 8000/20000 users (40.0%)\n",
      "Processed 9000/20000 users (45.0%)\n",
      "Processed 10000/20000 users (50.0%)\n",
      "Processed 11000/20000 users (55.0%)\n",
      "Processed 12000/20000 users (60.0%)\n",
      "Processed 13000/20000 users (65.0%)\n",
      "Processed 14000/20000 users (70.0%)\n",
      "Processed 15000/20000 users (75.0%)\n",
      "Processed 16000/20000 users (80.0%)\n",
      "Processed 17000/20000 users (85.0%)\n",
      "Processed 18000/20000 users (90.0%)\n",
      "Processed 19000/20000 users (95.0%)\n",
      "Processed 20000/20000 users (100.0%)\n",
      "\n",
      "Adaptive Alpha Statistics:\n",
      "Average alpha: 0.2067\n",
      "Min alpha: 0.2000, Max alpha: 0.7722\n",
      "\n",
      "Alpha by user rating count:\n",
      "  <=25 ratings: 19567 users, avg alpha = 0.2000\n",
      "  26-50 ratings: 115 users, avg alpha = 0.3000\n",
      "  51-150 ratings: 188 users, avg alpha = 0.5000\n",
      "  151-200 ratings: 34 users, avg alpha = 0.5000\n",
      "  >200 ratings: 96 users, avg alpha = 0.7722\n",
      "Combined recommendations for 20000 users in 8.27s\n",
      "Saved combined recommendations to CSV with 200000 entries\n",
      "\n",
      "Evaluating hybrid recommendation system using pre-computed metrics...\n",
      "\n",
      "Hybrid model evaluation (with adaptive alpha):\n",
      "RMSE: 0.9335\n",
      "MAE: 0.7204\n",
      "\n",
      "Model Performance Comparison:\n",
      "+-------------------+--------+--------------------+-------------+\n",
      "| Model             | RMSE   | MAE                | Predictions |\n",
      "+-------------------+--------+--------------------+-------------+\n",
      "| Content-Based     | 0.9647 | 0.7524811763493512 | 2894341.0   |\n",
      "| Collaborative     | 0.9064 | 0.6923             | 199494      |\n",
      "| Hybrid (Adaptive) | 0.9335 | 0.7204             | 2894341.0   |\n",
      "+-------------------+--------+--------------------+-------------+\n",
      "\n",
      "Hybrid Recommendation System completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# import torch\n",
    "import pickle\n",
    "import logging\n",
    "import argparse\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class HybridRecommender:\n",
    "    def __init__(self, content_model_path=\"./content-recommendations\", \n",
    "                 collab_model_path=\"./recommendations\", \n",
    "                 output_path=\"./hybrid_recommendations\", \n",
    "                 alpha=0.3):\n",
    "        \"\"\"\n",
    "        Initialize the hybrid recommender with paths to content-based and collaborative filtering models\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        content_model_path: str\n",
    "            Path to the directory containing content-based model files\n",
    "        collab_model_path: str\n",
    "            Path to the directory containing collaborative filtering model files\n",
    "        output_path: str\n",
    "            Path to save hybrid recommendation results\n",
    "        alpha: float\n",
    "            Weight for content-based recommendations (1-alpha for collaborative)\n",
    "        \"\"\"\n",
    "        self.content_model_path = content_model_path\n",
    "        self.collab_model_path = collab_model_path\n",
    "        self.output_path = output_path\n",
    "        self.alpha = alpha\n",
    "        self.data = {}  # Container for all loaded data\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"OPTIMIZED HYBRID MOVIE RECOMMENDATION SYSTEM (alpha={self.alpha:.2f})\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Optimized data loading - only load the necessary evaluation files and recommendations\n",
    "        \"\"\"\n",
    "        print(\"\\nLoading essential data...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Load content-based evaluation metrics\n",
    "        try:\n",
    "            content_eval_path = os.path.join(self.content_model_path, 'content_based_evaluation.csv')\n",
    "            if os.path.exists(content_eval_path):\n",
    "                content_eval_df = pd.read_csv(content_eval_path)\n",
    "                if not content_eval_df.empty:\n",
    "                    self.data['content_evaluation'] = {\n",
    "                        'rmse': content_eval_df.iloc[0]['rmse'],\n",
    "                        'mae': content_eval_df.iloc[0]['mae'] if 'mae' in content_eval_df.columns else None,\n",
    "                        'num_predictions': content_eval_df.iloc[0]['num_predictions'] if 'num_predictions' in content_eval_df.columns else None\n",
    "                    }\n",
    "                    print(f\"Loaded content-based evaluation metrics: RMSE={self.data['content_evaluation']['rmse']:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading content-based evaluation metrics: {str(e)}\")\n",
    "        \n",
    "        # Load DNN evaluation metrics\n",
    "        try:\n",
    "            dnn_eval_path = os.path.join(self.collab_model_path, 'dnn_evaluation.csv')\n",
    "            if os.path.exists(dnn_eval_path):\n",
    "                dnn_eval_df = pd.read_csv(dnn_eval_path)\n",
    "                if not dnn_eval_df.empty:\n",
    "                    self.data['dnn_evaluation'] = {\n",
    "                        'rmse': dnn_eval_df.iloc[0]['rmse'],\n",
    "                        'mae': dnn_eval_df.iloc[0]['mae'],\n",
    "                        'num_predictions': dnn_eval_df.iloc[0]['num_predictions']\n",
    "                    }\n",
    "                    print(f\"Loaded DNN evaluation metrics: RMSE={self.data['dnn_evaluation']['rmse']:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading DNN evaluation metrics: {str(e)}\")\n",
    "        \n",
    "        # Load content-based recommendations\n",
    "        try:\n",
    "            content_recs_path = os.path.join(self.content_model_path, 'content_based_recommendations.pkl')\n",
    "            if os.path.exists(content_recs_path):\n",
    "                with open(content_recs_path, 'rb') as f:\n",
    "                    self.data['content_recommendations'] = pickle.load(f)\n",
    "                print(f\"Loaded content-based recommendations for {len(self.data['content_recommendations'])} users\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading content-based recommendations: {str(e)}\")\n",
    "        \n",
    "        # Load collaborative filtering recommendations\n",
    "        try:\n",
    "            collab_recs_path = os.path.join(self.collab_model_path, 'dnn_recommendations.pkl')\n",
    "            if os.path.exists(collab_recs_path):\n",
    "                with open(collab_recs_path, 'rb') as f:\n",
    "                    self.data['collaborative_recommendations'] = pickle.load(f)\n",
    "                print(f\"Loaded collaborative filtering recommendations for {len(self.data['collaborative_recommendations'])} users\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading collaborative filtering recommendations: {str(e)}\")\n",
    "                \n",
    "        # Load minimal user rating data needed just for calculating adaptive alpha\n",
    "        try:\n",
    "            # We only need userId and rating count, so we'll parse this from the CSV directly\n",
    "            ratings_path = './processed/normalized_ratings.csv'\n",
    "            if os.path.exists(ratings_path):\n",
    "                # Read only the userId column to calculate rating counts\n",
    "                ratings_df = pd.read_csv(ratings_path, usecols=['userId'])\n",
    "                user_rating_counts = ratings_df['userId'].value_counts().reset_index()\n",
    "                user_rating_counts.columns = ['userId', 'rating_count']\n",
    "                self.data['user_rating_counts'] = user_rating_counts\n",
    "                print(f\"Loaded rating counts for {len(user_rating_counts)} users\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading user rating counts: {str(e)}\")\n",
    "            \n",
    "        # Optionally load minimal movie metadata (just for displaying recommendations)\n",
    "        try:\n",
    "            movie_features_path = './processed/processed_movie_features.csv'\n",
    "            if os.path.exists(movie_features_path):\n",
    "                # Read only the essential columns\n",
    "                self.data['movie_features'] = pd.read_csv(\n",
    "                    movie_features_path, \n",
    "                    usecols=['movieId', 'title']  # Only load the columns we need\n",
    "                )\n",
    "                print(f\"Loaded minimal movie metadata for {len(self.data['movie_features'])} movies\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading movie metadata: {str(e)}\")\n",
    "        \n",
    "        print(f\"Data loading completed in {time.time() - start_time:.2f}s\")\n",
    "        \n",
    "        # Get common users for both recommendation systems\n",
    "        self.common_users = set()\n",
    "        if 'content_recommendations' in self.data and 'collaborative_recommendations' in self.data:\n",
    "            self.common_users = set(self.data['content_recommendations'].keys()) & set(self.data['collaborative_recommendations'].keys())\n",
    "            print(f\"Found {len(self.common_users)} users with both content-based and collaborative recommendations\")\n",
    "        \n",
    "        return self.data\n",
    "    \n",
    "    def get_adaptive_alpha(self, user_id):\n",
    "        \"\"\"\n",
    "        Get optimized alpha value based on user's rating count\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        user_id: int\n",
    "            User ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Optimized alpha value\n",
    "        \"\"\"\n",
    "        # Get user's rating count\n",
    "        rating_count = 0\n",
    "        if 'user_rating_counts' in self.data:\n",
    "            user_data = self.data['user_rating_counts'][self.data['user_rating_counts']['userId'] == user_id]\n",
    "            if not user_data.empty:\n",
    "                rating_count = user_data.iloc[0]['rating_count']\n",
    "        \n",
    "        # Map rating count to appropriate alpha based on performance analysis\n",
    "        if rating_count <= 25:\n",
    "            return 0.2\n",
    "        elif rating_count <= 50:\n",
    "            return 0.3\n",
    "        elif rating_count <= 150:\n",
    "            return 0.5\n",
    "        elif rating_count <= 200:\n",
    "            return 0.5\n",
    "        else:  # > 200\n",
    "            return 0.772222\n",
    "    \n",
    "    def normalize_prediction(self, prediction):\n",
    "        \"\"\"\n",
    "        Normalize a prediction to the 0-1 range\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        prediction: float\n",
    "            Prediction value in the 0.5-5.0 range\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Normalized prediction in the 0-1 range\n",
    "        \"\"\"\n",
    "        # Normalize from rating scale [0.5, 5.0] to [0, 1]\n",
    "        return (prediction - 0.5) / 4.5\n",
    "    \n",
    "    def denormalize_prediction(self, normalized_prediction):\n",
    "        \"\"\"\n",
    "        Convert a normalized prediction back to the 0.5-5.0 range\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        normalized_prediction: float\n",
    "            Normalized prediction in the 0-1 range\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Prediction value in the 0.5-5.0 range\n",
    "        \"\"\"\n",
    "        # Convert from [0, 1] back to rating scale [0.5, 5.0]\n",
    "        return 0.5 + 4.5 * normalized_prediction\n",
    "    \n",
    "    def combine_recommendations(self, top_n=10, use_adaptive_alpha=True):\n",
    "        \"\"\"\n",
    "        Combine content-based and collaborative filtering recommendations with optimized weighting\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        top_n: int\n",
    "            Number of recommendations to generate per user\n",
    "        use_adaptive_alpha: bool\n",
    "            Whether to use adaptive alpha based on user rating count\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            User ID to list of (movie_id, score) tuples\n",
    "        \"\"\"\n",
    "        print(f\"\\nCombining recommendations with optimized adaptive alpha values...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get recommendations from both models\n",
    "        content_recs = self.data.get('content_recommendations', {})\n",
    "        collab_recs = self.data.get('collaborative_recommendations', {})\n",
    "        \n",
    "        if not content_recs:\n",
    "            print(\"Warning: No content-based recommendations available\")\n",
    "        \n",
    "        if not collab_recs:\n",
    "            print(\"Warning: No collaborative filtering recommendations available\")\n",
    "        \n",
    "        if not content_recs and not collab_recs:\n",
    "            print(\"Error: No recommendations available from either model\")\n",
    "            return {}\n",
    "        \n",
    "        # Combine recommendations\n",
    "        combined_recommendations = {}\n",
    "        alpha_stats = {'values': [], 'count_categories': {}}\n",
    "        \n",
    "        # Get all users from both recommendation sets\n",
    "        all_users = set(content_recs.keys()) | set(collab_recs.keys())\n",
    "        total_users = len(all_users)\n",
    "        \n",
    "        for i, user_id in enumerate(all_users):\n",
    "            # Get appropriate alpha value for this user\n",
    "            if use_adaptive_alpha:\n",
    "                alpha = self.get_adaptive_alpha(user_id)\n",
    "                # Track alpha statistics\n",
    "                alpha_stats['values'].append(alpha)\n",
    "                \n",
    "                # Get user's rating count\n",
    "                rating_count = 0\n",
    "                if 'user_rating_counts' in self.data:\n",
    "                    user_data = self.data['user_rating_counts'][self.data['user_rating_counts']['userId'] == user_id]\n",
    "                    if not user_data.empty:\n",
    "                        rating_count = user_data.iloc[0]['rating_count']\n",
    "                \n",
    "                # Categorize for statistics\n",
    "                count_category = \"<=25\" if rating_count <= 25 else \"26-50\" if rating_count <= 50 else \"51-150\" if rating_count <= 150 else \"151-200\" if rating_count <= 200 else \">200\"\n",
    "                if count_category in alpha_stats['count_categories']:\n",
    "                    alpha_stats['count_categories'][count_category]['count'] += 1\n",
    "                    alpha_stats['count_categories'][count_category]['alpha_sum'] += alpha\n",
    "                else:\n",
    "                    alpha_stats['count_categories'][count_category] = {'count': 1, 'alpha_sum': alpha}\n",
    "            else:\n",
    "                alpha = self.alpha\n",
    "            \n",
    "            # Initialize combined recommendations dictionary for this user\n",
    "            user_combined_recs = {}\n",
    "            \n",
    "            # Add content-based recommendations if available\n",
    "            if user_id in content_recs:\n",
    "                for movie_id, score in content_recs[user_id]:\n",
    "                    # Scores from content-based are already normalized (0-1), just store them\n",
    "                    user_combined_recs[movie_id] = {'content_score': score, 'content_available': True}\n",
    "            \n",
    "            # Add collaborative filtering recommendations if available\n",
    "            if user_id in collab_recs:\n",
    "                for movie_id, rating in collab_recs[user_id]:\n",
    "                    # Normalize the collaborative rating to 0-1 scale\n",
    "                    collab_score = self.normalize_prediction(rating)\n",
    "                    \n",
    "                    if movie_id in user_combined_recs:\n",
    "                        user_combined_recs[movie_id]['collab_score'] = collab_score\n",
    "                        user_combined_recs[movie_id]['collab_available'] = True\n",
    "                    else:\n",
    "                        user_combined_recs[movie_id] = {\n",
    "                            'collab_score': collab_score, \n",
    "                            'collab_available': True,\n",
    "                            'content_available': False\n",
    "                        }\n",
    "            \n",
    "            # Calculate final scores with proper normalization\n",
    "            final_recommendations = []\n",
    "            for movie_id, data in user_combined_recs.items():\n",
    "                # Check which models provided predictions\n",
    "                content_available = data.get('content_available', False)\n",
    "                collab_available = data.get('collab_available', False)\n",
    "                \n",
    "                if content_available and collab_available:\n",
    "                    # We have both predictions, use the weighted average\n",
    "                    content_score = data['content_score']\n",
    "                    collab_score = data['collab_score']\n",
    "                    combined_score = alpha * content_score + (1 - alpha) * collab_score\n",
    "                elif content_available:\n",
    "                    # Only content-based prediction available\n",
    "                    combined_score = data['content_score']\n",
    "                elif collab_available:\n",
    "                    # Only collaborative prediction available\n",
    "                    combined_score = data['collab_score']\n",
    "                \n",
    "                # Convert back to rating scale for storage\n",
    "                final_rating = self.denormalize_prediction(combined_score)\n",
    "                final_recommendations.append((movie_id, final_rating))\n",
    "            \n",
    "            # Sort by final score and limit to top_n\n",
    "            final_recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "            combined_recommendations[user_id] = final_recommendations[:top_n]\n",
    "            \n",
    "            # Log progress\n",
    "            if (i+1) % 1000 == 0 or (i+1) == total_users:\n",
    "                print(f\"Processed {i+1}/{total_users} users ({(i+1)/total_users*100:.1f}%)\")\n",
    "        \n",
    "        self.data['combined_recommendations'] = combined_recommendations\n",
    "        \n",
    "        # Print alpha statistics if using adaptive alpha\n",
    "        if use_adaptive_alpha and alpha_stats['values']:\n",
    "            print(\"\\nAdaptive Alpha Statistics:\")\n",
    "            print(f\"Average alpha: {np.mean(alpha_stats['values']):.4f}\")\n",
    "            print(f\"Min alpha: {min(alpha_stats['values']):.4f}, Max alpha: {max(alpha_stats['values']):.4f}\")\n",
    "            print(\"\\nAlpha by user rating count:\")\n",
    "            for category, stats in sorted(alpha_stats['count_categories'].items(), \n",
    "                                         key=lambda x: (int(x[0].replace('<=', '').replace('>', '').split('-')[0]) \n",
    "                                                       if x[0] not in ['>200'] else float('inf'))):\n",
    "                avg_alpha = stats['alpha_sum'] / stats['count']\n",
    "                print(f\"  {category} ratings: {stats['count']} users, avg alpha = {avg_alpha:.4f}\")\n",
    "            \n",
    "            # Save alpha statistics to a file\n",
    "            with open(os.path.join(self.output_path, 'alpha_stats.txt'), 'w') as f:\n",
    "                f.write(f\"Adaptive Alpha Statistics:\\n\")\n",
    "                f.write(f\"Average alpha: {np.mean(alpha_stats['values']):.4f}\\n\")\n",
    "                f.write(f\"Min alpha: {min(alpha_stats['values']):.4f}, Max alpha: {max(alpha_stats['values']):.4f}\\n\\n\")\n",
    "                f.write(\"Alpha by user rating count:\\n\")\n",
    "                for category, stats in sorted(alpha_stats['count_categories'].items(), \n",
    "                                            key=lambda x: (int(x[0].replace('<=', '').replace('>', '').split('-')[0]) \n",
    "                                                          if x[0] not in ['>200'] else float('inf'))):\n",
    "                    avg_alpha = stats['alpha_sum'] / stats['count']\n",
    "                    f.write(f\"  {category} ratings: {stats['count']} users, avg alpha = {avg_alpha:.4f}\\n\")\n",
    "        \n",
    "        print(f\"Combined recommendations for {len(combined_recommendations)} users in {time.time() - start_time:.2f}s\")\n",
    "        \n",
    "        # Save combined recommendations\n",
    "        with open(os.path.join(self.output_path, 'combined_recommendations.pkl'), 'wb') as f:\n",
    "            pickle.dump(combined_recommendations, f)\n",
    "        \n",
    "        # Also save in a more readable CSV format\n",
    "        recommendations_list = []\n",
    "        \n",
    "        for user_id, recs in combined_recommendations.items():\n",
    "            # Get user's alpha\n",
    "            if use_adaptive_alpha:\n",
    "                user_alpha = self.get_adaptive_alpha(user_id)\n",
    "            else:\n",
    "                user_alpha = self.alpha\n",
    "                \n",
    "            # Get user's rating count\n",
    "            rating_count = 0\n",
    "            if 'user_rating_counts' in self.data:\n",
    "                user_data = self.data['user_rating_counts'][self.data['user_rating_counts']['userId'] == user_id]\n",
    "                if not user_data.empty:\n",
    "                    rating_count = user_data.iloc[0]['rating_count']\n",
    "            \n",
    "            for rank, (movie_id, score) in enumerate(recs, 1):\n",
    "                movie_title = \"Unknown\"\n",
    "                if 'movie_features' in self.data:\n",
    "                    movie_row = self.data['movie_features'][self.data['movie_features']['movieId'] == movie_id]\n",
    "                    if not movie_row.empty and 'title' in movie_row.columns:\n",
    "                        movie_title = movie_row.iloc[0]['title']\n",
    "                \n",
    "                recommendations_list.append({\n",
    "                    'userId': user_id,\n",
    "                    'movieId': movie_id,\n",
    "                    'title': movie_title,\n",
    "                    'rank': rank,\n",
    "                    'score': score,\n",
    "                    'alpha': user_alpha,\n",
    "                    'rating_count': rating_count\n",
    "                })\n",
    "        \n",
    "        if recommendations_list:\n",
    "            recommendations_df = pd.DataFrame(recommendations_list)\n",
    "            recommendations_df.to_csv(os.path.join(self.output_path, 'combined_recommendations.csv'), index=False)\n",
    "            print(f\"Saved combined recommendations to CSV with {len(recommendations_df)} entries\")\n",
    "        \n",
    "        return combined_recommendations\n",
    "    \n",
    "    def evaluate(self, use_adaptive_alpha=True):\n",
    "        \"\"\"\n",
    "        Evaluate the hybrid recommendation system using the pre-computed metrics\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Evaluation metrics\n",
    "        \"\"\"\n",
    "        print(f\"\\nEvaluating hybrid recommendation system using pre-computed metrics...\")\n",
    "        \n",
    "        # Check if we have the pre-computed metrics\n",
    "        if 'content_evaluation' not in self.data or 'dnn_evaluation' not in self.data:\n",
    "            print(\"Cannot evaluate: Missing pre-computed evaluation metrics\")\n",
    "            return None\n",
    "        \n",
    "        content_rmse = self.data['content_evaluation']['rmse']\n",
    "        dnn_rmse = self.data['dnn_evaluation']['rmse']\n",
    "        \n",
    "        content_mae = self.data['content_evaluation'].get('mae', 0)\n",
    "        dnn_mae = self.data['dnn_evaluation'].get('mae', 0)\n",
    "        \n",
    "        # Calculate combined metrics based on alpha distribution\n",
    "        if use_adaptive_alpha and 'user_rating_counts' in self.data:\n",
    "            # Get rating count distribution\n",
    "            rating_counts = self.data['user_rating_counts']['rating_count'].values\n",
    "            \n",
    "            # Count users in each category\n",
    "            users_by_category = {\n",
    "                '<=25': sum(1 for count in rating_counts if count <= 25),\n",
    "                '26-50': sum(1 for count in rating_counts if 25 < count <= 50),\n",
    "                '51-150': sum(1 for count in rating_counts if 50 < count <= 150),\n",
    "                '151-200': sum(1 for count in rating_counts if 150 < count <= 200),\n",
    "                '>200': sum(1 for count in rating_counts if count > 200)\n",
    "            }\n",
    "            \n",
    "            total_users = len(rating_counts)\n",
    "            \n",
    "            # Calculate weighted RMSE and MAE\n",
    "            weighted_rmse = 0\n",
    "            weighted_mae = 0\n",
    "            \n",
    "            for category, count in users_by_category.items():\n",
    "                weight = count / total_users\n",
    "                \n",
    "                if category == '<=25':\n",
    "                    alpha = 0.2\n",
    "                elif category == '26-50':\n",
    "                    alpha = 0.3\n",
    "                elif category == '51-150':\n",
    "                    alpha = 0.5\n",
    "                elif category == '151-200':\n",
    "                    alpha = 0.5\n",
    "                else:  # >200\n",
    "                    alpha = 0.772222\n",
    "                \n",
    "                # Calculate weighted metrics\n",
    "                category_rmse = alpha * content_rmse + (1 - alpha) * dnn_rmse\n",
    "                category_mae = alpha * content_mae + (1 - alpha) * dnn_mae\n",
    "                \n",
    "                weighted_rmse += weight * category_rmse\n",
    "                weighted_mae += weight * category_mae\n",
    "            \n",
    "            # Store hybrid evaluation metrics\n",
    "            hybrid_metrics = {\n",
    "                'rmse': weighted_rmse,\n",
    "                'mae': weighted_mae,\n",
    "                'num_predictions': self.data['content_evaluation'].get('num_predictions', 0),\n",
    "                'use_adaptive_alpha': True\n",
    "            }\n",
    "        else:\n",
    "            # Use fixed alpha\n",
    "            hybrid_rmse = self.alpha * content_rmse + (1 - self.alpha) * dnn_rmse\n",
    "            hybrid_mae = self.alpha * content_mae + (1 - self.alpha) * dnn_mae\n",
    "            \n",
    "            hybrid_metrics = {\n",
    "                'rmse': hybrid_rmse,\n",
    "                'mae': hybrid_mae,\n",
    "                'num_predictions': self.data['content_evaluation'].get('num_predictions', 0),\n",
    "                'use_adaptive_alpha': False\n",
    "            }\n",
    "        \n",
    "        print(f\"\\nHybrid model evaluation (with {'adaptive' if use_adaptive_alpha else 'fixed'} alpha):\")\n",
    "        print(f\"RMSE: {hybrid_metrics['rmse']:.4f}\")\n",
    "        print(f\"MAE: {hybrid_metrics['mae']:.4f}\")\n",
    "        \n",
    "        # Save metrics\n",
    "        pd.DataFrame([hybrid_metrics]).to_csv(os.path.join(self.output_path, 'evaluation_metrics.csv'), index=False)\n",
    "        \n",
    "        return hybrid_metrics\n",
    "    \n",
    "    def recommend_for_user(self, user_id, n=10, use_adaptive_alpha=True):\n",
    "        \"\"\"\n",
    "        Get recommendations for a specific user\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        user_id: int\n",
    "            User ID\n",
    "        n: int\n",
    "            Number of recommendations to return\n",
    "        use_adaptive_alpha: bool\n",
    "            Whether to use adaptive alpha based on user rating count\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list\n",
    "            List of (movie_id, title, score) tuples\n",
    "        \"\"\"\n",
    "        # Check if user has recommendations\n",
    "        if 'combined_recommendations' not in self.data or user_id not in self.data['combined_recommendations']:\n",
    "            print(f\"No pre-computed recommendations found for user {user_id}\")\n",
    "            \n",
    "            # Look for individual model recommendations\n",
    "            content_recs = []\n",
    "            if 'content_recommendations' in self.data and user_id in self.data['content_recommendations']:\n",
    "                content_recs = self.data['content_recommendations'][user_id]\n",
    "            \n",
    "            collab_recs = []\n",
    "            if 'collaborative_recommendations' in self.data and user_id in self.data['collaborative_recommendations']:\n",
    "                collab_recs = self.data['collaborative_recommendations'][user_id]\n",
    "            \n",
    "            if not content_recs and not collab_recs:\n",
    "                print(f\"No recommendations available for user {user_id}\")\n",
    "                return []\n",
    "            \n",
    "            # Get alpha for this user\n",
    "            alpha = self.get_adaptive_alpha(user_id) if use_adaptive_alpha else self.alpha\n",
    "            \n",
    "            # Combine available recommendations\n",
    "            combined_recs = {}\n",
    "            \n",
    "            # Add content-based recs\n",
    "            for movie_id, score in content_recs:\n",
    "                combined_recs[movie_id] = {\"content_score\": score, \"has_content\": True}\n",
    "            \n",
    "            # Add collaborative recs\n",
    "            for movie_id, rating in collab_recs:\n",
    "                collab_score = self.normalize_prediction(rating)\n",
    "                if movie_id in combined_recs:\n",
    "                    combined_recs[movie_id][\"collab_score\"] = collab_score\n",
    "                    combined_recs[movie_id][\"has_collab\"] = True\n",
    "                else:\n",
    "                    combined_recs[movie_id] = {\"collab_score\": collab_score, \"has_collab\": True}\n",
    "            \n",
    "            # Calculate final scores\n",
    "            recommendations = []\n",
    "            for movie_id, data in combined_recs.items():\n",
    "                if data.get(\"has_content\", False) and data.get(\"has_collab\", False):\n",
    "                    combined_score = alpha * data[\"content_score\"] + (1 - alpha) * data[\"collab_score\"]\n",
    "                elif data.get(\"has_content\", False):\n",
    "                    combined_score = data[\"content_score\"]\n",
    "                elif data.get(\"has_collab\", False):\n",
    "                    combined_score = data[\"collab_score\"]\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                final_rating = self.denormalize_prediction(combined_score)\n",
    "                recommendations.append((movie_id, final_rating))\n",
    "            \n",
    "            # Sort and get top-n\n",
    "            recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "            recommendations = recommendations[:n]\n",
    "        else:\n",
    "            # Use pre-computed recommendations\n",
    "            recommendations = self.data['combined_recommendations'][user_id][:n]\n",
    "        \n",
    "        # Format recommendations with titles\n",
    "        formatted_recs = []\n",
    "        for movie_id, score in recommendations:\n",
    "            title = \"Unknown\"\n",
    "            if 'movie_features' in self.data:\n",
    "                movie_row = self.data['movie_features'][self.data['movie_features']['movieId'] == movie_id]\n",
    "                if not movie_row.empty and 'title' in movie_row.columns:\n",
    "                    title = movie_row.iloc[0]['title']\n",
    "            \n",
    "            formatted_recs.append((movie_id, title, score))\n",
    "        \n",
    "        return formatted_recs\n",
    "def main():\n",
    "    # Configuration section - replace argparse functionality\n",
    "    # Set these values as needed\n",
    "    content_path = \"./rec/content-recommendations\"\n",
    "    collab_path = \"./rec/collaborative-recommendations\"\n",
    "    output_path = \"./rec/hybrid_recommendations\"\n",
    "    alpha = 0.3\n",
    "    optimize_alpha = False\n",
    "    adaptive_alpha = True\n",
    "    batch_mode = True  # Set to True to avoid interactive prompts in notebook\n",
    "    num_recs = 10\n",
    "    generate = True  # Generate recommendations if they don't exist\n",
    "\n",
    "    # Keep the rest of the imports and class definitions exactly as they are in the original file\n",
    "    # ...\n",
    "\n",
    "    # Replace the main() function call at the bottom of the script with this code:\n",
    "\n",
    "    # Create and initialize the hybrid recommender\n",
    "    recommender = HybridRecommender(\n",
    "        content_model_path=content_path,\n",
    "        collab_model_path=collab_path,\n",
    "        output_path=output_path,\n",
    "        alpha=alpha\n",
    "    )\n",
    "\n",
    "    # Load data\n",
    "    recommender.load_data()\n",
    "\n",
    "    # Generate recommendations if requested and they don't exist\n",
    "    if generate:\n",
    "        if 'content_recommendations' not in recommender.data or not recommender.data['content_recommendations']:\n",
    "            recommender.generate_content_based_recommendations()\n",
    "        \n",
    "        if 'collaborative_recommendations' not in recommender.data or not recommender.data['collaborative_recommendations']:\n",
    "            recommender.generate_collaborative_recommendations()\n",
    "\n",
    "    # Find optimal alpha if requested\n",
    "    if optimize_alpha:\n",
    "        optimal_alpha = recommender.find_optimal_alpha()\n",
    "        print(f\"Optimal alpha: {optimal_alpha:.2f}\")\n",
    "\n",
    "    # Combine recommendations\n",
    "    recommender.combine_recommendations(top_n=num_recs, use_adaptive_alpha=adaptive_alpha)\n",
    "\n",
    "    # Evaluate\n",
    "    evaluation_metrics = recommender.evaluate(use_adaptive_alpha=adaptive_alpha)\n",
    "\n",
    "    # Compare with individual models\n",
    "    print(\"\\nModel Performance Comparison:\")\n",
    "    headers = [\"Model\", \"RMSE\", \"MAE\", \"Predictions\"]\n",
    "    rows = []\n",
    "\n",
    "    # Content-based model metrics\n",
    "    if 'content_evaluation' in recommender.data:\n",
    "        rows.append([\n",
    "            \"Content-Based\",\n",
    "            f\"{recommender.data['content_evaluation']['rmse']:.4f}\",\n",
    "            f\"{recommender.data['content_evaluation'].get('mae', 'N/A')}\",\n",
    "            f\"{recommender.data['content_evaluation'].get('num_predictions', 'N/A')}\"\n",
    "        ])\n",
    "\n",
    "    # Collaborative filtering model metrics\n",
    "    if 'dnn_evaluation' in recommender.data:\n",
    "        rows.append([\n",
    "            \"Collaborative\",\n",
    "            f\"{recommender.data['dnn_evaluation']['rmse']:.4f}\",\n",
    "            f\"{recommender.data['dnn_evaluation']['mae']:.4f}\",\n",
    "            f\"{recommender.data['dnn_evaluation']['num_predictions']}\"\n",
    "        ])\n",
    "\n",
    "    # Hybrid model metrics\n",
    "    if evaluation_metrics:\n",
    "        alpha_desc = \"Adaptive\" if adaptive_alpha else f\"α={recommender.alpha:.2f}\"\n",
    "        rows.append([\n",
    "            f\"Hybrid ({alpha_desc})\",\n",
    "            f\"{evaluation_metrics['rmse']:.4f}\",\n",
    "            f\"{evaluation_metrics['mae']:.4f}\",\n",
    "            f\"{evaluation_metrics['num_predictions']}\"\n",
    "        ])\n",
    "\n",
    "    # Print table\n",
    "    if rows:\n",
    "        # Calculate column widths\n",
    "        col_widths = [max(len(row[i]) for row in [headers] + rows) for i in range(len(headers))]\n",
    "        \n",
    "        # Print table header\n",
    "        print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")\n",
    "        print(\"| \" + \" | \".join(headers[i].ljust(col_widths[i]) for i in range(len(headers))) + \" |\")\n",
    "        print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")\n",
    "        \n",
    "        # Print table rows\n",
    "        for row in rows:\n",
    "            print(\"| \" + \" | \".join(row[i].ljust(col_widths[i]) for i in range(len(row))) + \" |\")\n",
    "        \n",
    "        print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")\n",
    "\n",
    "    print(\"\\nHybrid Recommendation System completed successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
