{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPTIMIZED HYBRID MOVIE RECOMMENDATION SYSTEM (alpha=0.30)\n",
      "================================================================================\n",
      "\n",
      "Loading essential data...\n",
      "Loaded content-based evaluation metrics: RMSE=0.9647\n",
      "Loaded DNN evaluation metrics: RMSE=0.9064\n",
      "Loaded content-based recommendations for 16090 users\n",
      "Loaded collaborative filtering recommendations for 20000 users\n",
      "Loaded rating counts for 500 users\n",
      "Loaded minimal movie metadata for 7359 movies\n",
      "Loaded learned optimal alpha values for 1 rating bins\n",
      "Data loading completed in 1.26s\n",
      "Found 16090 users with both content-based and collaborative recommendations\n",
      "\n",
      "Learning optimal alpha values with grid search for 8 bins...\n",
      "\n",
      "User distribution in rating bins:\n",
      "  0-9 ratings: 0 users\n",
      "  10-24 ratings: 57 users\n",
      "  25-49 ratings: 117 users\n",
      "  50-99 ratings: 142 users\n",
      "  100-149 ratings: 54 users\n",
      "  150-199 ratings: 34 users\n",
      "  200-299 ratings: 34 users\n",
      "  300+ ratings: 62 users\n",
      "Prepared validation data for 63 users\n",
      "\n",
      "Finding optimal alpha for bin 0-9...\n",
      "  No users in bin 0-9, skipping\n",
      "\n",
      "Finding optimal alpha for bin 10-24...\n",
      "  No users with validation data in bin 10-24, skipping\n",
      "\n",
      "Finding optimal alpha for bin 25-49...\n",
      "  No users with validation data in bin 25-49, skipping\n",
      "\n",
      "Finding optimal alpha for bin 50-99...\n",
      "  No users with validation data in bin 50-99, skipping\n",
      "\n",
      "Finding optimal alpha for bin 100-149...\n",
      "  No users with validation data in bin 100-149, skipping\n",
      "\n",
      "Finding optimal alpha for bin 150-199...\n",
      "  No users with validation data in bin 150-199, skipping\n",
      "\n",
      "Finding optimal alpha for bin 200-299...\n",
      "  No users with validation data in bin 200-299, skipping\n",
      "\n",
      "Finding optimal alpha for bin 300+...\n",
      "  No users with validation data in bin 300+, skipping\n",
      "\n",
      "Optimal alpha values saved to ./rec/hybrid_recommendations\\optimal_alphas.json\n",
      "\n",
      "Combining recommendations with adaptive alpha values...\n",
      "Processed 1000/20000 users (5.0%)\n",
      "Processed 2000/20000 users (10.0%)\n",
      "Processed 3000/20000 users (15.0%)\n",
      "Processed 4000/20000 users (20.0%)\n",
      "Processed 5000/20000 users (25.0%)\n",
      "Processed 6000/20000 users (30.0%)\n",
      "Processed 7000/20000 users (35.0%)\n",
      "Processed 8000/20000 users (40.0%)\n",
      "Processed 9000/20000 users (45.0%)\n",
      "Processed 10000/20000 users (50.0%)\n",
      "Processed 11000/20000 users (55.0%)\n",
      "Processed 12000/20000 users (60.0%)\n",
      "Processed 13000/20000 users (65.0%)\n",
      "Processed 14000/20000 users (70.0%)\n",
      "Processed 15000/20000 users (75.0%)\n",
      "Processed 16000/20000 users (80.0%)\n",
      "Processed 17000/20000 users (85.0%)\n",
      "Processed 18000/20000 users (90.0%)\n",
      "Processed 19000/20000 users (95.0%)\n",
      "Processed 20000/20000 users (100.0%)\n",
      "\n",
      "Adaptive Alpha Statistics:\n",
      "Average alpha: 0.0171\n",
      "Min alpha: 0.0100, Max alpha: 0.6500\n",
      "\n",
      "Alpha by user rating count:\n",
      "  <=10 ratings: 19500 users, avg alpha = 0.0100\n",
      "  11-25 ratings: 67 users, avg alpha = 0.0500\n",
      "  26-50 ratings: 115 users, avg alpha = 0.1500\n",
      "  51-100 ratings: 137 users, avg alpha = 0.2500\n",
      "  101-150 ratings: 51 users, avg alpha = 0.3500\n",
      "  151-200 ratings: 34 users, avg alpha = 0.4500\n",
      "  201-300 ratings: 34 users, avg alpha = 0.5500\n",
      "  >300 ratings: 62 users, avg alpha = 0.6500\n",
      "Combined recommendations for 20000 users in 12.89s\n",
      "Saved combined recommendations to CSV with 200000 entries\n",
      "\n",
      "Evaluating hybrid recommendation system using pre-computed metrics...\n",
      "  Bin >300: 62 users, alpha = 0.6500, RMSE = 0.9443\n",
      "  Bin 201-300: 34 users, alpha = 0.5500, RMSE = 0.9384\n",
      "  Bin 151-200: 34 users, alpha = 0.4500, RMSE = 0.9326\n",
      "  Bin 101-150: 51 users, alpha = 0.3500, RMSE = 0.9268\n",
      "  Bin 51-100: 137 users, alpha = 0.2500, RMSE = 0.9209\n",
      "  Bin 26-50: 115 users, alpha = 0.1500, RMSE = 0.9151\n",
      "  Bin 11-25: 67 users, alpha = 0.0500, RMSE = 0.9093\n",
      "\n",
      "Hybrid model evaluation (with adaptive alpha):\n",
      "RMSE: 0.9235\n",
      "MAE: 0.7100\n",
      "\n",
      "Model Performance Comparison:\n",
      "+-----------------------------+--------+--------------------+-------------+\n",
      "| Model                       | RMSE   | MAE                | Predictions |\n",
      "+-----------------------------+--------+--------------------+-------------+\n",
      "| Content-Based               | 0.9647 | 0.7524811763493512 | 2894341.0   |\n",
      "| Collaborative               | 0.9064 | 0.6923             | 199494      |\n",
      "| Hybrid (Adaptive (Learned)) | 0.9235 | 0.7100             | 2894341.0   |\n",
      "+-----------------------------+--------+--------------------+-------------+\n",
      "\n",
      "Sample recommendations for user 1.0:\n",
      "1. Unknown (ID: 117366) - Score: 4.39\n",
      "2. Spirited Away (Sen to Chihiro no kamikakushi) (2001) (ID: 5618) - Score: 4.37\n",
      "3. Unknown (ID: 93659) - Score: 4.36\n",
      "4. Unknown (ID: 72583) - Score: 4.36\n",
      "5. Beowulf & Grendel (2005) (ID: 38294) - Score: 4.36\n",
      "Visualization saved to ./rec/hybrid_recommendations\\recommendation_scores.png\n",
      "\n",
      "Hybrid Recommendation System with Learning completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class HybridRecommender:\n",
    "    def __init__(self, content_model_path=\"./rec/content-recommendations\", \n",
    "                 collab_model_path=\"./rec/collaborative-recommendations\", \n",
    "                 output_path=\"./rec/hybrid_recommendations\", \n",
    "                 alpha=0.3):\n",
    "        \"\"\"\n",
    "        Initialize the hybrid recommender with paths to content-based and collaborative filtering models\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        content_model_path: str\n",
    "            Path to the directory containing content-based model files\n",
    "        collab_model_path: str\n",
    "            Path to the directory containing collaborative filtering model files\n",
    "        output_path: str\n",
    "            Path to save hybrid recommendation results\n",
    "        alpha: float\n",
    "            Weight for content-based recommendations (1-alpha for collaborative)\n",
    "        \"\"\"\n",
    "        self.content_model_path = content_model_path\n",
    "        self.collab_model_path = collab_model_path\n",
    "        self.output_path = output_path\n",
    "        self.alpha = alpha\n",
    "        self.data = {}  # Container for all loaded data\n",
    "        self.optimal_alphas = None\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"OPTIMIZED HYBRID MOVIE RECOMMENDATION SYSTEM (alpha={self.alpha:.2f})\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Optimized data loading - only load the necessary evaluation files and recommendations\n",
    "        \"\"\"\n",
    "        print(\"\\nLoading essential data...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Load content-based evaluation metrics\n",
    "        try:\n",
    "            content_eval_path = os.path.join(self.content_model_path, 'content_based_evaluation.csv')\n",
    "            if os.path.exists(content_eval_path):\n",
    "                content_eval_df = pd.read_csv(content_eval_path)\n",
    "                if not content_eval_df.empty:\n",
    "                    self.data['content_evaluation'] = {\n",
    "                        'rmse': content_eval_df.iloc[0]['rmse'],\n",
    "                        'mae': content_eval_df.iloc[0]['mae'] if 'mae' in content_eval_df.columns else None,\n",
    "                        'num_predictions': content_eval_df.iloc[0]['num_predictions'] if 'num_predictions' in content_eval_df.columns else None\n",
    "                    }\n",
    "                    print(f\"Loaded content-based evaluation metrics: RMSE={self.data['content_evaluation']['rmse']:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading content-based evaluation metrics: {str(e)}\")\n",
    "        \n",
    "        # Load DNN evaluation metrics\n",
    "        try:\n",
    "            dnn_eval_path = os.path.join(self.collab_model_path, 'dnn_evaluation.csv')\n",
    "            if os.path.exists(dnn_eval_path):\n",
    "                dnn_eval_df = pd.read_csv(dnn_eval_path)\n",
    "                if not dnn_eval_df.empty:\n",
    "                    self.data['dnn_evaluation'] = {\n",
    "                        'rmse': dnn_eval_df.iloc[0]['rmse'],\n",
    "                        'mae': dnn_eval_df.iloc[0]['mae'],\n",
    "                        'num_predictions': dnn_eval_df.iloc[0]['num_predictions']\n",
    "                    }\n",
    "                    print(f\"Loaded DNN evaluation metrics: RMSE={self.data['dnn_evaluation']['rmse']:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading DNN evaluation metrics: {str(e)}\")\n",
    "        \n",
    "        # Load content-based recommendations\n",
    "        try:\n",
    "            content_recs_path = os.path.join(self.content_model_path, 'content_based_recommendations.pkl')\n",
    "            if os.path.exists(content_recs_path):\n",
    "                with open(content_recs_path, 'rb') as f:\n",
    "                    self.data['content_recommendations'] = pickle.load(f)\n",
    "                print(f\"Loaded content-based recommendations for {len(self.data['content_recommendations'])} users\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading content-based recommendations: {str(e)}\")\n",
    "        \n",
    "        # Load collaborative filtering recommendations\n",
    "        try:\n",
    "            collab_recs_path = os.path.join(self.collab_model_path, 'dnn_recommendations.pkl')\n",
    "            if os.path.exists(collab_recs_path):\n",
    "                with open(collab_recs_path, 'rb') as f:\n",
    "                    self.data['collaborative_recommendations'] = pickle.load(f)\n",
    "                print(f\"Loaded collaborative filtering recommendations for {len(self.data['collaborative_recommendations'])} users\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading collaborative filtering recommendations: {str(e)}\")\n",
    "                \n",
    "        # Load minimal user rating data needed just for calculating adaptive alpha\n",
    "        try:\n",
    "            # We only need userId and rating count, so we'll parse this from the CSV directly\n",
    "            ratings_path = './processed/normalized_ratings.csv'\n",
    "            if os.path.exists(ratings_path):\n",
    "                # Read only the userId column to calculate rating counts\n",
    "                ratings_df = pd.read_csv(ratings_path, usecols=['userId'])\n",
    "                user_rating_counts = ratings_df['userId'].value_counts().reset_index()\n",
    "                user_rating_counts.columns = ['userId', 'rating_count']\n",
    "                self.data['user_rating_counts'] = user_rating_counts\n",
    "                print(f\"Loaded rating counts for {len(user_rating_counts)} users\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading user rating counts: {str(e)}\")\n",
    "            \n",
    "        # Optionally load minimal movie metadata (just for displaying recommendations)\n",
    "        try:\n",
    "            movie_features_path = './processed/processed_movie_features.csv'\n",
    "            if os.path.exists(movie_features_path):\n",
    "                # Read only the essential columns\n",
    "                self.data['movie_features'] = pd.read_csv(\n",
    "                    movie_features_path, \n",
    "                    usecols=['movieId', 'title']  # Only load the columns we need\n",
    "                )\n",
    "                print(f\"Loaded minimal movie metadata for {len(self.data['movie_features'])} movies\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading movie metadata: {str(e)}\")\n",
    "        \n",
    "        # Try to load previously learned optimal alphas if they exist\n",
    "        try:\n",
    "            optimal_alphas_path = os.path.join(self.output_path, 'optimal_alphas.json')\n",
    "            if os.path.exists(optimal_alphas_path):\n",
    "                with open(optimal_alphas_path, 'r') as f:\n",
    "                    self.optimal_alphas = json.load(f)\n",
    "                print(f\"Loaded learned optimal alpha values for {len(self.optimal_alphas)} rating bins\")\n",
    "        except Exception as e:\n",
    "            print(f\"No learned alpha values found: {str(e)}\")\n",
    "        \n",
    "        print(f\"Data loading completed in {time.time() - start_time:.2f}s\")\n",
    "        \n",
    "        # Get common users for both recommendation systems\n",
    "        self.common_users = set()\n",
    "        if 'content_recommendations' in self.data and 'collaborative_recommendations' in self.data:\n",
    "            self.common_users = set(self.data['content_recommendations'].keys()) & set(self.data['collaborative_recommendations'].keys())\n",
    "            print(f\"Found {len(self.common_users)} users with both content-based and collaborative recommendations\")\n",
    "        \n",
    "        return self.data\n",
    "\n",
    "    def get_adaptive_alpha(self, user_id):\n",
    "        \"\"\"\n",
    "        Get optimized alpha value based on user's rating count and learned optimal values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        user_id: int\n",
    "            User ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Optimized alpha value\n",
    "        \"\"\"\n",
    "        # Get user's rating count\n",
    "        rating_count = 0\n",
    "        if 'user_rating_counts' in self.data:\n",
    "            user_data = self.data['user_rating_counts'][self.data['user_rating_counts']['userId'] == user_id]\n",
    "            if not user_data.empty:\n",
    "                rating_count = user_data.iloc[0]['rating_count']\n",
    "        \n",
    "        # Check if we have learned optimal alpha values\n",
    "        if self.optimal_alphas:\n",
    "            # Find appropriate bin for this user\n",
    "            for bin_name, alpha in self.optimal_alphas.items():\n",
    "                if '-' in bin_name:\n",
    "                    # Range bin (e.g., \"10-24\")\n",
    "                    lower, upper = map(int, bin_name.split('-'))\n",
    "                    if lower <= rating_count <= upper:\n",
    "                        return alpha\n",
    "                elif '+' in bin_name:\n",
    "                    # Last bin (e.g., \"300+\")\n",
    "                    threshold = int(bin_name.replace('+', ''))\n",
    "                    if rating_count >= threshold:\n",
    "                        return alpha\n",
    "        \n",
    "        # Fallback to default values if no learned values are available\n",
    "        if rating_count <= 10:\n",
    "            return 0.01\n",
    "        elif rating_count <= 25:\n",
    "            return 0.05\n",
    "        elif rating_count <= 50:\n",
    "            return 0.15\n",
    "        elif rating_count <= 100:\n",
    "            return 0.25\n",
    "        elif rating_count <= 150:\n",
    "            return 0.35\n",
    "        elif rating_count <= 200:\n",
    "            return 0.45\n",
    "        elif rating_count <= 300:\n",
    "            return 0.55\n",
    "        else:  # > 300\n",
    "            return 0.65\n",
    "    \n",
    "    def normalize_prediction(self, prediction):\n",
    "        \"\"\"\n",
    "        Normalize a prediction to the 0-1 range\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        prediction: float\n",
    "            Prediction value in the 0.5-5.0 range\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Normalized prediction in the 0-1 range\n",
    "        \"\"\"\n",
    "        # Normalize from rating scale [0.5, 5.0] to [0, 1]\n",
    "        return (prediction - 0.5) / 4.5\n",
    "    \n",
    "    def denormalize_prediction(self, normalized_prediction):\n",
    "        \"\"\"\n",
    "        Convert a normalized prediction back to the 0.5-5.0 range\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        normalized_prediction: float\n",
    "            Normalized prediction in the 0-1 range\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Prediction value in the 0.5-5.0 range\n",
    "        \"\"\"\n",
    "        # Convert from [0, 1] back to rating scale [0.5, 5.0]\n",
    "        return 0.5 + 4.5 * normalized_prediction\n",
    "    \n",
    "    def _prepare_validation_data(self):\n",
    "        \"\"\"\n",
    "        Prepare validation data for alpha optimization\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Validation data for evaluating different alpha values\n",
    "        \"\"\"\n",
    "        # We need content and collaborative predictions for the same user-item pairs\n",
    "        validation_data = {}\n",
    "        \n",
    "        # Get common users with both types of recommendations\n",
    "        common_users = self.common_users\n",
    "        if not common_users:\n",
    "            print(\"Error: No common users found with both types of recommendations\")\n",
    "            return None\n",
    "        \n",
    "        # Sample users for validation\n",
    "        sample_size = min(1000, len(common_users))\n",
    "        validation_users = np.random.choice(list(common_users), sample_size, replace=False)\n",
    "        \n",
    "        # For each user, find items with both content and collaborative predictions\n",
    "        for user_id in validation_users:\n",
    "            user_validation = {}\n",
    "            \n",
    "            # Get content recommendations\n",
    "            content_recs = {movie_id: score for movie_id, score \n",
    "                           in self.data['content_recommendations'].get(user_id, [])}\n",
    "            \n",
    "            # Get collaborative recommendations\n",
    "            collab_recs = {movie_id: rating for movie_id, rating \n",
    "                          in self.data['collaborative_recommendations'].get(user_id, [])}\n",
    "            \n",
    "            # Find common movies\n",
    "            common_movies = set(content_recs.keys()) & set(collab_recs.keys())\n",
    "            \n",
    "            if not common_movies:\n",
    "                continue\n",
    "            \n",
    "            # Add to validation data\n",
    "            for movie_id in common_movies:\n",
    "                user_validation[movie_id] = {\n",
    "                    'content_score': content_recs[movie_id],\n",
    "                    'collab_score': self.normalize_prediction(collab_recs[movie_id])\n",
    "                }\n",
    "            \n",
    "            if user_validation:\n",
    "                validation_data[user_id] = user_validation\n",
    "        \n",
    "        print(f\"Prepared validation data for {len(validation_data)} users\")\n",
    "        return validation_data\n",
    "\n",
    "    def _evaluate_alpha_for_users(self, alpha, user_ids, validation_data):\n",
    "        \"\"\"\n",
    "        Evaluate RMSE for a specific alpha value on a set of users\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        alpha: float\n",
    "            Alpha value to evaluate\n",
    "        user_ids: list\n",
    "            List of user IDs to evaluate\n",
    "        validation_data: dict\n",
    "            Validation data\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            RMSE for the given alpha value\n",
    "        \"\"\"\n",
    "        squared_errors = []\n",
    "        \n",
    "        for user_id in user_ids:\n",
    "            if user_id not in validation_data:\n",
    "                continue\n",
    "            \n",
    "            user_validation = validation_data[user_id]\n",
    "            \n",
    "            for movie_id, data in user_validation.items():\n",
    "                # Combine predictions using alpha\n",
    "                combined_score = alpha * data['content_score'] + (1 - alpha) * data['collab_score']\n",
    "                \n",
    "                # Convert to rating scale\n",
    "                final_rating = self.denormalize_prediction(combined_score)\n",
    "                \n",
    "                # Compare with ground truth (use collaborative rating as ground truth)\n",
    "                true_rating = self.denormalize_prediction(data['collab_score'])\n",
    "                \n",
    "                # Calculate squared error\n",
    "                squared_error = (final_rating - true_rating) ** 2\n",
    "                squared_errors.append(squared_error)\n",
    "        \n",
    "        if not squared_errors:\n",
    "            return float('inf')\n",
    "        \n",
    "        # Calculate RMSE\n",
    "        rmse = np.sqrt(np.mean(squared_errors))\n",
    "        return rmse\n",
    "    \n",
    "    def learn_optimal_alpha_values(self, rating_bins=[0, 10, 25, 50, 100, 150, 200, 300], \n",
    "                                 alpha_values=None):\n",
    "        \"\"\"\n",
    "        Learn optimal alpha values for different rating count bins using grid search\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        rating_bins: list\n",
    "            Rating count thresholds for binning users\n",
    "        alpha_values: array\n",
    "            Possible alpha values to explore\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Mapping of rating bins to optimal alpha values\n",
    "        \"\"\"\n",
    "        if alpha_values is None:\n",
    "            alpha_values = np.arange(0.0, 1.05, 0.05)\n",
    "            \n",
    "        print(f\"\\nLearning optimal alpha values with grid search for {len(rating_bins)} bins...\")\n",
    "        \n",
    "        # Get user rating counts and test data\n",
    "        if 'user_rating_counts' not in self.data:\n",
    "            print(\"Error: User rating counts not available\")\n",
    "            return None\n",
    "        \n",
    "        # Create user bins based on rating counts\n",
    "        user_bins = {}\n",
    "        for i in range(len(rating_bins)):\n",
    "            if i == len(rating_bins) - 1:\n",
    "                bin_name = f\"{rating_bins[i]}+\"\n",
    "            else:\n",
    "                bin_name = f\"{rating_bins[i]}-{rating_bins[i+1]-1}\"\n",
    "            user_bins[bin_name] = []\n",
    "        \n",
    "        # Assign users to bins\n",
    "        for _, row in self.data['user_rating_counts'].iterrows():\n",
    "            user_id = row['userId']\n",
    "            rating_count = row['rating_count']\n",
    "            \n",
    "            # Find appropriate bin\n",
    "            for i in range(len(rating_bins)):\n",
    "                if i == len(rating_bins) - 1:\n",
    "                    if rating_count >= rating_bins[i]:\n",
    "                        bin_name = f\"{rating_bins[i]}+\"\n",
    "                        user_bins[bin_name].append(user_id)\n",
    "                        break\n",
    "                else:\n",
    "                    if rating_bins[i] <= rating_count < rating_bins[i+1]:\n",
    "                        bin_name = f\"{rating_bins[i]}-{rating_bins[i+1]-1}\"\n",
    "                        user_bins[bin_name].append(user_id)\n",
    "                        break\n",
    "        \n",
    "        # Print user distribution in bins\n",
    "        print(\"\\nUser distribution in rating bins:\")\n",
    "        for bin_name, users in user_bins.items():\n",
    "            print(f\"  {bin_name} ratings: {len(users)} users\")\n",
    "        \n",
    "        # Prepare validation data\n",
    "        validation_data = self._prepare_validation_data()\n",
    "        if validation_data is None:\n",
    "            print(\"Error: Could not prepare validation data\")\n",
    "            return None\n",
    "        \n",
    "        # For each bin, find optimal alpha value\n",
    "        optimal_alphas = {}\n",
    "        \n",
    "        for bin_name, user_ids in user_bins.items():\n",
    "            print(f\"\\nFinding optimal alpha for bin {bin_name}...\")\n",
    "            \n",
    "            if not user_ids:\n",
    "                print(f\"  No users in bin {bin_name}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Filter users to those with validation data\n",
    "            valid_users = [uid for uid in user_ids if uid in validation_data]\n",
    "            \n",
    "            if not valid_users:\n",
    "                print(f\"  No users with validation data in bin {bin_name}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Select a sample of users for faster evaluation if bin is large\n",
    "            sample_size = min(500, len(valid_users))\n",
    "            sample_users = np.random.choice(valid_users, sample_size, replace=False)\n",
    "            \n",
    "            best_rmse = float('inf')\n",
    "            best_alpha = 0.0\n",
    "            \n",
    "            for alpha in alpha_values:\n",
    "                # Evaluate RMSE for this alpha value\n",
    "                rmse = self._evaluate_alpha_for_users(alpha, sample_users, validation_data)\n",
    "                \n",
    "                print(f\"  Alpha {alpha:.2f}: RMSE {rmse:.4f}\")\n",
    "                \n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    best_alpha = alpha\n",
    "            \n",
    "            optimal_alphas[bin_name] = best_alpha\n",
    "            print(f\"  Optimal alpha for bin {bin_name}: {best_alpha:.2f} (RMSE: {best_rmse:.4f})\")\n",
    "        \n",
    "        # Save optimal alpha values\n",
    "        self.optimal_alphas = optimal_alphas\n",
    "        \n",
    "        # Save to file\n",
    "        with open(os.path.join(self.output_path, 'optimal_alphas.json'), 'w') as f:\n",
    "            json.dump(optimal_alphas, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nOptimal alpha values saved to {os.path.join(self.output_path, 'optimal_alphas.json')}\")\n",
    "        \n",
    "        return optimal_alphas\n",
    "    \n",
    "    def combine_recommendations(self, top_n=10, use_adaptive_alpha=True):\n",
    "        \"\"\"\n",
    "        Combine content-based and collaborative filtering recommendations with optimized weighting\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        top_n: int\n",
    "            Number of recommendations to generate per user\n",
    "        use_adaptive_alpha: bool\n",
    "            Whether to use adaptive alpha based on user rating count\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            User ID to list of (movie_id, score) tuples\n",
    "        \"\"\"\n",
    "        print(f\"\\nCombining recommendations with {'adaptive' if use_adaptive_alpha else 'fixed'} alpha values...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get recommendations from both models\n",
    "        content_recs = self.data.get('content_recommendations', {})\n",
    "        collab_recs = self.data.get('collaborative_recommendations', {})\n",
    "        \n",
    "        if not content_recs:\n",
    "            print(\"Warning: No content-based recommendations available\")\n",
    "        \n",
    "        if not collab_recs:\n",
    "            print(\"Warning: No collaborative filtering recommendations available\")\n",
    "        \n",
    "        if not content_recs and not collab_recs:\n",
    "            print(\"Error: No recommendations available from either model\")\n",
    "            return {}\n",
    "        \n",
    "        # Combine recommendations\n",
    "        combined_recommendations = {}\n",
    "        alpha_stats = {'values': [], 'count_categories': {}}\n",
    "        \n",
    "        # Get all users from both recommendation sets\n",
    "        all_users = set(content_recs.keys()) | set(collab_recs.keys())\n",
    "        total_users = len(all_users)\n",
    "        \n",
    "        for i, user_id in enumerate(all_users):\n",
    "            # Get appropriate alpha value for this user\n",
    "            if use_adaptive_alpha:\n",
    "                alpha = self.get_adaptive_alpha(user_id)\n",
    "                # Track alpha statistics\n",
    "                alpha_stats['values'].append(alpha)\n",
    "                \n",
    "                # Get user's rating count\n",
    "                rating_count = 0\n",
    "                if 'user_rating_counts' in self.data:\n",
    "                    user_data = self.data['user_rating_counts'][self.data['user_rating_counts']['userId'] == user_id]\n",
    "                    if not user_data.empty:\n",
    "                        rating_count = user_data.iloc[0]['rating_count']\n",
    "                \n",
    "                # Categorize for statistics\n",
    "                count_category = \"<=10\" if rating_count <= 10 else \"11-25\" if rating_count <= 25 else \"26-50\" if rating_count <= 50 else \"51-100\" if rating_count <= 100 else \"101-150\" if rating_count <= 150 else \"151-200\" if rating_count <= 200 else \"201-300\" if rating_count <= 300 else \">300\"\n",
    "                if count_category in alpha_stats['count_categories']:\n",
    "                    alpha_stats['count_categories'][count_category]['count'] += 1\n",
    "                    alpha_stats['count_categories'][count_category]['alpha_sum'] += alpha\n",
    "                else:\n",
    "                    alpha_stats['count_categories'][count_category] = {'count': 1, 'alpha_sum': alpha}\n",
    "            else:\n",
    "                alpha = self.alpha\n",
    "            \n",
    "            # Initialize combined recommendations dictionary for this user\n",
    "            user_combined_recs = {}\n",
    "            \n",
    "            # Add content-based recommendations if available\n",
    "            if user_id in content_recs:\n",
    "                for movie_id, score in content_recs[user_id]:\n",
    "                    # Scores from content-based are already normalized (0-1), just store them\n",
    "                    user_combined_recs[movie_id] = {'content_score': score, 'content_available': True}\n",
    "            \n",
    "            # Add collaborative filtering recommendations if available\n",
    "            if user_id in collab_recs:\n",
    "                for movie_id, rating in collab_recs[user_id]:\n",
    "                    # Normalize the collaborative rating to 0-1 scale\n",
    "                    collab_score = self.normalize_prediction(rating)\n",
    "                    \n",
    "                    if movie_id in user_combined_recs:\n",
    "                        user_combined_recs[movie_id]['collab_score'] = collab_score\n",
    "                        user_combined_recs[movie_id]['collab_available'] = True\n",
    "                    else:\n",
    "                        user_combined_recs[movie_id] = {\n",
    "                            'collab_score': collab_score, \n",
    "                            'collab_available': True,\n",
    "                            'content_available': False\n",
    "                        }\n",
    "            \n",
    "            # Calculate final scores with proper normalization\n",
    "            final_recommendations = []\n",
    "            for movie_id, data in user_combined_recs.items():\n",
    "                # Check which models provided predictions\n",
    "                content_available = data.get('content_available', False)\n",
    "                collab_available = data.get('collab_available', False)\n",
    "                \n",
    "                if content_available and collab_available:\n",
    "                    # We have both predictions, use the weighted average\n",
    "                    content_score = data['content_score']\n",
    "                    collab_score = data['collab_score']\n",
    "                    combined_score = alpha * content_score + (1 - alpha) * collab_score\n",
    "                elif content_available:\n",
    "                    # Only content-based prediction available\n",
    "                    combined_score = data['content_score']\n",
    "                elif collab_available:\n",
    "                    # Only collaborative prediction available\n",
    "                    combined_score = data['collab_score']\n",
    "                \n",
    "                # Convert back to rating scale for storage\n",
    "                final_rating = self.denormalize_prediction(combined_score)\n",
    "                final_recommendations.append((movie_id, final_rating))\n",
    "            \n",
    "            # Sort by final score and limit to top_n\n",
    "            final_recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "            combined_recommendations[user_id] = final_recommendations[:top_n]\n",
    "            \n",
    "            # Log progress\n",
    "            if (i+1) % 1000 == 0 or (i+1) == total_users:\n",
    "                print(f\"Processed {i+1}/{total_users} users ({(i+1)/total_users*100:.1f}%)\")\n",
    "        \n",
    "        self.data['combined_recommendations'] = combined_recommendations\n",
    "        \n",
    "        # Print alpha statistics if using adaptive alpha\n",
    "        if use_adaptive_alpha and alpha_stats['values']:\n",
    "            print(\"\\nAdaptive Alpha Statistics:\")\n",
    "            print(f\"Average alpha: {np.mean(alpha_stats['values']):.4f}\")\n",
    "            print(f\"Min alpha: {min(alpha_stats['values']):.4f}, Max alpha: {max(alpha_stats['values']):.4f}\")\n",
    "            print(\"\\nAlpha by user rating count:\")\n",
    "            \n",
    "            # Sort categories by rating count\n",
    "            def sort_key(category):\n",
    "                if category.startswith(\"<=\"):\n",
    "                    return (0, int(category[2:]))\n",
    "                elif category.startswith(\">\"):\n",
    "                    return (999, int(category[1:]))\n",
    "                else:\n",
    "                    # Format like \"11-25\"\n",
    "                    lower = int(category.split(\"-\")[0])\n",
    "                    return (lower, lower)\n",
    "                \n",
    "            for category, stats in sorted(alpha_stats['count_categories'].items(), key=lambda x: sort_key(x[0])):\n",
    "                avg_alpha = stats['alpha_sum'] / stats['count']\n",
    "                print(f\"  {category} ratings: {stats['count']} users, avg alpha = {avg_alpha:.4f}\")\n",
    "            \n",
    "            # Save alpha statistics to a file\n",
    "            with open(os.path.join(self.output_path, 'alpha_stats.txt'), 'w') as f:\n",
    "                f.write(f\"Adaptive Alpha Statistics:\\n\")\n",
    "                f.write(f\"Average alpha: {np.mean(alpha_stats['values']):.4f}\\n\")\n",
    "                f.write(f\"Min alpha: {min(alpha_stats['values']):.4f}, Max alpha: {max(alpha_stats['values']):.4f}\\n\\n\")\n",
    "                f.write(\"Alpha by user rating count:\\n\")\n",
    "                for category, stats in sorted(alpha_stats['count_categories'].items(), key=lambda x: sort_key(x[0])):\n",
    "                    avg_alpha = stats['alpha_sum'] / stats['count']\n",
    "                    f.write(f\"  {category} ratings: {stats['count']} users, avg alpha = {avg_alpha:.4f}\\n\")\n",
    "            \n",
    "            # Create a visualization of alpha distribution\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.hist(alpha_stats['values'], bins=20, alpha=0.7)\n",
    "            plt.title('Distribution of Alpha Values')\n",
    "            plt.xlabel('Alpha Value')\n",
    "            plt.ylabel('Number of Users')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.savefig(os.path.join(self.output_path, 'alpha_distribution.png'))\n",
    "            plt.close()\n",
    "        \n",
    "        print(f\"Combined recommendations for {len(combined_recommendations)} users in {time.time() - start_time:.2f}s\")\n",
    "        \n",
    "        # Save combined recommendations\n",
    "        with open(os.path.join(self.output_path, 'combined_recommendations.pkl'), 'wb') as f:\n",
    "            pickle.dump(combined_recommendations, f)\n",
    "        \n",
    "        # Also save in a more readable CSV format\n",
    "        recommendations_list = []\n",
    "        \n",
    "        for user_id, recs in combined_recommendations.items():\n",
    "            # Get user's alpha\n",
    "            if use_adaptive_alpha:\n",
    "                user_alpha = self.get_adaptive_alpha(user_id)\n",
    "            else:\n",
    "                user_alpha = self.alpha\n",
    "                \n",
    "            # Get user's rating count\n",
    "            rating_count = 0\n",
    "            if 'user_rating_counts' in self.data:\n",
    "                user_data = self.data['user_rating_counts'][self.data['user_rating_counts']['userId'] == user_id]\n",
    "                if not user_data.empty:\n",
    "                    rating_count = user_data.iloc[0]['rating_count']\n",
    "            \n",
    "            for rank, (movie_id, score) in enumerate(recs, 1):\n",
    "                movie_title = \"Unknown\"\n",
    "                if 'movie_features' in self.data:\n",
    "                    movie_row = self.data['movie_features'][self.data['movie_features']['movieId'] == movie_id]\n",
    "                    if not movie_row.empty and 'title' in movie_row.columns:\n",
    "                        movie_title = movie_row.iloc[0]['title']\n",
    "                \n",
    "                recommendations_list.append({\n",
    "                    'userId': user_id,\n",
    "                    'movieId': movie_id,\n",
    "                    'title': movie_title,\n",
    "                    'rank': rank,\n",
    "                    'score': score,\n",
    "                    'alpha': user_alpha,\n",
    "                    'rating_count': rating_count\n",
    "                })\n",
    "        \n",
    "        if recommendations_list:\n",
    "            recommendations_df = pd.DataFrame(recommendations_list)\n",
    "            recommendations_df.to_csv(os.path.join(self.output_path, 'combined_recommendations.csv'), index=False)\n",
    "            print(f\"Saved combined recommendations to CSV with {len(recommendations_df)} entries\")\n",
    "        \n",
    "        return combined_recommendations\n",
    "    \n",
    "    def evaluate(self, use_adaptive_alpha=True):\n",
    "        \"\"\"\n",
    "        Evaluate the hybrid recommendation system using the pre-computed metrics\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Evaluation metrics\n",
    "        \"\"\"\n",
    "        print(f\"\\nEvaluating hybrid recommendation system using pre-computed metrics...\")\n",
    "        \n",
    "        # Check if we have the pre-computed metrics\n",
    "        if 'content_evaluation' not in self.data or 'dnn_evaluation' not in self.data:\n",
    "            print(\"Cannot evaluate: Missing pre-computed evaluation metrics\")\n",
    "            return None\n",
    "        \n",
    "        content_rmse = self.data['content_evaluation']['rmse']\n",
    "        dnn_rmse = self.data['dnn_evaluation']['rmse']\n",
    "        \n",
    "        content_mae = self.data['content_evaluation'].get('mae', 0)\n",
    "        dnn_mae = self.data['dnn_evaluation'].get('mae', 0)\n",
    "        \n",
    "        # Calculate combined metrics based on alpha distribution\n",
    "        if use_adaptive_alpha and 'user_rating_counts' in self.data:\n",
    "            # Get user distribution in different rating count bins\n",
    "            rating_counts = {}\n",
    "            for _, row in self.data['user_rating_counts'].iterrows():\n",
    "                rating_count = row['rating_count']\n",
    "                if rating_count <= 10:\n",
    "                    bin_name = \"<=10\"\n",
    "                elif rating_count <= 25:\n",
    "                    bin_name = \"11-25\"\n",
    "                elif rating_count <= 50:\n",
    "                    bin_name = \"26-50\"\n",
    "                elif rating_count <= 100:\n",
    "                    bin_name = \"51-100\"\n",
    "                elif rating_count <= 150:\n",
    "                    bin_name = \"101-150\"\n",
    "                elif rating_count <= 200:\n",
    "                    bin_name = \"151-200\"\n",
    "                elif rating_count <= 300:\n",
    "                    bin_name = \"201-300\"\n",
    "                else:  # > 300\n",
    "                    bin_name = \">300\"\n",
    "                \n",
    "                if bin_name in rating_counts:\n",
    "                    rating_counts[bin_name] += 1\n",
    "                else:\n",
    "                    rating_counts[bin_name] = 1\n",
    "            \n",
    "            total_users = sum(rating_counts.values())\n",
    "            \n",
    "            # Calculate weighted RMSE and MAE\n",
    "            weighted_rmse = 0\n",
    "            weighted_mae = 0\n",
    "            \n",
    "            # Function to get alpha for a bin\n",
    "            def get_bin_alpha(bin_name):\n",
    "                if self.optimal_alphas and bin_name in self.optimal_alphas:\n",
    "                    return self.optimal_alphas[bin_name]\n",
    "                \n",
    "                # Fallback to default values\n",
    "                if bin_name == \"<=10\":\n",
    "                    return 0.01\n",
    "                elif bin_name == \"11-25\":\n",
    "                    return 0.05\n",
    "                elif bin_name == \"26-50\":\n",
    "                    return 0.15\n",
    "                elif bin_name == \"51-100\":\n",
    "                    return 0.25\n",
    "                elif bin_name == \"101-150\":\n",
    "                    return 0.35\n",
    "                elif bin_name == \"151-200\":\n",
    "                    return 0.45\n",
    "                elif bin_name == \"201-300\":\n",
    "                    return 0.55\n",
    "                else:  # \">300\"\n",
    "                    return 0.65\n",
    "            \n",
    "            for bin_name, count in rating_counts.items():\n",
    "                weight = count / total_users\n",
    "                alpha = get_bin_alpha(bin_name)\n",
    "                \n",
    "                # Calculate weighted metrics\n",
    "                bin_rmse = alpha * content_rmse + (1 - alpha) * dnn_rmse\n",
    "                bin_mae = alpha * content_mae + (1 - alpha) * dnn_mae\n",
    "                \n",
    "                weighted_rmse += weight * bin_rmse\n",
    "                weighted_mae += weight * bin_mae\n",
    "                \n",
    "                print(f\"  Bin {bin_name}: {count} users, alpha = {alpha:.4f}, RMSE = {bin_rmse:.4f}\")\n",
    "            \n",
    "            # Store hybrid evaluation metrics\n",
    "            hybrid_metrics = {\n",
    "                'rmse': weighted_rmse,\n",
    "                'mae': weighted_mae,\n",
    "                'num_predictions': self.data['content_evaluation'].get('num_predictions', 0),\n",
    "                'use_adaptive_alpha': True\n",
    "            }\n",
    "        else:\n",
    "            # Use fixed alpha\n",
    "            hybrid_rmse = self.alpha * content_rmse + (1 - self.alpha) * dnn_rmse\n",
    "            hybrid_mae = self.alpha * content_mae + (1 - self.alpha) * dnn_mae\n",
    "            \n",
    "            hybrid_metrics = {\n",
    "                'rmse': hybrid_rmse,\n",
    "                'mae': hybrid_mae,\n",
    "                'num_predictions': self.data['content_evaluation'].get('num_predictions', 0),\n",
    "                'use_adaptive_alpha': False\n",
    "            }\n",
    "        \n",
    "        print(f\"\\nHybrid model evaluation (with {'adaptive' if use_adaptive_alpha else 'fixed'} alpha):\")\n",
    "        print(f\"RMSE: {hybrid_metrics['rmse']:.4f}\")\n",
    "        print(f\"MAE: {hybrid_metrics['mae']:.4f}\")\n",
    "        \n",
    "        # Save metrics\n",
    "        pd.DataFrame([hybrid_metrics]).to_csv(os.path.join(self.output_path, 'evaluation_metrics.csv'), index=False)\n",
    "        \n",
    "        return hybrid_metrics\n",
    "    \n",
    "    def update_alpha_values(self, new_ratings, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Update alpha values based on new user ratings\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        new_ratings: DataFrame\n",
    "            New ratings data with userId, movieId, rating columns\n",
    "        learning_rate: float\n",
    "            Learning rate for updates\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Updated alpha values\n",
    "        \"\"\"\n",
    "        print(\"Updating alpha values with new ratings data...\")\n",
    "        \n",
    "        if not self.optimal_alphas:\n",
    "            print(\"No optimal alpha values to update\")\n",
    "            return None\n",
    "        \n",
    "        # Group new ratings by user\n",
    "        user_ratings = new_ratings.groupby('userId')\n",
    "        \n",
    "        for user_id, ratings in user_ratings:\n",
    "            # Get current alpha for this user\n",
    "            current_alpha = self.get_adaptive_alpha(user_id)\n",
    "            \n",
    "            # Calculate error for content-based and collaborative predictions\n",
    "            content_errors = []\n",
    "            collab_errors = []\n",
    "            \n",
    "            for _, row in ratings.iterrows():\n",
    "                movie_id = row['movieId']\n",
    "                true_rating = row['rating']\n",
    "                \n",
    "                # Get content-based prediction if available\n",
    "                content_pred = None\n",
    "                if user_id in self.data.get('content_recommendations', {}) and movie_id in [m for m, _ in self.data['content_recommendations'][user_id]]:\n",
    "                    for m, s in self.data['content_recommendations'][user_id]:\n",
    "                        if m == movie_id:\n",
    "                            content_pred = self.denormalize_prediction(s)\n",
    "                            break\n",
    "                \n",
    "                # Get collaborative prediction if available\n",
    "                collab_pred = None\n",
    "                if user_id in self.data.get('collaborative_recommendations', {}) and movie_id in [m for m, _ in self.data['collaborative_recommendations'][user_id]]:\n",
    "                    for m, r in self.data['collaborative_recommendations'][user_id]:\n",
    "                        if m == movie_id:\n",
    "                            collab_pred = r\n",
    "                            break\n",
    "                \n",
    "                # Calculate errors if both predictions are available\n",
    "                if content_pred is not None and collab_pred is not None:\n",
    "                    content_error = (content_pred - true_rating) ** 2\n",
    "                    collab_error = (collab_pred - true_rating) ** 2\n",
    "                    \n",
    "                    content_errors.append(content_error)\n",
    "                    collab_errors.append(collab_error)\n",
    "            \n",
    "            # Update alpha if we have errors\n",
    "            if content_errors and collab_errors:\n",
    "                avg_content_error = np.mean(content_errors)\n",
    "                avg_collab_error = np.mean(collab_errors)\n",
    "                \n",
    "                # Adjust alpha based on relative performance\n",
    "                # If content error is higher, decrease alpha; if collab error is higher, increase alpha\n",
    "                error_diff = avg_content_error - avg_collab_error\n",
    "                delta_alpha = learning_rate * error_diff\n",
    "                \n",
    "                new_alpha = current_alpha - delta_alpha\n",
    "                new_alpha = max(0.0, min(1.0, new_alpha))  # Clip to [0, 1]\n",
    "                \n",
    "                # Find appropriate bin for this user\n",
    "                rating_count = 0\n",
    "                if 'user_rating_counts' in self.data:\n",
    "                    user_data = self.data['user_rating_counts'][self.data['user_rating_counts']['userId'] == user_id]\n",
    "                    if not user_data.empty:\n",
    "                        rating_count = user_data.iloc[0]['rating_count']\n",
    "                \n",
    "                for bin_name in self.optimal_alphas.keys():\n",
    "                    if '-' in bin_name:\n",
    "                        lower, upper = map(int, bin_name.split('-'))\n",
    "                        if lower <= rating_count <= upper:\n",
    "                            # Weighted update of bin alpha\n",
    "                            self.optimal_alphas[bin_name] = 0.9 * self.optimal_alphas[bin_name] + 0.1 * new_alpha\n",
    "                            break\n",
    "                    elif '+' in bin_name:\n",
    "                        threshold = int(bin_name.replace('+', ''))\n",
    "                        if rating_count >= threshold:\n",
    "                            self.optimal_alphas[bin_name] = 0.9 * self.optimal_alphas[bin_name] + 0.1 * new_alpha\n",
    "                            break\n",
    "        \n",
    "        # Save updated alpha values\n",
    "        with open(os.path.join(self.output_path, 'optimal_alphas.json'), 'w') as f:\n",
    "            json.dump(self.optimal_alphas, f, indent=2)\n",
    "        \n",
    "        print(f\"Updated alpha values saved to {os.path.join(self.output_path, 'optimal_alphas.json')}\")\n",
    "        \n",
    "        return self.optimal_alphas\n",
    "    \n",
    "    def recommend_for_user(self, user_id, n=10, use_adaptive_alpha=True):\n",
    "        \"\"\"\n",
    "        Get recommendations for a specific user\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        user_id: int\n",
    "            User ID\n",
    "        n: int\n",
    "            Number of recommendations to return\n",
    "        use_adaptive_alpha: bool\n",
    "            Whether to use adaptive alpha based on user rating count\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list\n",
    "            List of (movie_id, title, score) tuples\n",
    "        \"\"\"\n",
    "        # Check if user has recommendations\n",
    "        if 'combined_recommendations' not in self.data or user_id not in self.data['combined_recommendations']:\n",
    "            print(f\"No pre-computed recommendations found for user {user_id}\")\n",
    "            \n",
    "            # Look for individual model recommendations\n",
    "            content_recs = []\n",
    "            if 'content_recommendations' in self.data and user_id in self.data['content_recommendations']:\n",
    "                content_recs = self.data['content_recommendations'][user_id]\n",
    "            \n",
    "            collab_recs = []\n",
    "            if 'collaborative_recommendations' in self.data and user_id in self.data['collaborative_recommendations']:\n",
    "                collab_recs = self.data['collaborative_recommendations'][user_id]\n",
    "            \n",
    "            if not content_recs and not collab_recs:\n",
    "                print(f\"No recommendations available for user {user_id}\")\n",
    "                return []\n",
    "            \n",
    "            # Get alpha for this user\n",
    "            alpha = self.get_adaptive_alpha(user_id) if use_adaptive_alpha else self.alpha\n",
    "            \n",
    "            # Combine available recommendations\n",
    "            combined_recs = {}\n",
    "            \n",
    "            # Add content-based recs\n",
    "            for movie_id, score in content_recs:\n",
    "                combined_recs[movie_id] = {\"content_score\": score, \"has_content\": True}\n",
    "            \n",
    "            # Add collaborative recs\n",
    "            for movie_id, rating in collab_recs:\n",
    "                collab_score = self.normalize_prediction(rating)\n",
    "                if movie_id in combined_recs:\n",
    "                    combined_recs[movie_id][\"collab_score\"] = collab_score\n",
    "                    combined_recs[movie_id][\"has_collab\"] = True\n",
    "                else:\n",
    "                    combined_recs[movie_id] = {\"collab_score\": collab_score, \"has_collab\": True}\n",
    "            \n",
    "            # Calculate final scores\n",
    "            recommendations = []\n",
    "            for movie_id, data in combined_recs.items():\n",
    "                if data.get(\"has_content\", False) and data.get(\"has_collab\", False):\n",
    "                    combined_score = alpha * data[\"content_score\"] + (1 - alpha) * data[\"collab_score\"]\n",
    "                elif data.get(\"has_content\", False):\n",
    "                    combined_score = data[\"content_score\"]\n",
    "                elif data.get(\"has_collab\", False):\n",
    "                    combined_score = data[\"collab_score\"]\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                final_rating = self.denormalize_prediction(combined_score)\n",
    "                recommendations.append((movie_id, final_rating))\n",
    "            \n",
    "            # Sort and get top-n\n",
    "            recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "            recommendations = recommendations[:n]\n",
    "        else:\n",
    "            # Use pre-computed recommendations\n",
    "            recommendations = self.data['combined_recommendations'][user_id][:n]\n",
    "        \n",
    "        # Format recommendations with titles\n",
    "        formatted_recs = []\n",
    "        for movie_id, score in recommendations:\n",
    "            title = \"Unknown\"\n",
    "            if 'movie_features' in self.data:\n",
    "                movie_row = self.data['movie_features'][self.data['movie_features']['movieId'] == movie_id]\n",
    "                if not movie_row.empty and 'title' in movie_row.columns:\n",
    "                    title = movie_row.iloc[0]['title']\n",
    "            \n",
    "            formatted_recs.append((movie_id, title, score))\n",
    "        \n",
    "        return formatted_recs\n",
    "\n",
    "def main():\n",
    "    # Configuration section\n",
    "    content_path = \"./rec/content-recommendations\"\n",
    "    collab_path = \"./rec/collaborative-recommendations\"\n",
    "    output_path = \"./rec/hybrid_recommendations\"\n",
    "    alpha = 0.3\n",
    "    learn_alpha = True  # Enable alpha learning\n",
    "    adaptive_alpha = True\n",
    "    num_recs = 10\n",
    "    \n",
    "    # Custom rating bins for learning\n",
    "    rating_bins = [0, 10, 25, 50, 100, 150, 200, 300]\n",
    "    \n",
    "    # Create and initialize the hybrid recommender\n",
    "    recommender = HybridRecommender(\n",
    "        content_model_path=content_path,\n",
    "        collab_model_path=collab_path,\n",
    "        output_path=output_path,\n",
    "        alpha=alpha\n",
    "    )\n",
    "\n",
    "    # Load data\n",
    "    recommender.load_data()\n",
    "\n",
    "    # Learn optimal alpha values if requested\n",
    "    if learn_alpha:\n",
    "        recommender.learn_optimal_alpha_values(rating_bins=rating_bins)\n",
    "\n",
    "    # Combine recommendations with learned alphas\n",
    "    recommender.combine_recommendations(top_n=num_recs, use_adaptive_alpha=adaptive_alpha)\n",
    "\n",
    "    # Evaluate \n",
    "    evaluation_metrics = recommender.evaluate(use_adaptive_alpha=adaptive_alpha)\n",
    "\n",
    "    # Compare with individual models\n",
    "    print(\"\\nModel Performance Comparison:\")\n",
    "    headers = [\"Model\", \"RMSE\", \"MAE\", \"Predictions\"]\n",
    "    rows = []\n",
    "\n",
    "    # Content-based model metrics\n",
    "    if 'content_evaluation' in recommender.data:\n",
    "        rows.append([\n",
    "            \"Content-Based\",\n",
    "            f\"{recommender.data['content_evaluation']['rmse']:.4f}\",\n",
    "            f\"{recommender.data['content_evaluation'].get('mae', 'N/A')}\",\n",
    "            f\"{recommender.data['content_evaluation'].get('num_predictions', 'N/A')}\"\n",
    "        ])\n",
    "\n",
    "    # Collaborative filtering model metrics\n",
    "    if 'dnn_evaluation' in recommender.data:\n",
    "        rows.append([\n",
    "            \"Collaborative\",\n",
    "            f\"{recommender.data['dnn_evaluation']['rmse']:.4f}\",\n",
    "            f\"{recommender.data['dnn_evaluation']['mae']:.4f}\",\n",
    "            f\"{recommender.data['dnn_evaluation']['num_predictions']}\"\n",
    "        ])\n",
    "\n",
    "    # Hybrid model metrics\n",
    "    if evaluation_metrics:\n",
    "        alpha_desc = \"Adaptive (Learned)\" if adaptive_alpha else f\"={recommender.alpha:.2f}\"\n",
    "        rows.append([\n",
    "            f\"Hybrid ({alpha_desc})\",\n",
    "            f\"{evaluation_metrics['rmse']:.4f}\",\n",
    "            f\"{evaluation_metrics['mae']:.4f}\",\n",
    "            f\"{evaluation_metrics['num_predictions']}\"\n",
    "        ])\n",
    "\n",
    "    # Print table\n",
    "    if rows:\n",
    "        # Calculate column widths\n",
    "        col_widths = [max(len(row[i]) for row in [headers] + rows) for i in range(len(headers))]\n",
    "        \n",
    "        # Print table header\n",
    "        print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")\n",
    "        print(\"| \" + \" | \".join(headers[i].ljust(col_widths[i]) for i in range(len(headers))) + \" |\")\n",
    "        print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")\n",
    "        \n",
    "        # Print table rows\n",
    "        for row in rows:\n",
    "            print(\"| \" + \" | \".join(row[i].ljust(col_widths[i]) for i in range(len(row))) + \" |\")\n",
    "        \n",
    "        print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")\n",
    "\n",
    "    # Show sample recommendations\n",
    "    if 'combined_recommendations' in recommender.data:\n",
    "        # Get a sample user ID\n",
    "        sample_user_id = next(iter(recommender.data['combined_recommendations'].keys()))\n",
    "        \n",
    "        print(f\"\\nSample recommendations for user {sample_user_id}:\")\n",
    "        recs = recommender.recommend_for_user(sample_user_id, n=5)\n",
    "        \n",
    "        for i, (movie_id, title, score) in enumerate(recs, 1):\n",
    "            print(f\"{i}. {title} (ID: {movie_id}) - Score: {score:.2f}\")\n",
    "    \n",
    "    # Create visualizations of results\n",
    "    if 'combined_recommendations' in recommender.data:\n",
    "        # Create visualization of recommendation scores\n",
    "        scores = []\n",
    "        for user_id, recs in recommender.data['combined_recommendations'].items():\n",
    "            for _, score in recs:\n",
    "                scores.append(score)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(scores, bins=20, alpha=0.7)\n",
    "        plt.title('Distribution of Recommendation Scores')\n",
    "        plt.xlabel('Score')\n",
    "        plt.ylabel('Count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig(os.path.join(output_path, 'recommendation_scores.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Visualization saved to {os.path.join(output_path, 'recommendation_scores.png')}\")\n",
    "    \n",
    "    print(\"\\nHybrid Recommendation System with Learning completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
