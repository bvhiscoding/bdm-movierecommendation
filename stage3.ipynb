{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: Using NVIDIA GeForce RTX 2060 SUPER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--content_path CONTENT_PATH]\n",
      "                             [--collab_path COLLAB_PATH]\n",
      "                             [--output_path OUTPUT_PATH] [--alpha ALPHA]\n",
      "                             [--optimize_alpha] [--adaptive_alpha]\n",
      "                             [--batch_mode] [--num_recs NUM_RECS] [--generate]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\NCPC\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-29444Uk6gxJEzUdj6.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import logging\n",
    "import argparse\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check for CUDA availability\n",
    "cuda_available = torch.cuda.is_available()\n",
    "if cuda_available:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"CUDA is available: Using {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available: Using CPU instead\")\n",
    "\n",
    "class HybridRecommender:\n",
    "    def predict_rating(self, user_id, movie_id):\n",
    "        \"\"\"\n",
    "        Predict a user's rating for a movie using content-based approach\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        user_id: int\n",
    "            User ID\n",
    "        movie_id: int\n",
    "            Movie ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Predicted rating (0.5-5.0 scale)\n",
    "        \"\"\"\n",
    "        # If using user-movie similarities\n",
    "        if 'user_movie_similarities' in self.data and user_id in self.data['user_movie_similarities']:\n",
    "            user_sims = self.data['user_movie_similarities'][user_id]\n",
    "            \n",
    "            # If movie is in similarities\n",
    "            if movie_id in user_sims:\n",
    "                # Convert similarity (0-1) to rating (0.5-5.0)\n",
    "                sim_score = user_sims[movie_id]\n",
    "                return 0.5 + 4.5 * sim_score\n",
    "        \n",
    "        # Get user's average rating from training data\n",
    "        if 'train_ratings' in self.data:\n",
    "            user_ratings = self.data['train_ratings'][self.data['train_ratings']['userId'] == user_id]\n",
    "            if len(user_ratings) > 0:\n",
    "                return user_ratings['rating'].mean()\n",
    "        \n",
    "        # Default to mid-point if no other information\n",
    "        return 3.0\n",
    "    def get_adaptive_alpha(self, user_id):\n",
    "        # Get user's ratings count\n",
    "        user_ratings = self.data['train_ratings'][self.data['train_ratings']['userId'] == user_id]\n",
    "        rating_count = len(user_ratings)\n",
    "        \n",
    "        # Map rating count to appropriate alpha\n",
    "        alpha_mapping = [\n",
    "            (10, 0.2),     # Users with <= 10 ratings: heavy weight on collaborative (0.2)\n",
    "            (25, 0.3),     # Users with 11-25 ratings: more weight on collaborative (0.3)\n",
    "            (50, 0.5),     # Users with 26-50 ratings: balanced (0.5) \n",
    "            (100, 0.7),    # Users with 51-100 ratings: more weight on content-based (0.7)\n",
    "            (float('inf'), 0.8)  # Users with >100 ratings: heavy weight on content-based (0.8)\n",
    "        ]\n",
    "        \n",
    "        # Find appropriate alpha\n",
    "        for threshold, alpha in alpha_mapping:\n",
    "            if rating_count <= threshold:\n",
    "                return alpha\n",
    "        \n",
    "        return self.alpha  # Fall back to default\n",
    "    def predict_rating_collaborative(self, user_id, movie_id):\n",
    "        \"\"\"\n",
    "        Predict a user's rating for a movie using DNN\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        user_id: int\n",
    "            User ID\n",
    "        movie_id: int\n",
    "            Movie ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Predicted rating (0.5-5.0 scale)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if we have the necessary components\n",
    "            if 'dnn_model' in self.data and 'user_genre_preferences' in self.data and 'movie_genre_features' in self.data:\n",
    "                # Get user preferences\n",
    "                user_prefs = self.data['user_genre_preferences'][self.data['user_genre_preferences']['userId'] == user_id]\n",
    "                if user_prefs.empty:\n",
    "                    return 3.0  # Default if user not found\n",
    "                \n",
    "                # Get movie genres\n",
    "                movie_genres = self.data['movie_genre_features'][self.data['movie_genre_features']['movieId'] == movie_id]\n",
    "                if movie_genres.empty:\n",
    "                    return 3.0  # Default if movie not found\n",
    "                \n",
    "                # Create feature vector\n",
    "                genre_columns = [col for col in self.data['movie_genre_features'].columns if col != 'movieId']\n",
    "                feature_vector = []\n",
    "                \n",
    "                for genre in genre_columns:\n",
    "                    feature_vector.append(user_prefs.iloc[0][genre])\n",
    "                    feature_vector.append(movie_genres.iloc[0][genre])\n",
    "                \n",
    "                # Reshape for prediction\n",
    "                feature_vector = np.array([feature_vector])\n",
    "                \n",
    "                # Predict movie rating\n",
    "                predicted_rating = self.data['dnn_model'].predict(feature_vector, verbose=0)[0][0]\n",
    "                \n",
    "                # Ensure rating is within bounds\n",
    "                return max(0.5, min(5.0, predicted_rating))\n",
    "        except Exception as e:\n",
    "            print(f\"Error in collaborative prediction: {str(e)}\")\n",
    "        \n",
    "        # Default to mid-point if prediction fails\n",
    "        return 3.0\n",
    "\n",
    "    def __init__(self, content_model_path=\"./content-recommendations\", \n",
    "                 collab_model_path=\"./recommendations\", \n",
    "                 output_path=\"./hybrid_recommendations\", \n",
    "                 alpha=0.3):\n",
    "        \"\"\"\n",
    "        Initialize the hybrid recommender with paths to content-based and collaborative filtering models\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        content_model_path: str\n",
    "            Path to the directory containing content-based model files\n",
    "        collab_model_path: str\n",
    "            Path to the directory containing collaborative filtering model files\n",
    "        output_path: str\n",
    "            Path to save hybrid recommendation results\n",
    "        alpha: float\n",
    "            Weight for content-based recommendations (1-alpha for collaborative)\n",
    "        \"\"\"\n",
    "        self.content_model_path = content_model_path\n",
    "        self.collab_model_path = collab_model_path\n",
    "        self.output_path = output_path\n",
    "        self.alpha = alpha\n",
    "        self.data = {}  # Container for all loaded data\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"HYBRID MOVIE RECOMMENDATION SYSTEM (alpha={self.alpha:.2f})\")\n",
    "        print(\"=\"*80)  \n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load movie data, content-based and collaborative filtering model outputs\"\"\"\n",
    "        print(\"\\nLoading data...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Load movie features\n",
    "        try:\n",
    "            movie_features_path = './processed/processed_movie_features.csv'\n",
    "            self.data['movie_features'] = pd.read_csv(movie_features_path)\n",
    "            # Convert string representation of tokens and top_keywords back to lists\n",
    "            if 'tokens' in self.data['movie_features'].columns:\n",
    "                self.data['movie_features']['tokens'] = self.data['movie_features']['tokens'].apply(\n",
    "                    lambda x: eval(x) if isinstance(x, str) else []\n",
    "                )\n",
    "            if 'top_keywords' in self.data['movie_features'].columns:\n",
    "                self.data['movie_features']['top_keywords'] = self.data['movie_features']['top_keywords'].apply(\n",
    "                    lambda x: eval(x) if isinstance(x, str) else []\n",
    "                )\n",
    "            print(f\"Loaded features for {len(self.data['movie_features'])} movies\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading movie features: {str(e)}\")\n",
    "        \n",
    "        # Load normalized ratings\n",
    "        try:\n",
    "            ratings_path = './processed/normalized_ratings.csv'\n",
    "            self.data['ratings'] = pd.read_csv(ratings_path)\n",
    "            print(f\"Loaded {len(self.data['ratings'])} ratings\")\n",
    "            \n",
    "            # Create training and testing sets with 80-20 split\n",
    "            user_groups = self.data['ratings'].groupby('userId')\n",
    "            train_data = []\n",
    "            test_data = []\n",
    "            \n",
    "            for _, group in user_groups:\n",
    "                n = len(group)\n",
    "                split_idx = int(n * 0.8)\n",
    "                train_data.append(group.iloc[:split_idx])\n",
    "                test_data.append(group.iloc[split_idx:])\n",
    "            \n",
    "            self.data['train_ratings'] = pd.concat(train_data).reset_index(drop=True)\n",
    "            self.data['test_ratings'] = pd.concat(test_data).reset_index(drop=True)\n",
    "            \n",
    "            print(f\"Split into {len(self.data['train_ratings'])} training and {len(self.data['test_ratings'])} testing ratings\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading ratings data: {str(e)}\")\n",
    "            \n",
    "        # Load content-based model components\n",
    "        try:\n",
    "            # Load content-based recommendations\n",
    "            content_recs_path = os.path.join(self.content_model_path, 'content_based_recommendations.pkl')\n",
    "            if os.path.exists(content_recs_path):\n",
    "                with open(content_recs_path, 'rb') as f:\n",
    "                    self.data['content_recommendations'] = pickle.load(f)\n",
    "                print(f\"Loaded content-based recommendations for {len(self.data['content_recommendations'])} users\")\n",
    "            \n",
    "            # Load movie vectors - from non-CUDA version\n",
    "            movie_vectors_path = os.path.join(self.content_model_path, 'movie_vectors.pkl')\n",
    "            if os.path.exists(movie_vectors_path):\n",
    "                with open(movie_vectors_path, 'rb') as f:\n",
    "                    self.data['movie_vectors'] = pickle.load(f)\n",
    "                print(f\"Loaded content-based vectors for {len(self.data['movie_vectors'])} movies\")\n",
    "            \n",
    "            # Load user vectors \n",
    "            user_vectors_path = os.path.join(self.content_model_path, 'user_vectors.pkl')\n",
    "            if os.path.exists(user_vectors_path):\n",
    "                with open(user_vectors_path, 'rb') as f:\n",
    "                    self.data['user_vectors'] = pickle.load(f)\n",
    "                print(f\"Loaded content-based vectors for {len(self.data['user_vectors'])} users\")\n",
    "            \n",
    "            # Load user-movie similarities\n",
    "            similarities_path = os.path.join(self.content_model_path, 'user_movie_similarities.pkl')\n",
    "            if os.path.exists(similarities_path):\n",
    "                with open(similarities_path, 'rb') as f:\n",
    "                    self.data['user_movie_similarities'] = pickle.load(f)\n",
    "                print(f\"Loaded user-movie similarities for {len(self.data['user_movie_similarities'])} users\")\n",
    "            \n",
    "            # Load content-based evaluation metrics\n",
    "            try:\n",
    "                content_eval_path = os.path.join(self.content_model_path, 'content_based_evaluation.csv')\n",
    "                if os.path.exists(content_eval_path):\n",
    "                    content_eval_df = pd.read_csv(content_eval_path)\n",
    "                    if not content_eval_df.empty:\n",
    "                        self.data['content_evaluation'] = {\n",
    "                            'rmse': content_eval_df.iloc[0]['rmse'],\n",
    "                            'mae': content_eval_df.iloc[0]['mae'] if 'mae' in content_eval_df.columns else None,\n",
    "                            'num_predictions': content_eval_df.iloc[0]['num_predictions'] if 'num_predictions' in content_eval_df.columns else None\n",
    "                        }\n",
    "                        print(f\"Loaded content-based evaluation metrics: RMSE={self.data['content_evaluation']['rmse']:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading content-based evaluation metrics: {str(e)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading content-based model components: {str(e)}\")\n",
    "\n",
    "        # Load collaborative filtering model components\n",
    "        try:\n",
    "            # Load collaborative filtering recommendations\n",
    "            collab_recs_path = os.path.join(self.collab_model_path, 'dnn_recommendations.pkl')\n",
    "            if os.path.exists(collab_recs_path):\n",
    "                with open(collab_recs_path, 'rb') as f:\n",
    "                    self.data['collaborative_recommendations'] = pickle.load(f)\n",
    "                print(f\"Loaded collaborative filtering recommendations for {len(self.data['collaborative_recommendations'])} users\")\n",
    "            \n",
    "            # Load DNN evaluation metrics\n",
    "            dnn_eval_path = os.path.join(self.collab_model_path, 'dnn_evaluation.csv')\n",
    "            if os.path.exists(dnn_eval_path):\n",
    "                dnn_eval_df = pd.read_csv(dnn_eval_path)\n",
    "                if not dnn_eval_df.empty:\n",
    "                    self.data['dnn_evaluation'] = {\n",
    "                        'rmse': dnn_eval_df.iloc[0]['rmse'],\n",
    "                        'mae': dnn_eval_df.iloc[0]['mae'],\n",
    "                        'num_predictions': dnn_eval_df.iloc[0]['num_predictions']\n",
    "                    }\n",
    "                    print(f\"Loaded DNN evaluation metrics: RMSE={self.data['dnn_evaluation']['rmse']:.4f}\")\n",
    "            \n",
    "            # Load user genre preferences for DNN\n",
    "            user_prefs_path = os.path.join(self.collab_model_path, 'user_genre_preferences.csv')\n",
    "            if os.path.exists(user_prefs_path):\n",
    "                self.data['user_genre_preferences'] = pd.read_csv(user_prefs_path)\n",
    "                print(f\"Loaded user genre preferences for {len(self.data['user_genre_preferences'])} users\")\n",
    "            \n",
    "            # Load movie genre features for DNN\n",
    "            movie_genre_path = os.path.join(self.collab_model_path, 'movie_genre_features.csv')\n",
    "            if os.path.exists(movie_genre_path):\n",
    "                self.data['movie_genre_features'] = pd.read_csv(movie_genre_path)\n",
    "                print(f\"Loaded movie genre features for {len(self.data['movie_genre_features'])} movies\")\n",
    "            \n",
    "            # Try to load DNN model if TensorFlow is available\n",
    "            try:\n",
    "                import tensorflow as tf\n",
    "                dnn_model_path = os.path.join(self.collab_model_path, 'dnn_model.h5')\n",
    "                if os.path.exists(dnn_model_path):\n",
    "                    self.data['dnn_model'] = tf.keras.models.load_model(dnn_model_path)\n",
    "                    print(f\"Loaded DNN model from {dnn_model_path}\")\n",
    "            except ImportError:\n",
    "                print(\"TensorFlow not available - skipping DNN model loading\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading collaborative filtering model components: {str(e)}\")\n",
    "        \n",
    "        print(f\"Data loading completed in {time.time() - start_time:.2f}s\")\n",
    "        \n",
    "        # Get common users\n",
    "        self.common_users = set()\n",
    "        if 'content_recommendations' in self.data and 'collaborative_recommendations' in self.data:\n",
    "            self.common_users = set(self.data['content_recommendations'].keys()) & set(self.data['collaborative_recommendations'].keys())\n",
    "            print(f\"Found {len(self.common_users)} users with both content-based and collaborative recommendations\")\n",
    "        \n",
    "        return self.data\n",
    "    \n",
    "    def normalize_prediction(self, prediction):\n",
    "        \"\"\"\n",
    "        Normalize a prediction to the 0-1 range\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        prediction: float\n",
    "            Prediction value in the 0.5-5.0 range\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Normalized prediction in the 0-1 range\n",
    "        \"\"\"\n",
    "        # Normalize from rating scale [0.5, 5.0] to [0, 1]\n",
    "        return (prediction - 0.5) / 4.5\n",
    "    \n",
    "    def denormalize_prediction(self, normalized_prediction):\n",
    "        \"\"\"\n",
    "        Convert a normalized prediction back to the 0.5-5.0 range\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        normalized_prediction: float\n",
    "            Normalized prediction in the 0-1 range\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Prediction value in the 0.5-5.0 range\n",
    "        \"\"\"\n",
    "        # Convert from [0, 1] back to rating scale [0.5, 5.0]\n",
    "        return 0.5 + 4.5 * normalized_prediction\n",
    "    \n",
    "    def combine_recommendations(self, top_n=10, use_adaptive_alpha=True):\n",
    "        \"\"\"\n",
    "        Combine content-based and collaborative filtering recommendations with weighting\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        top_n: int\n",
    "            Number of recommendations to generate per user\n",
    "        use_adaptive_alpha: bool\n",
    "            Whether to use adaptive alpha based on user rating count\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            User ID to list of (movie_id, score) tuples\n",
    "        \"\"\"\n",
    "        print(f\"\\nCombining recommendations{' with adaptive alpha' if use_adaptive_alpha else f' with alpha={self.alpha:.2f}'}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get recommendations from both models\n",
    "        content_recs = self.data.get('content_recommendations', {})\n",
    "        collab_recs = self.data.get('collaborative_recommendations', {})\n",
    "        \n",
    "        if not content_recs:\n",
    "            print(\"Warning: No content-based recommendations available\")\n",
    "        \n",
    "        if not collab_recs:\n",
    "            print(\"Warning: No collaborative filtering recommendations available\")\n",
    "        \n",
    "        if not content_recs and not collab_recs:\n",
    "            print(\"Error: No recommendations available from either model\")\n",
    "            return {}\n",
    "        \n",
    "        # Combine recommendations\n",
    "        combined_recommendations = {}\n",
    "        alpha_stats = {'values': [], 'count_categories': {}}\n",
    "        \n",
    "        # Get all users from both recommendation sets\n",
    "        all_users = set(content_recs.keys()) | set(collab_recs.keys())\n",
    "        total_users = len(all_users)\n",
    "        \n",
    "        for i, user_id in enumerate(all_users):\n",
    "            # Get appropriate alpha value for this user\n",
    "            if use_adaptive_alpha:\n",
    "                alpha = self.get_adaptive_alpha(user_id)\n",
    "                # Track alpha statistics\n",
    "                alpha_stats['values'].append(alpha)\n",
    "                rating_count = len(self.data['train_ratings'][self.data['train_ratings']['userId'] == user_id])\n",
    "                count_category = \"<=10\" if rating_count <= 25 else \"11-25\" if rating_count <= 50 else \"26-50\" if rating_count <= 100 else \"51-100\" if rating_count <= 150 else \">100\"\n",
    "                if count_category in alpha_stats['count_categories']:\n",
    "                    alpha_stats['count_categories'][count_category]['count'] += 1\n",
    "                    alpha_stats['count_categories'][count_category]['alpha_sum'] += alpha\n",
    "                else:\n",
    "                    alpha_stats['count_categories'][count_category] = {'count': 1, 'alpha_sum': alpha}\n",
    "            else:\n",
    "                alpha = self.alpha\n",
    "            \n",
    "            # Initialize combined recommendations dictionary for this user\n",
    "            user_combined_recs = {}\n",
    "            \n",
    "            # Add content-based recommendations if available\n",
    "            if user_id in content_recs:\n",
    "                for movie_id, score in content_recs[user_id]:\n",
    "                    # Scores from content-based are already normalized (0-1), just store them\n",
    "                    user_combined_recs[movie_id] = {'content_score': score, 'content_available': True}\n",
    "            \n",
    "            # Add collaborative filtering recommendations if available\n",
    "            if user_id in collab_recs:\n",
    "                for movie_id, rating in collab_recs[user_id]:\n",
    "                    # Normalize the collaborative rating to 0-1 scale\n",
    "                    collab_score = self.normalize_prediction(rating)\n",
    "                    \n",
    "                    if movie_id in user_combined_recs:\n",
    "                        user_combined_recs[movie_id]['collab_score'] = collab_score\n",
    "                        user_combined_recs[movie_id]['collab_available'] = True\n",
    "                    else:\n",
    "                        user_combined_recs[movie_id] = {\n",
    "                            'collab_score': collab_score, \n",
    "                            'collab_available': True,\n",
    "                            'content_available': False\n",
    "                        }\n",
    "            \n",
    "            # Calculate final scores with proper normalization\n",
    "            final_recommendations = []\n",
    "            for movie_id, data in user_combined_recs.items():\n",
    "                # Check which models provided predictions\n",
    "                content_available = data.get('content_available', False)\n",
    "                collab_available = data.get('collab_available', False)\n",
    "                \n",
    "                if content_available and collab_available:\n",
    "                    # We have both predictions, use the weighted average\n",
    "                    content_score = data['content_score']\n",
    "                    collab_score = data['collab_score']\n",
    "                    combined_score = alpha * content_score + (1 - alpha) * collab_score\n",
    "                elif content_available:\n",
    "                    # Only content-based prediction available\n",
    "                    combined_score = data['content_score']\n",
    "                elif collab_available:\n",
    "                    # Only collaborative prediction available\n",
    "                    combined_score = data['collab_score']\n",
    "                \n",
    "                # Convert back to rating scale for storage\n",
    "                final_rating = self.denormalize_prediction(combined_score)\n",
    "                final_recommendations.append((movie_id, final_rating))\n",
    "            \n",
    "            # Sort by final score and limit to top_n\n",
    "            final_recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "            combined_recommendations[user_id] = final_recommendations[:top_n]\n",
    "            \n",
    "            # Log progress\n",
    "            if (i+1) % 1000 == 0 or (i+1) == total_users:\n",
    "                print(f\"Processed {i+1}/{total_users} users ({(i+1)/total_users*100:.1f}%)\")\n",
    "        \n",
    "        self.data['combined_recommendations'] = combined_recommendations\n",
    "        \n",
    "        # Print alpha statistics if using adaptive alpha\n",
    "        if use_adaptive_alpha and alpha_stats['values']:\n",
    "            print(\"\\nAdaptive Alpha Statistics:\")\n",
    "            print(f\"Average alpha: {np.mean(alpha_stats['values']):.4f}\")\n",
    "            print(f\"Min alpha: {min(alpha_stats['values']):.4f}, Max alpha: {max(alpha_stats['values']):.4f}\")\n",
    "            print(\"\\nAlpha by user rating count:\")\n",
    "            for category, stats in sorted(alpha_stats['count_categories'].items(), \n",
    "                                         key=lambda x: (int(x[0].replace('<=', '').replace('>', '').split('-')[0]) \n",
    "                                                       if x[0] not in ['>100'] else float('inf'))):\n",
    "                avg_alpha = stats['alpha_sum'] / stats['count']\n",
    "                print(f\"  {category} ratings: {stats['count']} users, avg alpha = {avg_alpha:.4f}\")\n",
    "            \n",
    "            # Save alpha statistics to a file\n",
    "            with open(os.path.join(self.output_path, 'alpha_stats.txt'), 'w') as f:\n",
    "                f.write(f\"Adaptive Alpha Statistics:\\n\")\n",
    "                f.write(f\"Average alpha: {np.mean(alpha_stats['values']):.4f}\\n\")\n",
    "                f.write(f\"Min alpha: {min(alpha_stats['values']):.4f}, Max alpha: {max(alpha_stats['values']):.4f}\\n\\n\")\n",
    "                f.write(\"Alpha by user rating count:\\n\")\n",
    "                for category, stats in sorted(alpha_stats['count_categories'].items(), \n",
    "                                            key=lambda x: (int(x[0].replace('<=', '').replace('>', '').split('-')[0]) \n",
    "                                                          if x[0] not in ['>100'] else float('inf'))):\n",
    "                    avg_alpha = stats['alpha_sum'] / stats['count']\n",
    "                    f.write(f\"  {category} ratings: {stats['count']} users, avg alpha = {avg_alpha:.4f}\\n\")\n",
    "        \n",
    "        print(f\"Combined recommendations for {len(combined_recommendations)} users in {time.time() - start_time:.2f}s\")\n",
    "        \n",
    "        # Save combined recommendations\n",
    "        with open(os.path.join(self.output_path, 'combined_recommendations.pkl'), 'wb') as f:\n",
    "            pickle.dump(combined_recommendations, f)\n",
    "        \n",
    "        # Also save in a more readable CSV format\n",
    "        recommendations_list = []\n",
    "        \n",
    "        for user_id, recs in combined_recommendations.items():\n",
    "            # Get user's alpha\n",
    "            if use_adaptive_alpha:\n",
    "                user_alpha = self.get_adaptive_alpha(user_id)\n",
    "            else:\n",
    "                user_alpha = self.alpha\n",
    "                \n",
    "            # Get user's rating count\n",
    "            rating_count = len(self.data['train_ratings'][self.data['train_ratings']['userId'] == user_id])\n",
    "            \n",
    "            for rank, (movie_id, score) in enumerate(recs, 1):\n",
    "                movie_title = \"Unknown\"\n",
    "                if 'movie_features' in self.data:\n",
    "                    movie_row = self.data['movie_features'][self.data['movie_features']['movieId'] == movie_id]\n",
    "                    if not movie_row.empty and 'title' in movie_row.columns:\n",
    "                        movie_title = movie_row.iloc[0]['title']\n",
    "                \n",
    "                recommendations_list.append({\n",
    "                    'userId': user_id,\n",
    "                    'movieId': movie_id,\n",
    "                    'title': movie_title,\n",
    "                    'rank': rank,\n",
    "                    'score': score,\n",
    "                    'alpha': user_alpha,\n",
    "                    'rating_count': rating_count\n",
    "                })\n",
    "        \n",
    "        if recommendations_list:\n",
    "            recommendations_df = pd.DataFrame(recommendations_list)\n",
    "            recommendations_df.to_csv(os.path.join(self.output_path, 'combined_recommendations.csv'), index=False)\n",
    "            print(f\"Saved combined recommendations to CSV with {len(recommendations_df)} entries\")\n",
    "        \n",
    "        return combined_recommendations\n",
    "    \n",
    "    def evaluate(self, use_adaptive_alpha=True):\n",
    "        \"\"\"\n",
    "        Evaluate the hybrid recommendation system using RMSE and MAE\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        use_adaptive_alpha: bool\n",
    "            Whether to use adaptive alpha based on user rating count\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Evaluation metrics\n",
    "        \"\"\"\n",
    "        print(f\"\\nEvaluating hybrid recommendation system {' with adaptive alpha' if use_adaptive_alpha else ''}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get test ratings\n",
    "        test_ratings = self.data.get('test_ratings')\n",
    "        \n",
    "        if test_ratings is None:\n",
    "            print(\"Error: No test ratings available for evaluation\")\n",
    "            return None\n",
    "        \n",
    "        # Initialize containers for predictions and actual ratings\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        errors = []\n",
    "        user_data = []\n",
    "        \n",
    "        # Match test ratings with predictions\n",
    "        total_users = len(test_ratings['userId'].unique())\n",
    "        processed_users = 0\n",
    "        \n",
    "        for user_id in test_ratings['userId'].unique():\n",
    "            # Get appropriate alpha for this user\n",
    "            if use_adaptive_alpha:\n",
    "                alpha = self.get_adaptive_alpha(user_id)\n",
    "            else:\n",
    "                alpha = self.alpha\n",
    "                \n",
    "            # Get user's rating count for analysis\n",
    "            rating_count = len(self.data['train_ratings'][self.data['train_ratings']['userId'] == user_id])\n",
    "                \n",
    "            # Get user's test ratings\n",
    "            user_test_ratings = test_ratings[test_ratings['userId'] == user_id]\n",
    "            \n",
    "            user_predictions = []\n",
    "            user_actuals = []\n",
    "            \n",
    "            # Match test ratings with predictions\n",
    "            for _, row in user_test_ratings.iterrows():\n",
    "                movie_id = row['movieId']\n",
    "                actual_rating = row['rating']\n",
    "                \n",
    "                # Get content-based and collaborative predictions\n",
    "                content_pred = self.predict_rating(user_id, movie_id)\n",
    "                collab_pred = self.predict_rating_collaborative(user_id, movie_id)\n",
    "                \n",
    "                # Normalize predictions to 0-1 scale\n",
    "                content_pred_norm = self.normalize_prediction(content_pred)\n",
    "                collab_pred_norm = self.normalize_prediction(collab_pred)\n",
    "                \n",
    "                # Combine predictions using alpha\n",
    "                hybrid_pred_norm = alpha * content_pred_norm + (1 - alpha) * collab_pred_norm\n",
    "                \n",
    "                # Convert back to rating scale\n",
    "                hybrid_pred = self.denormalize_prediction(hybrid_pred_norm)\n",
    "                \n",
    "                # Add to prediction lists\n",
    "                predictions.append(hybrid_pred)\n",
    "                actuals.append(actual_rating)\n",
    "                errors.append((hybrid_pred - actual_rating) ** 2)\n",
    "                \n",
    "                # Keep track of user-specific performance\n",
    "                user_predictions.append(hybrid_pred)\n",
    "                user_actuals.append(actual_rating)\n",
    "            \n",
    "            # Calculate user-specific metrics if we have predictions\n",
    "            if user_predictions:\n",
    "                user_rmse = np.sqrt(np.mean([(p - a) ** 2 for p, a in zip(user_predictions, user_actuals)]))\n",
    "                user_mae = np.mean([abs(p - a) for p, a in zip(user_predictions, user_actuals)])\n",
    "                \n",
    "                # Store user data for later analysis\n",
    "                user_data.append({\n",
    "                    'userId': user_id,\n",
    "                    'rating_count': rating_count,\n",
    "                    'alpha': alpha,\n",
    "                    'rmse': user_rmse,\n",
    "                    'mae': user_mae,\n",
    "                    'num_predictions': len(user_predictions)\n",
    "                })\n",
    "            \n",
    "            processed_users += 1\n",
    "            # Log progress\n",
    "            if processed_users % 100 == 0 or processed_users == total_users:\n",
    "                print(f\"Processed {processed_users}/{total_users} users ({processed_users/total_users*100:.1f}%)\")\n",
    "                if errors:\n",
    "                    current_rmse = np.sqrt(np.mean(errors))\n",
    "                    print(f\"Current RMSE: {current_rmse:.4f} over {len(errors)} predictions\")\n",
    "        \n",
    "        # Calculate RMSE and MAE\n",
    "        if predictions:\n",
    "            predictions = np.array(predictions)\n",
    "            actuals = np.array(actuals)\n",
    "            \n",
    "            print(\"\\nCreating prediction statistics...\")\n",
    "            print(f\"Prediction range: {predictions.min():.2f} - {predictions.max():.2f}\")\n",
    "            print(f\"Actual range: {actuals.min():.2f} - {actuals.max():.2f}\")\n",
    "            \n",
    "            # Calculate residuals\n",
    "            residuals = predictions - actuals\n",
    "            \n",
    "            # Check if we have extreme outliers\n",
    "            outlier_threshold = 3  # 3 star difference\n",
    "            outliers = np.abs(residuals) > outlier_threshold\n",
    "            outlier_count = np.sum(outliers)\n",
    "            \n",
    "            if outlier_count > 0:\n",
    "                print(f\"Warning: Found {outlier_count} predictions with error > {outlier_threshold} stars\")\n",
    "                print(f\"Outlier residuals: {residuals[outliers][:10]} (showing up to 10)\")\n",
    "                \n",
    "                # Calculate metrics both with and without outliers\n",
    "                rmse_with_outliers = np.sqrt(np.mean(residuals ** 2))\n",
    "                mae_with_outliers = np.mean(np.abs(residuals))\n",
    "                \n",
    "                # Remove outliers for adjusted metrics\n",
    "                predictions_filtered = predictions[~outliers]\n",
    "                actuals_filtered = actuals[~outliers]\n",
    "                residuals_filtered = residuals[~outliers]\n",
    "                \n",
    "                rmse_without_outliers = np.sqrt(np.mean(residuals_filtered ** 2))\n",
    "                mae_without_outliers = np.mean(np.abs(residuals_filtered))\n",
    "                \n",
    "                print(f\"RMSE with outliers: {rmse_with_outliers:.4f}\")\n",
    "                print(f\"RMSE without outliers: {rmse_without_outliers:.4f}\")\n",
    "                \n",
    "                # Use the filtered metrics\n",
    "                rmse = rmse_without_outliers\n",
    "                mae = mae_without_outliers\n",
    "                num_predictions = len(predictions_filtered)\n",
    "            else:\n",
    "                # No outliers, use all data\n",
    "                rmse = np.sqrt(np.mean(residuals ** 2))\n",
    "                mae = np.mean(np.abs(residuals))\n",
    "                num_predictions = len(predictions)\n",
    "            \n",
    "            metrics = {\n",
    "                'rmse': rmse,\n",
    "                'mae': mae,\n",
    "                'num_predictions': num_predictions,\n",
    "                'use_adaptive_alpha': use_adaptive_alpha\n",
    "            }\n",
    "            \n",
    "            print(f\"Evaluation completed with {num_predictions} predictions:\")\n",
    "            print(f\"RMSE: {rmse:.4f}\")\n",
    "            print(f\"MAE: {mae:.4f}\")\n",
    "            \n",
    "            # Save metrics\n",
    "            pd.DataFrame([metrics]).to_csv(os.path.join(self.output_path, 'evaluation_metrics.csv'), index=False)\n",
    "            \n",
    "            # Analyze performance by user rating count\n",
    "            if user_data:\n",
    "                user_df = pd.DataFrame(user_data)\n",
    "                \n",
    "                # Group by rating count ranges\n",
    "                user_df['rating_count_range'] = pd.cut(\n",
    "                    user_df['rating_count'], \n",
    "                    bins=[0, 25, 50, 100, 150, float('inf')],\n",
    "                    labels=['<=10', '11-25', '26-50', '51-100', '>100']\n",
    "                )\n",
    "                \n",
    "                # Calculate average performance by rating count\n",
    "                performance_by_count = user_df.groupby('rating_count_range').agg({\n",
    "                    'rmse': 'mean',\n",
    "                    'mae': 'mean',\n",
    "                    'alpha': 'mean',\n",
    "                    'userId': 'count'\n",
    "                }).reset_index()\n",
    "                \n",
    "                performance_by_count.rename(columns={'userId': 'num_users'}, inplace=True)\n",
    "                \n",
    "                print(\"\\nPerformance by user rating count:\")\n",
    "                print(performance_by_count.to_string(index=False))\n",
    "                \n",
    "                # Save user-level metrics\n",
    "                user_df.to_csv(os.path.join(self.output_path, 'user_level_metrics.csv'), index=False)\n",
    "                performance_by_count.to_csv(os.path.join(self.output_path, 'performance_by_count.csv'), index=False)\n",
    "                \n",
    "                # Create visualization of performance by rating count\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                \n",
    "                plt.subplot(2, 2, 1)\n",
    "                sns.barplot(x='rating_count_range', y='rmse', data=performance_by_count)\n",
    "                plt.title('RMSE by User Rating Count')\n",
    "                plt.ylim(bottom=0)\n",
    "                \n",
    "                plt.subplot(2, 2, 2)\n",
    "                sns.barplot(x='rating_count_range', y='mae', data=performance_by_count)\n",
    "                plt.title('MAE by User Rating Count')\n",
    "                plt.ylim(bottom=0)\n",
    "                \n",
    "                plt.subplot(2, 2, 3)\n",
    "                sns.barplot(x='rating_count_range', y='alpha', data=performance_by_count)\n",
    "                plt.title('Average Alpha by User Rating Count')\n",
    "                plt.ylim(0, 1)\n",
    "                \n",
    "                plt.subplot(2, 2, 4)\n",
    "                sns.barplot(x='rating_count_range', y='num_users', data=performance_by_count)\n",
    "                plt.title('Number of Users by Rating Count')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(self.output_path, 'performance_by_rating_count.png'))\n",
    "                plt.close()\n",
    "            \n",
    "            # Plot prediction vs actual\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(actuals, predictions, alpha=0.3)\n",
    "            plt.plot([0.5, 5.0], [0.5, 5.0], 'r--')\n",
    "            plt.xlabel('Actual Ratings')\n",
    "            plt.ylabel('Predicted Ratings')\n",
    "            plt.title(f'Predicted vs Actual Ratings {\"(Adaptive Alpha)\" if use_adaptive_alpha else f\"(Alpha={self.alpha:.2f})\"}')\n",
    "            plt.savefig(os.path.join(self.output_path, 'prediction_scatter.png'))\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot error distribution\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(residuals, bins=30, alpha=0.7)\n",
    "            plt.axvline(x=0, color='r', linestyle='--')\n",
    "            plt.xlabel('Prediction Error (Predicted - Actual)')\n",
    "            plt.ylabel('Count')\n",
    "            plt.title(f'Error Distribution {\"(Adaptive Alpha)\" if use_adaptive_alpha else f\"(Alpha={self.alpha:.2f})\"}')\n",
    "            plt.savefig(os.path.join(self.output_path, 'error_histogram.png'))\n",
    "            plt.close()\n",
    "            \n",
    "            # Store metrics in data\n",
    "            self.data['combined_evaluation_metrics'] = metrics\n",
    "            \n",
    "            return metrics\n",
    "        else:\n",
    "            print(\"No predictions available for evaluation\")\n",
    "            return None\n",
    "    \n",
    "    def find_optimal_alpha(self, alpha_values=None):\n",
    "        \"\"\"\n",
    "        Find optimal alpha value by evaluating RMSE at different alpha levels\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        alpha_values: list\n",
    "            List of alpha values to test\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Optimal alpha value\n",
    "        \"\"\"\n",
    "        if alpha_values is None:\n",
    "            alpha_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "        \n",
    "        print(\"\\nFinding optimal alpha value...\")\n",
    "        results = []\n",
    "        \n",
    "        original_alpha = self.alpha\n",
    "        \n",
    "        for alpha in alpha_values:\n",
    "            print(f\"\\nTesting alpha = {alpha:.1f}\")\n",
    "            self.alpha = alpha\n",
    "            self.combine_recommendations()\n",
    "            metrics = self.evaluate()\n",
    "            \n",
    "            if metrics:\n",
    "                results.append({\n",
    "                    'alpha': alpha,\n",
    "                    'rmse': metrics['rmse'],\n",
    "                    'mae': metrics['mae']\n",
    "                })\n",
    "        \n",
    "        if results:\n",
    "            # Convert to DataFrame for easier analysis\n",
    "            results_df = pd.DataFrame(results)\n",
    "            \n",
    "            # Find optimal alpha (minimize RMSE)\n",
    "            optimal_idx = results_df['rmse'].idxmin()\n",
    "            optimal_alpha = results_df.loc[optimal_idx, 'alpha']\n",
    "            optimal_rmse = results_df.loc[optimal_idx, 'rmse']\n",
    "            \n",
    "            print(f\"\\nOptimal alpha = {optimal_alpha:.2f} with RMSE = {optimal_rmse:.4f}\")\n",
    "            \n",
    "            # Set alpha to optimal value\n",
    "            self.alpha = optimal_alpha\n",
    "            \n",
    "            # Save results\n",
    "            results_df.to_csv(os.path.join(self.output_path, 'alpha_optimization.csv'), index=False)\n",
    "            \n",
    "            # Plot results\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(results_df['alpha'], results_df['rmse'], 'o-', label='RMSE')\n",
    "            plt.plot(results_df['alpha'], results_df['mae'], 's-', label='MAE')\n",
    "            plt.axvline(x=optimal_alpha, color='r', linestyle='--', label=f'Optimal alpha: {optimal_alpha:.2f}')\n",
    "            plt.xlabel('Alpha (Weight of Content-Based Recommendations)')\n",
    "            plt.ylabel('Error Metric Value')\n",
    "            plt.title('Effect of Alpha on Recommendation Performance')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.savefig(os.path.join(self.output_path, 'alpha_optimization.png'))\n",
    "            plt.close()\n",
    "            \n",
    "            return optimal_alpha\n",
    "        else:\n",
    "            # Restore original alpha\n",
    "            self.alpha = original_alpha\n",
    "            print(\"Could not determine optimal alpha due to insufficient data\")\n",
    "            return self.alpha\n",
    "    \n",
    "    def get_user_rated_movies(self, user_id):\n",
    "        \"\"\"\n",
    "        Get movies already rated by a user\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        user_id: int\n",
    "            User ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            DataFrame of user's rated movies with ratings\n",
    "        \"\"\"\n",
    "        if 'train_ratings' not in self.data:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        user_ratings = self.data['train_ratings'][self.data['train_ratings']['userId'] == user_id]\n",
    "        \n",
    "        if len(user_ratings) > 0 and 'movie_features' in self.data:\n",
    "            # Join with movie titles\n",
    "            user_ratings = pd.merge(\n",
    "                user_ratings,\n",
    "                self.data['movie_features'][['movieId', 'title']],\n",
    "                on='movieId',\n",
    "                how='left'\n",
    "            )\n",
    "        \n",
    "        return user_ratings\n",
    "    \n",
    "    def recommend_for_user(self, user_id, n=10, use_adaptive_alpha=True):\n",
    "        \"\"\"\n",
    "        Get recommendations for a specific user\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        user_id: int\n",
    "            User ID\n",
    "        n: int\n",
    "            Number of recommendations to return\n",
    "        use_adaptive_alpha: bool\n",
    "            Whether to use adaptive alpha based on user rating count\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list\n",
    "            List of (movie_id, title, score) tuples\n",
    "        \"\"\"\n",
    "        # Check if user has recommendations\n",
    "        if 'combined_recommendations' not in self.data or user_id not in self.data['combined_recommendations']:\n",
    "            # If no pre-computed recommendations, try to generate on-the-fly\n",
    "            if 'user_vectors' in self.data and 'movie_vectors' in self.data:\n",
    "                print(f\"No pre-computed recommendations found. Generating on-the-fly recommendations for user {user_id}...\")\n",
    "                \n",
    "                # Get user's vector\n",
    "                if user_id not in self.data['user_vectors']:\n",
    "                    print(f\"No user vector found for user {user_id}\")\n",
    "                    return []\n",
    "                \n",
    "                user_vector = self.data['user_vectors'][user_id]\n",
    "                \n",
    "                # Get rated movies to exclude\n",
    "                rated_movies = set()\n",
    "                if 'train_ratings' in self.data:\n",
    "                    user_ratings = self.data['train_ratings'][self.data['train_ratings']['userId'] == user_id]\n",
    "                    rated_movies = set(user_ratings['movieId'].values)\n",
    "                \n",
    "                # Calculate similarity to all movies\n",
    "                movie_similarities = []\n",
    "                for movie_id, movie_vector in self.data['movie_vectors'].items():\n",
    "                    if movie_id in rated_movies:\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculate cosine similarity\n",
    "                    similarity = np.dot(user_vector, movie_vector) / (np.linalg.norm(user_vector) * np.linalg.norm(movie_vector))\n",
    "                    movie_similarities.append((movie_id, similarity))\n",
    "                \n",
    "                # Sort by similarity\n",
    "                movie_similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                # Convert to rating predictions\n",
    "                recommendations = []\n",
    "                for movie_id, similarity in movie_similarities[:n]:\n",
    "                    # Get collaborative prediction if available\n",
    "                    collab_pred = self.predict_rating_collaborative(user_id, movie_id)\n",
    "                    \n",
    "                    # Normalize both scores\n",
    "                    content_score = similarity  # Already in [0,1] range\n",
    "                    collab_score = self.normalize_prediction(collab_pred)\n",
    "                    \n",
    "                    # Get appropriate alpha\n",
    "                    if use_adaptive_alpha:\n",
    "                        alpha = self.get_adaptive_alpha(user_id)\n",
    "                    else:\n",
    "                        alpha = self.alpha\n",
    "                    \n",
    "                    # Combine scores\n",
    "                    combined_score = alpha * content_score + (1 - alpha) * collab_score\n",
    "                    \n",
    "                    # Convert to rating\n",
    "                    rating = self.denormalize_prediction(combined_score)\n",
    "                    \n",
    "                    recommendations.append((movie_id, rating))\n",
    "                \n",
    "                # Sort by predicted rating\n",
    "                recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                # Format with titles\n",
    "                formatted_recs = []\n",
    "                for movie_id, score in recommendations[:n]:\n",
    "                    title = \"Unknown\"\n",
    "                    if 'movie_features' in self.data:\n",
    "                        movie_row = self.data['movie_features'][self.data['movie_features']['movieId'] == movie_id]\n",
    "                        if not movie_row.empty and 'title' in movie_row.columns:\n",
    "                            title = movie_row.iloc[0]['title']\n",
    "                    \n",
    "                    formatted_recs.append((movie_id, title, score))\n",
    "                \n",
    "                return formatted_recs\n",
    "            else:\n",
    "                return []\n",
    "        \n",
    "        # Get recommendations from pre-computed data\n",
    "        recs = self.data['combined_recommendations'][user_id][:n]\n",
    "        \n",
    "        # Format recommendations with titles\n",
    "        formatted_recs = []\n",
    "        for movie_id, score in recs:\n",
    "            title = \"Unknown\"\n",
    "            if 'movie_features' in self.data:\n",
    "                movie_row = self.data['movie_features'][self.data['movie_features']['movieId'] == movie_id]\n",
    "                if not movie_row.empty and 'title' in movie_row.columns:\n",
    "                    title = movie_row.iloc[0]['title']\n",
    "            \n",
    "            formatted_recs.append((movie_id, title, score))\n",
    "        \n",
    "        return formatted_recs\n",
    "    \n",
    "    def get_user_rating_statistics(self, user_id):\n",
    "        \"\"\"\n",
    "        Get detailed rating statistics for a user\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        user_id: int\n",
    "            User ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of user rating statistics\n",
    "        \"\"\"\n",
    "        if 'train_ratings' not in self.data:\n",
    "            return None\n",
    "        \n",
    "        # Get user's ratings\n",
    "        user_ratings = self.data['train_ratings'][self.data['train_ratings']['userId'] == user_id]\n",
    "        \n",
    "        if len(user_ratings) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Calculate statistics\n",
    "        stats = {\n",
    "            'rating_count': len(user_ratings),\n",
    "            'avg_rating': user_ratings['rating'].mean(),\n",
    "            'min_rating': user_ratings['rating'].min(),\n",
    "            'max_rating': user_ratings['rating'].max(),\n",
    "            'rating_std': user_ratings['rating'].std(),\n",
    "            'adaptive_alpha': self.get_adaptive_alpha(user_id)\n",
    "        }\n",
    "        \n",
    "        # Count ratings by value\n",
    "        rating_counts = user_ratings['rating'].value_counts().sort_index().to_dict()\n",
    "        stats['rating_distribution'] = rating_counts\n",
    "        \n",
    "        # Get genre preferences if available\n",
    "        if 'movie_features' in self.data:\n",
    "            # Create mapping of movie IDs to genres\n",
    "            movie_genres = {}\n",
    "            genre_columns = [col for col in self.data['movie_features'].columns if col not in \n",
    "                             ['movieId', 'title', 'tokens', 'token_count', 'top_keywords']]\n",
    "            \n",
    "            for _, row in self.data['movie_features'].iterrows():\n",
    "                movie_id = row['movieId']\n",
    "                movie_genres[movie_id] = [genre for genre in genre_columns if row[genre] == 1]\n",
    "            \n",
    "            # Count genre occurrences in user ratings\n",
    "            genre_counts = {}\n",
    "            for _, row in user_ratings.iterrows():\n",
    "                movie_id = row['movieId']\n",
    "                rating = row['rating']\n",
    "                \n",
    "                if movie_id in movie_genres:\n",
    "                    for genre in movie_genres[movie_id]:\n",
    "                        if genre not in genre_counts:\n",
    "                            genre_counts[genre] = {'count': 0, 'sum': 0, 'ratings': []}\n",
    "                        \n",
    "                        genre_counts[genre]['count'] += 1\n",
    "                        genre_counts[genre]['sum'] += rating\n",
    "                        genre_counts[genre]['ratings'].append(rating)\n",
    "            \n",
    "            # Calculate average rating per genre\n",
    "            genre_preferences = {}\n",
    "            for genre, data in genre_counts.items():\n",
    "                if data['count'] > 0:\n",
    "                    genre_preferences[genre] = {\n",
    "                        'count': data['count'],\n",
    "                        'avg_rating': data['sum'] / data['count'],\n",
    "                        'std': np.std(data['ratings']) if len(data['ratings']) > 1 else 0\n",
    "                    }\n",
    "            \n",
    "            stats['genre_preferences'] = genre_preferences\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def get_movie_details(self, movie_id):\n",
    "        \"\"\"\n",
    "        Get detailed information about a movie\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        movie_id: int\n",
    "            Movie ID\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Movie details\n",
    "        \"\"\"\n",
    "        if 'movie_features' not in self.data:\n",
    "            return None\n",
    "        \n",
    "        movie_row = self.data['movie_features'][self.data['movie_features']['movieId'] == movie_id]\n",
    "        \n",
    "        if movie_row.empty:\n",
    "            return None\n",
    "        \n",
    "        # Extract genre columns\n",
    "        genre_columns = [col for col in movie_row.columns if col not in \n",
    "                         ['movieId', 'title', 'tokens', 'token_count', 'top_keywords']]\n",
    "        \n",
    "        # Get genres\n",
    "        genres = [genre for genre in genre_columns if movie_row[genre].iloc[0] == 1]\n",
    "        \n",
    "        # Get keywords\n",
    "        keywords = []\n",
    "        if 'top_keywords' in movie_row.columns:\n",
    "            keywords_raw = movie_row['top_keywords'].iloc[0]\n",
    "            if isinstance(keywords_raw, str):\n",
    "                # Parse string representation of list if needed\n",
    "                try:\n",
    "                    keywords = eval(keywords_raw)\n",
    "                except:\n",
    "                    keywords = []\n",
    "            else:\n",
    "                keywords = keywords_raw\n",
    "        \n",
    "        # Create details dictionary\n",
    "        details = {\n",
    "            'movieId': movie_id,\n",
    "            'title': movie_row['title'].iloc[0],\n",
    "            'genres': genres,\n",
    "            'keywords': keywords\n",
    "        }\n",
    "        \n",
    "        return details\n",
    "    \n",
    "    def generate_content_based_recommendations(self):\n",
    "        \"\"\"\n",
    "        Generate content-based recommendations if not already available\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        bool\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        if 'content_recommendations' in self.data and self.data['content_recommendations']:\n",
    "            print(\"Content-based recommendations already available, skipping generation\")\n",
    "            return True\n",
    "            \n",
    "        # Check if we need to load or generate user-movie similarities\n",
    "        if 'user_movie_similarities' not in self.data or not self.data['user_movie_similarities']:\n",
    "            if 'user_vectors' not in self.data or 'movie_vectors' not in self.data:\n",
    "                print(\"Error: Required vectors not available for content-based recommendation generation\")\n",
    "                return False\n",
    "                \n",
    "            print(\"\\nCalculating user-movie similarities...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                user_vectors = self.data['user_vectors']\n",
    "                movie_vectors = self.data['movie_vectors']\n",
    "                \n",
    "                # Initialize user-movie similarities\n",
    "                user_movie_similarities = {}\n",
    "                \n",
    "                # Calculate similarity for each user\n",
    "                for i, (user_id, user_vector) in enumerate(user_vectors.items()):\n",
    "                    user_sims = {}\n",
    "                    \n",
    "                    for movie_id, movie_vector in movie_vectors.items():\n",
    "                        # Calculate cosine similarity\n",
    "                        if len(user_vector) == len(movie_vector):\n",
    "                            similarity = np.dot(user_vector, movie_vector) / (np.linalg.norm(user_vector) * np.linalg.norm(movie_vector))\n",
    "                            \n",
    "                            # Only store if above threshold\n",
    "                            if similarity > 0.3:  # Minimum similarity threshold\n",
    "                                user_sims[movie_id] = similarity\n",
    "                    \n",
    "                    user_movie_similarities[user_id] = user_sims\n",
    "                    \n",
    "                    # Log progress\n",
    "                    if (i+1) % 100 == 0 or (i+1) == len(user_vectors):\n",
    "                        print(f\"Processed {i+1}/{len(user_vectors)} users ({(i+1)/len(user_vectors)*100:.1f}%)\")\n",
    "                \n",
    "                self.data['user_movie_similarities'] = user_movie_similarities\n",
    "                print(f\"Calculated user-movie similarities in {time.time() - start_time:.2f}s\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating user-movie similarities: {str(e)}\")\n",
    "                return False\n",
    "        \n",
    "        # Generate recommendations\n",
    "        if 'user_movie_similarities' in self.data and self.data['user_movie_similarities']:\n",
    "            print(\"\\nGenerating content-based recommendations...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                user_movie_similarities = self.data['user_movie_similarities']\n",
    "                \n",
    "                # Get training ratings to avoid recommending already rated movies\n",
    "                if 'train_ratings' in self.data:\n",
    "                    train_ratings = self.data['train_ratings']\n",
    "                    # Create user to movies mapping for fast lookups\n",
    "                    user_rated_movies = defaultdict(set)\n",
    "                    for _, row in train_ratings.iterrows():\n",
    "                        user_rated_movies[row['userId']].add(row['movieId'])\n",
    "                else:\n",
    "                    user_rated_movies = defaultdict(set)\n",
    "                \n",
    "                # Generate recommendations for all users\n",
    "                content_recommendations = {}\n",
    "                \n",
    "                # Process users\n",
    "                total_users = len(user_movie_similarities)\n",
    "                for i, (user_id, sims) in enumerate(user_movie_similarities.items()):\n",
    "                    # Get movies already rated by the user\n",
    "                    rated_movies = user_rated_movies.get(user_id, set())\n",
    "                    \n",
    "                    # Filter out already rated movies\n",
    "                    candidates = [(movie_id, sim) for movie_id, sim in sims.items() \n",
    "                                 if movie_id not in rated_movies]\n",
    "                    \n",
    "                    # Sort by similarity (descending)\n",
    "                    recommendations = sorted(candidates, key=lambda x: x[1], reverse=True)[:10]  # Top 10\n",
    "                    \n",
    "                    if recommendations:\n",
    "                        content_recommendations[user_id] = recommendations\n",
    "                    \n",
    "                    # Log progress\n",
    "                    if (i+1) % 100 == 0 or (i+1) == total_users:\n",
    "                        print(f\"Processed {i+1}/{total_users} users ({(i+1)/total_users*100:.1f}%)\")\n",
    "                \n",
    "                self.data['content_recommendations'] = content_recommendations\n",
    "                \n",
    "                print(f\"Generated content-based recommendations for {len(content_recommendations)} users in {time.time() - start_time:.2f}s\")\n",
    "                return True\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error generating content-based recommendations: {str(e)}\")\n",
    "                return False\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def generate_collaborative_recommendations(self):\n",
    "        \"\"\"\n",
    "        Generate collaborative filtering recommendations if not already available\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        bool\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        if 'collaborative_recommendations' in self.data and self.data['collaborative_recommendations']:\n",
    "            print(\"Collaborative filtering recommendations already available, skipping generation\")\n",
    "            return True\n",
    "            \n",
    "        # Check if we have the necessary components for generating DNN-based recommendations\n",
    "        if 'dnn_model' not in self.data or 'user_genre_preferences' not in self.data or 'movie_genre_features' not in self.data:\n",
    "            print(\"Error: Required components for DNN-based recommendation generation not available\")\n",
    "            return False\n",
    "        \n",
    "        print(\"\\nGenerating collaborative filtering recommendations...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Get required data\n",
    "            dnn_model = self.data['dnn_model']\n",
    "            user_genre_preferences = self.data['user_genre_preferences']\n",
    "            movie_genre_features = self.data['movie_genre_features']\n",
    "            train_ratings = self.data.get('train_ratings')\n",
    "            \n",
    "            # Generate recommendations for all users\n",
    "            all_users = user_genre_preferences['userId'].unique()\n",
    "            genre_columns = [col for col in movie_genre_features.columns if col != 'movieId']\n",
    "            \n",
    "            collab_recommendations = {}\n",
    "            \n",
    "            # Process each user\n",
    "            for i, user_id in enumerate(all_users):\n",
    "                # Skip if user not found in genre preferences\n",
    "                if user_id not in user_genre_preferences['userId'].values:\n",
    "                    continue\n",
    "                \n",
    "                # Get user genre preferences\n",
    "                user_prefs = user_genre_preferences[user_genre_preferences['userId'] == user_id].iloc[0]\n",
    "                \n",
    "                # Get movies already rated by the user\n",
    "                rated_movies = set()\n",
    "                if train_ratings is not None:\n",
    "                    rated_movies = set(train_ratings[train_ratings['userId'] == user_id]['movieId'].values)\n",
    "                \n",
    "                # Calculate predictions for all unrated movies\n",
    "                movie_predictions = []\n",
    "                \n",
    "                # Process each movie\n",
    "                for _, movie_row in movie_genre_features.iterrows():\n",
    "                    movie_id = movie_row['movieId']\n",
    "                    \n",
    "                    # Skip if movie already rated\n",
    "                    if movie_id in rated_movies:\n",
    "                        continue\n",
    "                    \n",
    "                    # Create feature vector\n",
    "                    feature_vector = []\n",
    "                    \n",
    "                    for genre in genre_columns:\n",
    "                        feature_vector.append(user_prefs[genre])\n",
    "                        feature_vector.append(movie_row[genre])\n",
    "                    \n",
    "                    # Reshape for prediction\n",
    "                    feature_vector = np.array([feature_vector])\n",
    "                    \n",
    "                    # Predict movie rating\n",
    "                    predicted_rating = dnn_model.predict(feature_vector, verbose=0)[0][0]\n",
    "                    \n",
    "                    # Ensure rating is within bounds\n",
    "                    predicted_rating = max(0.5, min(5.0, predicted_rating))\n",
    "                    \n",
    "                    movie_predictions.append((movie_id, predicted_rating))\n",
    "                \n",
    "                # Sort by predicted rating in descending order\n",
    "                movie_predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                # Store top recommendations\n",
    "                collab_recommendations[user_id] = movie_predictions[:10]  # Top 10 recommendations\n",
    "                \n",
    "                # Log progress\n",
    "                if (i+1) % 100 == 0 or (i+1) == len(all_users):\n",
    "                    print(f\"Processed {i+1}/{len(all_users)} users ({(i+1)/len(all_users)*100:.1f}%)\")\n",
    "            \n",
    "            self.data['collaborative_recommendations'] = collab_recommendations\n",
    "            \n",
    "            print(f\"Generated collaborative filtering recommendations for {len(collab_recommendations)} users in {time.time() - start_time:.2f}s\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating collaborative filtering recommendations: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Hybrid Movie Recommendation System')\n",
    "    parser.add_argument('--content_path', type=str, default='./rec/content-recommendations', \n",
    "                      help='Path to content-based model files')\n",
    "    parser.add_argument('--collab_path', type=str, default='./rec/collaborative-recommendations',\n",
    "                      help='Path to collaborative filtering model files')\n",
    "    parser.add_argument('--output_path', type=str, default='./rec/hybrid_recommendations',\n",
    "                      help='Path to save hybrid recommendation results')\n",
    "    parser.add_argument('--alpha', type=float, default=0.3,\n",
    "                      help='Weight for content-based recommendations (1-alpha for collaborative)')\n",
    "    parser.add_argument('--optimize_alpha', action='store_true',\n",
    "                      help='Find optimal alpha value')\n",
    "    parser.add_argument('--adaptive_alpha', action='store_true', default=True,\n",
    "                      help='Use adaptive alpha based on user rating count')\n",
    "    parser.add_argument('--batch_mode', action='store_true',\n",
    "                      help='Run in batch mode (no interactive prompts)')\n",
    "    parser.add_argument('--num_recs', type=int, default=10,\n",
    "                      help='Number of recommendations to generate')\n",
    "    parser.add_argument('--generate', action='store_true',\n",
    "                      help='Generate recommendations if they are not already available')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Create and initialize the hybrid recommender\n",
    "    recommender = HybridRecommender(\n",
    "        content_model_path=args.content_path,\n",
    "        collab_model_path=args.collab_path,\n",
    "        output_path=args.output_path,\n",
    "        alpha=args.alpha\n",
    "    )\n",
    "    \n",
    "    # Load data\n",
    "    recommender.load_data()\n",
    "    \n",
    "    # Generate recommendations if requested and they don't exist\n",
    "    if args.generate:\n",
    "        if 'content_recommendations' not in recommender.data or not recommender.data['content_recommendations']:\n",
    "            recommender.generate_content_based_recommendations()\n",
    "        \n",
    "        if 'collaborative_recommendations' not in recommender.data or not recommender.data['collaborative_recommendations']:\n",
    "            recommender.generate_collaborative_recommendations()\n",
    "    \n",
    "    # Find optimal alpha if requested\n",
    "    if args.optimize_alpha:\n",
    "        optimal_alpha = recommender.find_optimal_alpha()\n",
    "        print(f\"Optimal alpha: {optimal_alpha:.2f}\")\n",
    "    \n",
    "    # Combine recommendations\n",
    "    recommender.combine_recommendations(top_n=args.num_recs, use_adaptive_alpha=args.adaptive_alpha)\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluation_metrics = recommender.evaluate(use_adaptive_alpha=args.adaptive_alpha)\n",
    "    \n",
    "    # Compare with individual models\n",
    "    print(\"\\nModel Performance Comparison:\")\n",
    "    headers = [\"Model\", \"RMSE\", \"MAE\", \"Predictions\"]\n",
    "    rows = []\n",
    "    \n",
    "    # Content-based model metrics\n",
    "    if 'content_evaluation' in recommender.data:\n",
    "        rows.append([\n",
    "            \"Content-Based\",\n",
    "            f\"{recommender.data['content_evaluation']['rmse']:.4f}\",\n",
    "            f\"{recommender.data['content_evaluation'].get('mae', 'N/A')}\",\n",
    "            f\"{recommender.data['content_evaluation'].get('num_predictions', 'N/A')}\"\n",
    "        ])\n",
    "    \n",
    "    # Collaborative filtering model metrics\n",
    "    if 'dnn_evaluation' in recommender.data:\n",
    "        rows.append([\n",
    "            \"Collaborative\",\n",
    "            f\"{recommender.data['dnn_evaluation']['rmse']:.4f}\",\n",
    "            f\"{recommender.data['dnn_evaluation']['mae']:.4f}\",\n",
    "            f\"{recommender.data['dnn_evaluation']['num_predictions']}\"\n",
    "        ])\n",
    "    \n",
    "    # Hybrid model metrics\n",
    "    if evaluation_metrics:\n",
    "        alpha_desc = \"Adaptive\" if args.adaptive_alpha else f\"={recommender.alpha:.2f}\"\n",
    "        rows.append([\n",
    "            f\"Hybrid ({alpha_desc})\",\n",
    "            f\"{evaluation_metrics['rmse']:.4f}\",\n",
    "            f\"{evaluation_metrics['mae']:.4f}\",\n",
    "            f\"{evaluation_metrics['num_predictions']}\"\n",
    "        ])\n",
    "    \n",
    "    # Print table\n",
    "    if rows:\n",
    "        # Calculate column widths\n",
    "        col_widths = [max(len(row[i]) for row in [headers] + rows) for i in range(len(headers))]\n",
    "        \n",
    "        # Print table header\n",
    "        print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")\n",
    "        print(\"| \" + \" | \".join(headers[i].ljust(col_widths[i]) for i in range(len(headers))) + \" |\")\n",
    "        print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")\n",
    "        \n",
    "        # Print table rows\n",
    "        for row in rows:\n",
    "            print(\"| \" + \" | \".join(row[i].ljust(col_widths[i]) for i in range(len(row))) + \" |\")\n",
    "        \n",
    "        print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")\n",
    "    \n",
    "    if args.batch_mode:\n",
    "        print(\"\\nHybrid Recommendation System completed successfully!\")\n",
    "        return\n",
    "    \n",
    "    # Interactive mode - prompt for user IDs\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\nEnter user id to recommend (blank to stop): \")\n",
    "            \n",
    "            if not user_input.strip():\n",
    "                print(\"\\nExiting recommendation system. Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                user_id = int(user_input)\n",
    "            except ValueError:\n",
    "                print(\"Please enter a valid numeric user ID.\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nGenerating recommendations for User ID: {user_id}\")\n",
    "            \n",
    "            # Get user's statistics\n",
    "            user_stats = recommender.get_user_rating_statistics(user_id)\n",
    "            \n",
    "            if user_stats:\n",
    "                # Display adaptive alpha information\n",
    "                if args.adaptive_alpha:\n",
    "                    alpha = user_stats['adaptive_alpha']\n",
    "                    print(f\"\\nUser Rating Profile:\")\n",
    "                    print(f\"- Total ratings: {user_stats['rating_count']}\")\n",
    "                    print(f\"- Rating range: {user_stats['min_rating']:.1f} - {user_stats['max_rating']:.1f} (avg: {user_stats['avg_rating']:.2f})\")\n",
    "                    \n",
    "                    # Determine user category based on rating count\n",
    "                    if user_stats['rating_count'] <= 10:\n",
    "                        user_category = \"New user\"\n",
    "                    elif user_stats['rating_count'] <= 25:\n",
    "                        user_category = \"Casual user\"\n",
    "                    elif user_stats['rating_count'] <= 50:\n",
    "                        user_category = \"Regular user\"\n",
    "                    elif user_stats['rating_count'] <= 100:\n",
    "                        user_category = \"Active user\"\n",
    "                    else:\n",
    "                        user_category = \"Power user\"\n",
    "                        \n",
    "                    print(f\"- User category: {user_category}\")\n",
    "                    print(f\"- Adaptive alpha: {alpha:.2f}\" + \n",
    "                          f\" (more weight on {'content-based' if alpha > 0.5 else 'collaborative filtering'})\")\n",
    "                    \n",
    "                    # Show genre preferences if available\n",
    "                    if 'genre_preferences' in user_stats and user_stats['genre_preferences']:\n",
    "                        print(\"\\nTop genre preferences:\")\n",
    "                        sorted_genres = sorted(\n",
    "                            [(genre, data) for genre, data in user_stats['genre_preferences'].items() if data['count'] >= 3],\n",
    "                            key=lambda x: x[1]['avg_rating'], \n",
    "                            reverse=True\n",
    "                        )[:5]\n",
    "                        \n",
    "                        for genre, data in sorted_genres:\n",
    "                            print(f\"  - {genre}: {data['avg_rating']:.2f} avg rating ({data['count']} movies)\")\n",
    "            \n",
    "            # Get user's rated movies\n",
    "            rated_movies = recommender.get_user_rated_movies(user_id)\n",
    "            \n",
    "            if len(rated_movies) > 0:\n",
    "                print(f\"\\nSample of user's highest rated movies:\")\n",
    "                top_rated = rated_movies.sort_values('rating', ascending=False).head(5)\n",
    "                for _, row in top_rated.iterrows():\n",
    "                    print(f\"  '{row['title']}' - Rating: {row['rating']:.1f}\")\n",
    "            else:\n",
    "                print(f\"\\nUser {user_id} has no rated movies in the training set\")\n",
    "            \n",
    "            # Get recommendations\n",
    "            recommendations = recommender.recommend_for_user(user_id, n=args.num_recs, use_adaptive_alpha=args.adaptive_alpha)\n",
    "            \n",
    "            if recommendations:\n",
    "                print(f\"\\nTop {len(recommendations)} recommendations for User {user_id}:\")\n",
    "                for i, (movie_id, title, score) in enumerate(recommendations, 1):\n",
    "                    print(f\"{i}. '{title}' - Predicted Rating: {score:.2f}\")\n",
    "                    \n",
    "                    # Get movie details\n",
    "                    details = recommender.get_movie_details(movie_id)\n",
    "                    if details and details['genres']:\n",
    "                        print(f\"   Genres: {', '.join(details['genres'])}\")\n",
    "                    if details and details['keywords']:\n",
    "                        print(f\"   Keywords: {', '.join(details['keywords'][:5])}\")\n",
    "            else:\n",
    "                print(f\"\\nNo recommendations found for User {user_id}\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nProcess interrupted by user. Exiting...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\nAn error occurred: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    print(\"\\nHybrid Recommendation System completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
