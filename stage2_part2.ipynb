{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COLLABORATIVE FILTERING WITH DEEP NEURAL NETWORK\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 1: DATA LOADING\n",
      "================================================================================\n",
      "Loading processed data from stage1.py...\n",
      "Loaded features for 15597 movies\n",
      "Loaded 1476401 normalized ratings\n",
      "Split ratings into 1175202 training and 301199 testing samples\n",
      "\n",
      "--------------------------------------------------\n",
      "DATA ANALYSIS: LOADED DATASETS\n",
      "--------------------------------------------------\n",
      "\n",
      "Movie Features Summary:\n",
      "- Total movies: 15597\n",
      "- Number of genres: 20\n",
      "- Genre columns: ['(no genres listed)', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'IMAX', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
      "\n",
      "Sample movie features:\n",
      "   movieId                    title  (no genres listed)  Action  Adventure\n",
      "0        1         Toy Story (1995)                   0       0          1\n",
      "1        2           Jumanji (1995)                   0       0          1\n",
      "2        3  Grumpier Old Men (1995)                   0       0          0\n",
      "\n",
      "Ratings Summary:\n",
      "- Total ratings: 1476401\n",
      "- Unique users: 10000\n",
      "- Unique movies: 15597\n",
      "- Rating range: 0.5 - 5.0\n",
      "- Average rating: 3.52\n",
      "\n",
      "Rating distribution plot saved to ./rec/collaborative-recommendations\\rating_distribution.png\n",
      "\n",
      "Train/Test Split Summary:\n",
      "- Training ratings: 1175202 (79.6%)\n",
      "- Testing ratings: 301199 (20.4%)\n",
      "- Training users: 8000\n",
      "- Testing users: 2000\n",
      "\n",
      "Ratings per user:\n",
      "- Training set - Avg: 146.90, Min: 20, Max: 7515\n",
      "- Testing set - Avg: 150.60, Min: 20, Max: 2448\n",
      "\n",
      "================================================================================\n",
      "STEP 2: MOVIE GENRE FEATURE EXTRACTION\n",
      "================================================================================\n",
      "Extracting genre features for movies...\n",
      "Extracted 20 genre features for 15597 movies\n",
      "\n",
      "--------------------------------------------------\n",
      "DATA ANALYSIS: GENRE FEATURES\n",
      "--------------------------------------------------\n",
      "\n",
      "Genre Distribution:\n",
      "- (no genres listed): 8 movies (0.1%)\n",
      "- Action: 2204 movies (14.1%)\n",
      "- Adventure: 1529 movies (9.8%)\n",
      "- Animation: 601 movies (3.9%)\n",
      "- Children: 759 movies (4.9%)\n",
      "- Comedy: 5188 movies (33.3%)\n",
      "- Crime: 1806 movies (11.6%)\n",
      "- Documentary: 1001 movies (6.4%)\n",
      "- Drama: 7844 movies (50.3%)\n",
      "- Fantasy: 913 movies (5.9%)\n",
      "- Film-Noir: 170 movies (1.1%)\n",
      "- Horror: 1563 movies (10.0%)\n",
      "- IMAX: 179 movies (1.1%)\n",
      "- Musical: 627 movies (4.0%)\n",
      "- Mystery: 895 movies (5.7%)\n",
      "- Romance: 2505 movies (16.1%)\n",
      "- Sci-Fi: 1162 movies (7.5%)\n",
      "- Thriller: 2726 movies (17.5%)\n",
      "- War: 698 movies (4.5%)\n",
      "- Western: 351 movies (2.3%)\n",
      "\n",
      "Genre distribution plot saved to ./rec/collaborative-recommendations\\genre_distribution.png\n",
      "\n",
      "Genre Co-occurrence Analysis:\n",
      "Most common genre combinations:\n",
      "- (no genres listed) most commonly appears with: Action (0.00), War (0.00), Thriller (0.00), Sci-Fi (0.00), Romance (0.00)\n",
      "- Action most commonly appears with: IMAX (0.53), Adventure (0.43), Sci-Fi (0.38), Thriller (0.28), Crime (0.28)\n",
      "- Adventure most commonly appears with: IMAX (0.47), Children (0.43), Animation (0.40), Fantasy (0.39), Action (0.30)\n",
      "- Animation most commonly appears with: Children (0.41), Fantasy (0.20), IMAX (0.19), Adventure (0.16), Musical (0.14)\n",
      "- Children most commonly appears with: Animation (0.52), Fantasy (0.23), Adventure (0.21), IMAX (0.17), Musical (0.17)\n",
      "\n",
      "Genre co-occurrence matrix saved to ./rec/collaborative-recommendations\\genre_co_occurrence.png\n",
      "\n",
      "Sample of movie genre features:\n",
      "   movieId  (no genres listed)  Action  Adventure  Animation  Children  \\\n",
      "0        1                   0       0          1          1         1   \n",
      "1        2                   0       0          1          0         1   \n",
      "2        3                   0       0          0          0         0   \n",
      "\n",
      "   Comedy  Crime  Documentary  Drama  ...  Film-Noir  Horror  IMAX  Musical  \\\n",
      "0       1      0            0      0  ...          0       0     0        0   \n",
      "1       0      0            0      0  ...          0       0     0        0   \n",
      "2       1      0            0      0  ...          0       0     0        0   \n",
      "\n",
      "   Mystery  Romance  Sci-Fi  Thriller  War  Western  \n",
      "0        0        0       0         0    0        0  \n",
      "1        0        0       0         0    0        0  \n",
      "2        0        1       0         0    0        0  \n",
      "\n",
      "[3 rows x 21 columns]\n",
      "\n",
      "Saved movie genre features to ./rec/collaborative-recommendations\\movie_genre_features.csv\n",
      "\n",
      "================================================================================\n",
      "STEP 3: USER GENRE PREFERENCE CALCULATION\n",
      "================================================================================\n",
      "Calculating user preferences for movie genres...\n",
      "Processed 100/8000 users (1.2%)\n",
      "Processed 200/8000 users (2.5%)\n",
      "Processed 300/8000 users (3.8%)\n",
      "Processed 400/8000 users (5.0%)\n",
      "Processed 500/8000 users (6.2%)\n",
      "Processed 600/8000 users (7.5%)\n",
      "Processed 700/8000 users (8.8%)\n",
      "Processed 800/8000 users (10.0%)\n",
      "Processed 900/8000 users (11.2%)\n",
      "Processed 1000/8000 users (12.5%)\n",
      "Processed 1100/8000 users (13.8%)\n",
      "Processed 1200/8000 users (15.0%)\n",
      "Processed 1300/8000 users (16.2%)\n",
      "Processed 1400/8000 users (17.5%)\n",
      "Processed 1500/8000 users (18.8%)\n",
      "Processed 1600/8000 users (20.0%)\n",
      "Processed 1700/8000 users (21.2%)\n",
      "Processed 1800/8000 users (22.5%)\n",
      "Processed 1900/8000 users (23.8%)\n",
      "Processed 2000/8000 users (25.0%)\n",
      "Processed 2100/8000 users (26.2%)\n",
      "Processed 2200/8000 users (27.5%)\n",
      "Processed 2300/8000 users (28.7%)\n",
      "Processed 2400/8000 users (30.0%)\n",
      "Processed 2500/8000 users (31.2%)\n",
      "Processed 2600/8000 users (32.5%)\n",
      "Processed 2700/8000 users (33.8%)\n",
      "Processed 2800/8000 users (35.0%)\n",
      "Processed 2900/8000 users (36.2%)\n",
      "Processed 3000/8000 users (37.5%)\n",
      "Processed 3100/8000 users (38.8%)\n",
      "Processed 3200/8000 users (40.0%)\n",
      "Processed 3300/8000 users (41.2%)\n",
      "Processed 3400/8000 users (42.5%)\n",
      "Processed 3500/8000 users (43.8%)\n",
      "Processed 3600/8000 users (45.0%)\n",
      "Processed 3700/8000 users (46.2%)\n",
      "Processed 3800/8000 users (47.5%)\n",
      "Processed 3900/8000 users (48.8%)\n",
      "Processed 4000/8000 users (50.0%)\n",
      "Processed 4100/8000 users (51.2%)\n",
      "Processed 4200/8000 users (52.5%)\n",
      "Processed 4300/8000 users (53.8%)\n",
      "Processed 4400/8000 users (55.0%)\n",
      "Processed 4500/8000 users (56.2%)\n",
      "Processed 4600/8000 users (57.5%)\n",
      "Processed 4700/8000 users (58.8%)\n",
      "Processed 4800/8000 users (60.0%)\n",
      "Processed 4900/8000 users (61.3%)\n",
      "Processed 5000/8000 users (62.5%)\n",
      "Processed 5100/8000 users (63.7%)\n",
      "Processed 5200/8000 users (65.0%)\n",
      "Processed 5300/8000 users (66.2%)\n",
      "Processed 5400/8000 users (67.5%)\n",
      "Processed 5500/8000 users (68.8%)\n",
      "Processed 5600/8000 users (70.0%)\n",
      "Processed 5700/8000 users (71.2%)\n",
      "Processed 5800/8000 users (72.5%)\n",
      "Processed 5900/8000 users (73.8%)\n",
      "Processed 6000/8000 users (75.0%)\n",
      "Processed 6100/8000 users (76.2%)\n",
      "Processed 6200/8000 users (77.5%)\n",
      "Processed 6300/8000 users (78.8%)\n",
      "Processed 6400/8000 users (80.0%)\n",
      "Processed 6500/8000 users (81.2%)\n",
      "Processed 6600/8000 users (82.5%)\n",
      "Processed 6700/8000 users (83.8%)\n",
      "Processed 6800/8000 users (85.0%)\n",
      "Processed 6900/8000 users (86.2%)\n",
      "Processed 7000/8000 users (87.5%)\n",
      "Processed 7100/8000 users (88.8%)\n",
      "Processed 7200/8000 users (90.0%)\n",
      "Processed 7300/8000 users (91.2%)\n",
      "Processed 7400/8000 users (92.5%)\n",
      "Processed 7500/8000 users (93.8%)\n",
      "Processed 7600/8000 users (95.0%)\n",
      "Processed 7700/8000 users (96.2%)\n",
      "Processed 7800/8000 users (97.5%)\n",
      "Processed 7900/8000 users (98.8%)\n",
      "Processed 8000/8000 users (100.0%)\n",
      "Calculated genre preferences for 8000 users\n",
      "\n",
      "--------------------------------------------------\n",
      "DATA ANALYSIS: USER GENRE PREFERENCES\n",
      "--------------------------------------------------\n",
      "\n",
      "User Genre Preferences Summary:\n",
      "\n",
      "Statistics for top genres:\n",
      "- Drama:\n",
      "  * Mean preference: 0.359 (std: 0.661)\n",
      "  * Range: -1.000 to 1.000\n",
      "  * Users with positive preference: 5434 (67.9%)\n",
      "  * Users with negative preference: 2282 (28.5%)\n",
      "- Crime:\n",
      "  * Mean preference: 0.156 (std: 0.374)\n",
      "  * Range: -1.000 to 1.000\n",
      "  * Users with positive preference: 4952 (61.9%)\n",
      "  * Users with negative preference: 2386 (29.8%)\n",
      "- War:\n",
      "  * Mean preference: 0.085 (std: 0.198)\n",
      "  * Range: -1.000 to 1.000\n",
      "  * Users with positive preference: 4735 (59.2%)\n",
      "  * Users with negative preference: 1752 (21.9%)\n",
      "- Mystery:\n",
      "  * Mean preference: 0.064 (std: 0.227)\n",
      "  * Range: -1.000 to 1.000\n",
      "  * Users with positive preference: 4356 (54.4%)\n",
      "  * Users with negative preference: 2326 (29.1%)\n",
      "- Thriller:\n",
      "  * Mean preference: 0.046 (std: 0.536)\n",
      "  * Range: -1.000 to 1.000\n",
      "  * Users with positive preference: 4293 (53.7%)\n",
      "  * Users with negative preference: 3267 (40.8%)\n",
      "\n",
      "User preference distributions saved to ./rec/collaborative-recommendations\\user_preference_distributions.png\n",
      "\n",
      "Genre preference correlation matrix saved to ./rec/collaborative-recommendations\\genre_preference_correlation.png\n",
      "\n",
      "Example users with diverse preferences:\n",
      "\n",
      "User 8374.0 (diversity score: 0.658):\n",
      "- Most liked genres: Animation (1.00), Children (1.00), Musical (1.00)\n",
      "- Most disliked genres: Comedy (-1.00), Crime (-1.00), Drama (-1.00)\n",
      "\n",
      "User 1109.0 (diversity score: 0.646):\n",
      "- Most liked genres: Romance (1.00), War (1.00), Comedy (0.50)\n",
      "- Most disliked genres: Action (-1.00), Fantasy (-1.00), IMAX (-1.00)\n",
      "\n",
      "User 1047.0 (diversity score: 0.634):\n",
      "- Most liked genres: Comedy (1.00), Mystery (1.00), Romance (1.00)\n",
      "- Most disliked genres: Children (-1.00), Sci-Fi (-1.00), Action (-0.50)\n",
      "\n",
      "Sample of user genre preferences:\n",
      "\n",
      "User 2326.0 preferences:\n",
      "- Drama: 1.000\n",
      "- Romance: 1.000\n",
      "- Documentary: 0.333\n",
      "- Animation: 0.167\n",
      "- Comedy: 0.167\n",
      "\n",
      "User 9984.0 preferences:\n",
      "- Action: -1.000\n",
      "- Drama: -0.800\n",
      "- Thriller: -0.700\n",
      "- Mystery: -0.500\n",
      "- Sci-Fi: -0.500\n",
      "\n",
      "User 479.0 preferences:\n",
      "- Children: -1.000\n",
      "- Comedy: -0.835\n",
      "- Adventure: -0.759\n",
      "- Fantasy: -0.430\n",
      "- Action: -0.392\n",
      "\n",
      "Saved user genre preferences to ./rec/collaborative-recommendations\\user_genre_preferences.csv\n",
      "\n",
      "================================================================================\n",
      "STEP 4: DNN TRAINING DATA PREPARATION\n",
      "================================================================================\n",
      "Preparing training data for DNN model...\n",
      "Using 1000000 ratings to train the DNN model\n",
      "Processed 10000/1000000 ratings (1.0%)\n",
      "Processed 20000/1000000 ratings (2.0%)\n",
      "Processed 30000/1000000 ratings (3.0%)\n",
      "Processed 40000/1000000 ratings (4.0%)\n",
      "Processed 50000/1000000 ratings (5.0%)\n",
      "Processed 60000/1000000 ratings (6.0%)\n",
      "Processed 70000/1000000 ratings (7.0%)\n",
      "Processed 80000/1000000 ratings (8.0%)\n",
      "Processed 90000/1000000 ratings (9.0%)\n",
      "Processed 100000/1000000 ratings (10.0%)\n",
      "Processed 110000/1000000 ratings (11.0%)\n",
      "Processed 120000/1000000 ratings (12.0%)\n",
      "Processed 130000/1000000 ratings (13.0%)\n",
      "Processed 140000/1000000 ratings (14.0%)\n",
      "Processed 150000/1000000 ratings (15.0%)\n",
      "Processed 160000/1000000 ratings (16.0%)\n",
      "Processed 170000/1000000 ratings (17.0%)\n",
      "Processed 180000/1000000 ratings (18.0%)\n",
      "Processed 190000/1000000 ratings (19.0%)\n",
      "Processed 200000/1000000 ratings (20.0%)\n",
      "Processed 210000/1000000 ratings (21.0%)\n",
      "Processed 220000/1000000 ratings (22.0%)\n",
      "Processed 230000/1000000 ratings (23.0%)\n",
      "Processed 240000/1000000 ratings (24.0%)\n",
      "Processed 250000/1000000 ratings (25.0%)\n",
      "Processed 260000/1000000 ratings (26.0%)\n",
      "Processed 270000/1000000 ratings (27.0%)\n",
      "Processed 280000/1000000 ratings (28.0%)\n",
      "Processed 290000/1000000 ratings (29.0%)\n",
      "Processed 300000/1000000 ratings (30.0%)\n",
      "Processed 310000/1000000 ratings (31.0%)\n",
      "Processed 320000/1000000 ratings (32.0%)\n",
      "Processed 330000/1000000 ratings (33.0%)\n",
      "Processed 340000/1000000 ratings (34.0%)\n",
      "Processed 350000/1000000 ratings (35.0%)\n",
      "Processed 360000/1000000 ratings (36.0%)\n",
      "Processed 370000/1000000 ratings (37.0%)\n",
      "Processed 380000/1000000 ratings (38.0%)\n",
      "Processed 390000/1000000 ratings (39.0%)\n",
      "Processed 400000/1000000 ratings (40.0%)\n",
      "Processed 410000/1000000 ratings (41.0%)\n",
      "Processed 420000/1000000 ratings (42.0%)\n",
      "Processed 430000/1000000 ratings (43.0%)\n",
      "Processed 440000/1000000 ratings (44.0%)\n",
      "Processed 450000/1000000 ratings (45.0%)\n",
      "Processed 460000/1000000 ratings (46.0%)\n",
      "Processed 470000/1000000 ratings (47.0%)\n",
      "Processed 480000/1000000 ratings (48.0%)\n",
      "Processed 490000/1000000 ratings (49.0%)\n",
      "Processed 500000/1000000 ratings (50.0%)\n",
      "Processed 510000/1000000 ratings (51.0%)\n",
      "Processed 520000/1000000 ratings (52.0%)\n",
      "Processed 530000/1000000 ratings (53.0%)\n",
      "Processed 540000/1000000 ratings (54.0%)\n",
      "Processed 550000/1000000 ratings (55.0%)\n",
      "Processed 560000/1000000 ratings (56.0%)\n",
      "Processed 570000/1000000 ratings (57.0%)\n",
      "Processed 580000/1000000 ratings (58.0%)\n",
      "Processed 590000/1000000 ratings (59.0%)\n",
      "Processed 600000/1000000 ratings (60.0%)\n",
      "Processed 610000/1000000 ratings (61.0%)\n",
      "Processed 620000/1000000 ratings (62.0%)\n",
      "Processed 630000/1000000 ratings (63.0%)\n",
      "Processed 640000/1000000 ratings (64.0%)\n",
      "Processed 650000/1000000 ratings (65.0%)\n",
      "Processed 660000/1000000 ratings (66.0%)\n",
      "Processed 670000/1000000 ratings (67.0%)\n",
      "Processed 680000/1000000 ratings (68.0%)\n",
      "Processed 690000/1000000 ratings (69.0%)\n",
      "Processed 700000/1000000 ratings (70.0%)\n",
      "Processed 710000/1000000 ratings (71.0%)\n",
      "Processed 720000/1000000 ratings (72.0%)\n",
      "Processed 730000/1000000 ratings (73.0%)\n",
      "Processed 740000/1000000 ratings (74.0%)\n",
      "Processed 750000/1000000 ratings (75.0%)\n",
      "Processed 760000/1000000 ratings (76.0%)\n",
      "Processed 770000/1000000 ratings (77.0%)\n",
      "Processed 780000/1000000 ratings (78.0%)\n",
      "Processed 790000/1000000 ratings (79.0%)\n",
      "Processed 800000/1000000 ratings (80.0%)\n",
      "Processed 810000/1000000 ratings (81.0%)\n",
      "Processed 820000/1000000 ratings (82.0%)\n",
      "Processed 830000/1000000 ratings (83.0%)\n",
      "Processed 840000/1000000 ratings (84.0%)\n",
      "Processed 850000/1000000 ratings (85.0%)\n",
      "Processed 860000/1000000 ratings (86.0%)\n",
      "Processed 870000/1000000 ratings (87.0%)\n",
      "Processed 880000/1000000 ratings (88.0%)\n",
      "Processed 890000/1000000 ratings (89.0%)\n",
      "Processed 900000/1000000 ratings (90.0%)\n",
      "Processed 910000/1000000 ratings (91.0%)\n",
      "Processed 920000/1000000 ratings (92.0%)\n",
      "Processed 930000/1000000 ratings (93.0%)\n",
      "Processed 940000/1000000 ratings (94.0%)\n",
      "Processed 950000/1000000 ratings (95.0%)\n",
      "Processed 960000/1000000 ratings (96.0%)\n",
      "Processed 970000/1000000 ratings (97.0%)\n",
      "Processed 980000/1000000 ratings (98.0%)\n",
      "Processed 990000/1000000 ratings (99.0%)\n",
      "Processed 1000000/1000000 ratings (100.0%)\n",
      "Created feature matrix with shape (1000000, 40) and labels with shape (1000000,)\n",
      "Prepared training data with 800000 samples, validation data with 200000 samples\n",
      "\n",
      "--------------------------------------------------\n",
      "DATA ANALYSIS: DNN TRAINING DATA\n",
      "--------------------------------------------------\n",
      "\n",
      "Feature Vector Dimension: 40\n",
      "Number of genres: 20\n",
      "Features per genre: 2 (user preference + movie indicator)\n",
      "Total features: 40\n",
      "\n",
      "Training Labels Distribution:\n",
      "Training labels distribution saved to ./rec/collaborative-recommendations\\training_labels_distribution.png\n",
      "\n",
      "Feature Statistics:\n",
      "\n",
      "Feature statistics by genre (top 5 genres):\n",
      "        Genre  User_Pref_Mean  User_Pref_Std  Movie_Ind_Mean  Movie_Ind_Std\n",
      "8       Drama        0.254780       0.728525        0.441955       0.498463\n",
      "6       Crime        0.107365       0.345230        0.163153       0.368616\n",
      "18        War        0.070448       0.160797        0.052616       0.223119\n",
      "14    Mystery        0.046645       0.201174        0.077827       0.269074\n",
      "10  Film-Noir        0.024457       0.075219        0.010760       0.102753\n",
      "Feature distributions saved to ./rec/collaborative-recommendations\\feature_distributions.png\n",
      "\n",
      "Sample of DNN training data (showing first 3 genres for 1 sample):\n",
      "(no genres listed)_user_pref    0.000000\n",
      "Action_user_pref                1.000000\n",
      "Adventure_user_pref             0.444444\n",
      "(no genres listed)_movie_ind    0.000000\n",
      "Action_movie_ind                1.000000\n",
      "Adventure_movie_ind             0.000000\n",
      "rating                          5.000000\n",
      "\n",
      "================================================================================\n",
      "STEP 5: DNN MODEL BUILDING AND TRAINING\n",
      "================================================================================\n",
      "Building and training DNN model...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NCPC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1ms/step - loss: 2.2039 - mae: 1.0871 - val_loss: 0.8860 - val_mae: 0.7248\n",
      "Epoch 2/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1ms/step - loss: 0.9132 - mae: 0.7395 - val_loss: 0.8741 - val_mae: 0.7151\n",
      "Epoch 3/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1ms/step - loss: 0.8915 - mae: 0.7276 - val_loss: 0.8698 - val_mae: 0.7164\n",
      "Epoch 4/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1ms/step - loss: 0.8841 - mae: 0.7242 - val_loss: 0.8645 - val_mae: 0.7130\n",
      "Epoch 5/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1ms/step - loss: 0.8803 - mae: 0.7220 - val_loss: 0.8621 - val_mae: 0.7100\n",
      "Epoch 6/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1ms/step - loss: 0.8799 - mae: 0.7221 - val_loss: 0.8608 - val_mae: 0.7130\n",
      "Epoch 7/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1ms/step - loss: 0.8760 - mae: 0.7211 - val_loss: 0.8577 - val_mae: 0.7123\n",
      "Epoch 8/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1ms/step - loss: 0.8773 - mae: 0.7209 - val_loss: 0.8609 - val_mae: 0.7109\n",
      "Epoch 9/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1ms/step - loss: 0.8733 - mae: 0.7196 - val_loss: 0.8559 - val_mae: 0.7119\n",
      "Epoch 10/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1ms/step - loss: 0.8750 - mae: 0.7208 - val_loss: 0.8562 - val_mae: 0.7108\n",
      "Epoch 11/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1ms/step - loss: 0.8722 - mae: 0.7189 - val_loss: 0.8551 - val_mae: 0.7091\n",
      "Epoch 12/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1ms/step - loss: 0.8719 - mae: 0.7189 - val_loss: 0.8551 - val_mae: 0.7081\n",
      "Epoch 13/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1ms/step - loss: 0.8728 - mae: 0.7197 - val_loss: 0.8586 - val_mae: 0.7096\n",
      "Epoch 14/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1ms/step - loss: 0.8702 - mae: 0.7185 - val_loss: 0.8527 - val_mae: 0.7071\n",
      "Epoch 15/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1ms/step - loss: 0.8678 - mae: 0.7178 - val_loss: 0.8546 - val_mae: 0.7117\n",
      "Epoch 16/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1ms/step - loss: 0.8727 - mae: 0.7195 - val_loss: 0.8515 - val_mae: 0.7087\n",
      "Epoch 17/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1ms/step - loss: 0.8703 - mae: 0.7187 - val_loss: 0.8515 - val_mae: 0.7100\n",
      "Epoch 18/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1ms/step - loss: 0.8686 - mae: 0.7176 - val_loss: 0.8531 - val_mae: 0.7098\n",
      "Epoch 19/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1ms/step - loss: 0.8701 - mae: 0.7183 - val_loss: 0.8506 - val_mae: 0.7079\n",
      "Epoch 20/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - loss: 0.8697 - mae: 0.7180 - val_loss: 0.8497 - val_mae: 0.7083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 19:00:50,603 : WARNING : You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training completed. Validation MSE: 0.8497, validation MAE: 0.7083\n",
      "Saved DNN model to ./rec/collaborative-recommendations\\dnn_model.h5\n",
      "\n",
      "--------------------------------------------------\n",
      "MODEL ANALYSIS: DNN TRAINING RESULTS\n",
      "--------------------------------------------------\n",
      "Training history plot saved to ./rec/collaborative-recommendations\\dnn_training_history.png\n",
      "\n",
      "DNN Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,624</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,624\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,645</span> (65.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m16,645\u001b[0m (65.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,473</span> (21.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,473\u001b[0m (21.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> (896.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m224\u001b[0m (896.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,948</span> (42.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m10,948\u001b[0m (42.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Training Metrics:\n",
      "- MSE Loss: 0.8690\n",
      "- MAE: 0.7179\n",
      "\n",
      "Final Validation Metrics:\n",
      "- MSE Loss: 0.8497\n",
      "- MAE: 0.7083\n",
      "\n",
      "Final RMSE:\n",
      "- Training RMSE: 0.9322\n",
      "- Validation RMSE: 0.9218\n",
      "\n",
      "Prediction Quality Analysis:\n",
      "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 659us/step\n",
      "Prediction error distribution saved to ./rec/collaborative-recommendations\\prediction_error_distribution.png\n",
      "Predicted vs actual plot saved to ./rec/collaborative-recommendations\\predicted_vs_actual.png\n",
      "\n",
      "Error by rating level:\n",
      "     count  mean_error  abs_error      rmse\n",
      "0.5   2456    2.563294   2.564002  2.617478\n",
      "1.0   6861    2.143149   2.143992  2.190667\n",
      "1.5   2629    1.589153   1.589153  1.636626\n",
      "2.0  14678    1.214228   1.214956  1.284562\n",
      "2.5   8741    0.680036   0.684804  0.785856\n",
      "3.0  43470    0.344438   0.427744  0.545486\n",
      "3.5  21369   -0.097266   0.366026  0.435585\n",
      "4.0  55544   -0.326390   0.413078  0.532367\n",
      "4.5  15199   -0.701196   0.701671  0.811758\n",
      "5.0  29053   -1.123168   1.123168  1.184714\n",
      "Error by rating plot saved to ./rec/collaborative-recommendations\\error_by_rating.png\n",
      "\n",
      "================================================================================\n",
      "STEP 6: MOVIE RECOMMENDATION GENERATION\n",
      "================================================================================\n",
      "Generating top-20 DNN recommendations for all users with optimized batching...\n",
      "Limiting to 200 users out of 8000 total users\n",
      "Creating user rating lookup dictionary...\n",
      "Processing batch of 50 users (1-50 of 200)\n",
      "  Processed 10/50 users in batch, avg time: 0.81s per user\n",
      "  Processed 20/50 users in batch, avg time: 0.79s per user\n",
      "  Processed 30/50 users in batch, avg time: 0.82s per user\n",
      "  Processed 40/50 users in batch, avg time: 0.83s per user\n",
      "  Processed 50/50 users in batch, avg time: 0.83s per user\n",
      "Completed batch 1/4\n",
      "Progress: 25.0% - Elapsed: 41.28s - Est. remaining: 123.84s\n",
      "Processing batch of 50 users (51-100 of 200)\n",
      "  Processed 10/50 users in batch, avg time: 0.87s per user\n",
      "  Processed 20/50 users in batch, avg time: 0.81s per user\n",
      "  Processed 30/50 users in batch, avg time: 0.79s per user\n",
      "  Processed 40/50 users in batch, avg time: 0.77s per user\n",
      "  Processed 50/50 users in batch, avg time: 0.77s per user\n",
      "Completed batch 2/4\n",
      "Progress: 50.0% - Elapsed: 79.94s - Est. remaining: 159.88s\n",
      "Processing batch of 50 users (101-150 of 200)\n",
      "  Processed 10/50 users in batch, avg time: 0.74s per user\n",
      "  Processed 20/50 users in batch, avg time: 0.74s per user\n",
      "  Processed 30/50 users in batch, avg time: 0.74s per user\n",
      "  Processed 40/50 users in batch, avg time: 0.74s per user\n",
      "  Processed 50/50 users in batch, avg time: 0.75s per user\n",
      "Completed batch 3/4\n",
      "Progress: 75.0% - Elapsed: 117.46s - Est. remaining: 117.46s\n",
      "Processing batch of 50 users (151-200 of 200)\n",
      "  Processed 10/50 users in batch, avg time: 0.75s per user\n",
      "  Processed 20/50 users in batch, avg time: 0.75s per user\n",
      "  Processed 30/50 users in batch, avg time: 0.76s per user\n",
      "  Processed 40/50 users in batch, avg time: 0.76s per user\n",
      "  Processed 50/50 users in batch, avg time: 0.76s per user\n",
      "Completed batch 4/4\n",
      "Progress: 100.0% - Elapsed: 155.83s - Est. remaining: 0.00s\n",
      "Generated recommendations for 200 users\n",
      "\n",
      "--------------------------------------------------\n",
      "RECOMMENDATION ANALYSIS: DNN RECOMMENDATIONS\n",
      "--------------------------------------------------\n",
      "Saved recommendations to ./rec/collaborative-recommendations\\dnn_recommendations.csv\n",
      "\n",
      "Recommendation Statistics:\n",
      "- Users with recommendations: 200\n",
      "- Total recommendation entries: 4000\n",
      "- Average recommendations per user: 20.00\n",
      "\n",
      "Predicted Rating Distribution:\n",
      "- Min: 3.28\n",
      "- Max: 4.70\n",
      "- Mean: 4.12\n",
      "- Median: 4.21\n",
      "- Std Dev: 0.25\n",
      "Recommendation rating distribution saved to ./rec/collaborative-recommendations\\recommendation_rating_distribution.png\n",
      "\n",
      "Top Recommended Movies:\n",
      "1. 'Maltese Falcon, The (1941)' - Recommended to 94 users\n",
      "2. 'Underneath (1995)' - Recommended to 63 users\n",
      "3. 'Just Cause (1995)' - Recommended to 62 users\n",
      "4. 'True Crime (1996)' - Recommended to 61 users\n",
      "5. 'Dead Man (1995)' - Recommended to 60 users\n",
      "6. 'Before the Rain (Pred dozhdot) (1994)' - Recommended to 60 users\n",
      "7. 'Misérables, Les (1995)' - Recommended to 58 users\n",
      "8. 'Richard III (1995)' - Recommended to 58 users\n",
      "9. 'Shanghai Triad (Yao a yao yao dao waipo qiao) (1995)' - Recommended to 57 users\n",
      "10. 'Walking Dead, The (1995)' - Recommended to 57 users\n",
      "Top recommendations genre distribution saved to ./rec/collaborative-recommendations\\top_recommendations_genre_distribution.png\n",
      "\n",
      "Sample Recommendations for 3 Users:\n",
      "\n",
      "User 1:\n",
      "1. African Queen, The (1951) - Predicted Rating: 4.43\n",
      "2. Cutthroat Island (1995) - Predicted Rating: 4.28\n",
      "3. Adventures of Robin Hood, The (1938) - Predicted Rating: 4.28\n",
      "4. Three Musketeers, The (1993) - Predicted Rating: 4.24\n",
      "5. Bottle Rocket (1996) - Predicted Rating: 4.20\n",
      "\n",
      "User 3:\n",
      "1. Seven (a.k.a. Se7en) (1995) - Predicted Rating: 4.47\n",
      "2. Just Cause (1995) - Predicted Rating: 4.47\n",
      "3. Underneath (1995) - Predicted Rating: 4.47\n",
      "4. True Crime (1996) - Predicted Rating: 4.47\n",
      "5. Pulp Fiction (1994) - Predicted Rating: 4.46\n",
      "\n",
      "User 4:\n",
      "1. Dumbo (1941) - Predicted Rating: 4.19\n",
      "2. Lion King, The (1994) - Predicted Rating: 4.18\n",
      "3. Aladdin (1992) - Predicted Rating: 4.17\n",
      "4. Oliver & Company (1988) - Predicted Rating: 4.17\n",
      "5. Pocahontas (1995) - Predicted Rating: 4.09\n",
      "\n",
      "================================================================================\n",
      "STEP 7: EVALUATION\n",
      "================================================================================\n",
      "\n",
      "Evaluating DNN recommendations...\n",
      "Evaluating recommendations using RMSE and MAE...\n",
      "Train users: 8000, Test users: 2000, Common users: 0\n",
      "Using user-based split - no common users between train and test.\n",
      "Evaluating using average rating for all predictions instead.\n",
      "Baseline evaluation results (using avg rating 3.53):\n",
      "RMSE: 1.0382\n",
      "MAE: 0.8301\n",
      "Number of predictions: 301199\n",
      "Saved evaluation metrics to ./rec/collaborative-recommendations\\dnn_evaluation.csv\n",
      "\n",
      "--------------------------------------------------\n",
      "EVALUATION ANALYSIS: MODEL PERFORMANCE\n",
      "--------------------------------------------------\n",
      "\n",
      "Evaluation Metrics:\n",
      "- rmse: 1.0382\n",
      "- mae: 0.8301\n",
      "- num_predictions: 301199\n",
      "- method: baseline_average_rating\n",
      "\n",
      "Sample recommendation for exploration:\n",
      "\n",
      "User 13 Genre Preferences:\n",
      "- Most liked genres: Drama (1.00), Thriller (0.75), War (0.42)\n",
      "- Most disliked genres: Children (-0.42), Comedy (-0.42), Adventure (-0.25)\n",
      "\n",
      "Top 10 recommendations for user 13:\n",
      "1. Dead Man (1995) - Predicted Rating: 4.27\n",
      "2. Lone Star (1996) - Predicted Rating: 4.27\n",
      "3. Maltese Falcon, The (1941) - Predicted Rating: 4.21\n",
      "4. Unforgettable (1996) - Predicted Rating: 4.20\n",
      "5. Just Cause (1995) - Predicted Rating: 4.20\n",
      "6. Underneath (1995) - Predicted Rating: 4.20\n",
      "7. True Crime (1996) - Predicted Rating: 4.20\n",
      "8. Rear Window (1954) - Predicted Rating: 4.20\n",
      "9. Richard III (1995) - Predicted Rating: 4.19\n",
      "10. Misérables, Les (1995) - Predicted Rating: 4.19\n",
      "\n",
      "================================================================================\n",
      "SUMMARY: COLLABORATIVE FILTERING WITH DNN\n",
      "================================================================================\n",
      "\n",
      "Model Characteristics:\n",
      "- Hidden layer sizes: [64, 32, 16]\n",
      "- Dropout rate: 0.2\n",
      "- Learning rate: 0.001\n",
      "- Batch size: 64\n",
      "\n",
      "Dataset Statistics:\n",
      "- Training samples: 800000\n",
      "- Validation samples: 200000\n",
      "- Feature dimensions: 40\n",
      "- Number of users with genre preferences: 8000\n",
      "- Number of movies with genre features: 15597\n",
      "\n",
      "Performance Metrics:\n",
      "- RMSE: 1.0382\n",
      "- MAE: 0.8301\n",
      "- Predictions evaluated: 301199\n",
      "\n",
      "Comparison with Other Methods:\n",
      "+-------------------------------+--------+--------+-------------+\n",
      "| Model                         | RMSE   | MAE    | Predictions |\n",
      "+-------------------------------+--------+--------+-------------+\n",
      "| Collaborative Filtering (DNN) | 1.0382 | 0.8301 | 301199      |\n",
      "+-------------------------------+--------+--------+-------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import time\n",
    "import gc  # For garbage collection\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COLLABORATIVE FILTERING WITH DEEP NEURAL NETWORK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Set paths\n",
    "input_path = \"./processed/\"  # Current directory where stage1.py saved the files\n",
    "output_path = \"./rec/collaborative-recommendations\"\n",
    "top_n = 20\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Model parameters\n",
    "dnn_hidden_layers = [64, 32, 16]  # Optimized architecture \n",
    "dnn_dropout_rate = 0.2\n",
    "dnn_learning_rate = 0.001\n",
    "dnn_batch_size = 64   # Increased batch size for faster training\n",
    "dnn_epochs = 20       # Reduced epochs with early stopping\n",
    "threshold_rating = 3.5  # Rating threshold to classify as \"like\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: DATA LOADING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load processed data from stage1.py\n",
    "    \n",
    "    Input: None (reads from files)\n",
    "    Output: Dictionary containing DataFrames for movie features and ratings\n",
    "    \"\"\"\n",
    "    print(\"Loading processed data from stage1.py...\")\n",
    "    \n",
    "    # Data containers\n",
    "    data = {}\n",
    "    \n",
    "    # Load movie features\n",
    "    movie_features_path = os.path.join(input_path, 'processed_movie_features.csv')\n",
    "    if os.path.exists(movie_features_path):\n",
    "        data['movie_features'] = pd.read_csv(movie_features_path)\n",
    "        print(f\"Loaded features for {len(data['movie_features'])} movies\")\n",
    "    else:\n",
    "        print(f\"Movie features not found at {movie_features_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Load normalized ratings\n",
    "    ratings_path = os.path.join(input_path, 'normalized_ratings.csv')\n",
    "    if os.path.exists(ratings_path):\n",
    "        data['ratings'] = pd.read_csv(ratings_path)\n",
    "        print(f\"Loaded {len(data['ratings'])} normalized ratings\")\n",
    "    else:\n",
    "        print(f\"Normalized ratings not found at {ratings_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Create training and testing sets with 80-20 split\n",
    "    if 'ratings' in data:\n",
    "        # Get all unique user IDs\n",
    "        all_user_ids = data['ratings']['userId'].unique()\n",
    "\n",
    "        # Split users into train (80%) and test (20%) sets\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        np.random.shuffle(all_user_ids)\n",
    "\n",
    "        split_idx = int(len(all_user_ids) * 0.8)\n",
    "        train_users = all_user_ids[:split_idx]\n",
    "        test_users = all_user_ids[split_idx:]\n",
    "\n",
    "        # Split ratings based on user assignments\n",
    "        data['train_ratings'] = data['ratings'][data['ratings']['userId'].isin(train_users)]\n",
    "        data['test_ratings'] = data['ratings'][data['ratings']['userId'].isin(test_users)]\n",
    "        \n",
    "        print(f\"Split ratings into {len(data['train_ratings'])} training and {len(data['test_ratings'])} testing samples\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load the data\n",
    "data = load_data()\n",
    "if data is None:\n",
    "    print(\"Failed to load required data\")\n",
    "    exit(1)\n",
    "\n",
    "# Analyze the loaded data\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"DATA ANALYSIS: LOADED DATASETS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Show movie features summary\n",
    "if 'movie_features' in data:\n",
    "    print(f\"\\nMovie Features Summary:\")\n",
    "    print(f\"- Total movies: {len(data['movie_features'])}\")\n",
    "    \n",
    "    # Get genre columns\n",
    "    genre_columns = [col for col in data['movie_features'].columns if col not in \n",
    "                     ['movieId', 'title', 'tokens', 'token_count', 'top_keywords']]\n",
    "    \n",
    "    print(f\"- Number of genres: {len(genre_columns)}\")\n",
    "    print(f\"- Genre columns: {genre_columns}\")\n",
    "    \n",
    "    # Show sample movie features\n",
    "    print(\"\\nSample movie features:\")\n",
    "    print(data['movie_features'][['movieId', 'title'] + genre_columns[:3]].head(3))\n",
    "\n",
    "# Show ratings summary\n",
    "if 'ratings' in data:\n",
    "    print(f\"\\nRatings Summary:\")\n",
    "    print(f\"- Total ratings: {len(data['ratings'])}\")\n",
    "    print(f\"- Unique users: {data['ratings']['userId'].nunique()}\")\n",
    "    print(f\"- Unique movies: {data['ratings']['movieId'].nunique()}\")\n",
    "    print(f\"- Rating range: {data['ratings']['rating'].min()} - {data['ratings']['rating'].max()}\")\n",
    "    print(f\"- Average rating: {data['ratings']['rating'].mean():.2f}\")\n",
    "    \n",
    "    # Show rating distribution\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(data=data['ratings'], x='rating', bins=9, kde=True)\n",
    "    plt.title('Distribution of Ratings')\n",
    "    plt.xlabel('Rating')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig(os.path.join(output_path, 'rating_distribution.png'))\n",
    "    print(f\"\\nRating distribution plot saved to {os.path.join(output_path, 'rating_distribution.png')}\")\n",
    "    plt.close()\n",
    "\n",
    "# Show train/test split summary\n",
    "if 'train_ratings' in data and 'test_ratings' in data:\n",
    "    print(f\"\\nTrain/Test Split Summary:\")\n",
    "    print(f\"- Training ratings: {len(data['train_ratings'])} ({len(data['train_ratings'])/len(data['ratings'])*100:.1f}%)\")\n",
    "    print(f\"- Testing ratings: {len(data['test_ratings'])} ({len(data['test_ratings'])/len(data['ratings'])*100:.1f}%)\")\n",
    "    print(f\"- Training users: {data['train_ratings']['userId'].nunique()}\")\n",
    "    print(f\"- Testing users: {data['test_ratings']['userId'].nunique()}\")\n",
    "    \n",
    "    # Analyze user rating distribution in train/test sets\n",
    "    train_ratings_per_user = data['train_ratings'].groupby('userId').size()\n",
    "    test_ratings_per_user = data['test_ratings'].groupby('userId').size()\n",
    "    \n",
    "    print(f\"\\nRatings per user:\")\n",
    "    print(f\"- Training set - Avg: {train_ratings_per_user.mean():.2f}, Min: {train_ratings_per_user.min()}, Max: {train_ratings_per_user.max()}\")\n",
    "    print(f\"- Testing set - Avg: {test_ratings_per_user.mean():.2f}, Min: {test_ratings_per_user.min()}, Max: {test_ratings_per_user.max()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: MOVIE GENRE FEATURE EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def extract_genre_features(movie_features):\n",
    "    \"\"\"\n",
    "    Extract genre features for each movie\n",
    "    \n",
    "    Input: \n",
    "      - movie_features: DataFrame with movie features including genre columns\n",
    "    \n",
    "    Output:\n",
    "      - movie_genre_features: DataFrame with movieId and genre columns only\n",
    "    \"\"\"\n",
    "    print(\"Extracting genre features for movies...\")\n",
    "    \n",
    "    # Get all genre columns (assuming they're already one-hot encoded)\n",
    "    genre_columns = [col for col in movie_features.columns if col not in \n",
    "                     ['movieId', 'title', 'tokens', 'token_count', 'top_keywords']]\n",
    "    \n",
    "    if not genre_columns:\n",
    "        print(\"No genre columns found in movie features\")\n",
    "        return None\n",
    "    \n",
    "    # Create genre feature matrix\n",
    "    movie_genre_features = movie_features[['movieId'] + genre_columns].copy()\n",
    "    \n",
    "    print(f\"Extracted {len(genre_columns)} genre features for {len(movie_features)} movies\")\n",
    "    \n",
    "    return movie_genre_features\n",
    "\n",
    "# Extract genre features\n",
    "movie_genre_features = extract_genre_features(data['movie_features'])\n",
    "if movie_genre_features is None:\n",
    "    print(\"Failed to extract genre features\")\n",
    "    exit(1)\n",
    "\n",
    "# Analyze the extracted genre features\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"DATA ANALYSIS: GENRE FEATURES\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Show genre distribution\n",
    "print(\"\\nGenre Distribution:\")\n",
    "genre_columns = [col for col in movie_genre_features.columns if col != 'movieId']\n",
    "genre_counts = {}\n",
    "\n",
    "for genre in genre_columns:\n",
    "    count = movie_genre_features[genre].sum()\n",
    "    genre_counts[genre] = count\n",
    "    print(f\"- {genre}: {count} movies ({count/len(movie_genre_features)*100:.1f}%)\")\n",
    "\n",
    "# Plot genre distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sorted_genres = sorted(genre_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "genres, counts = zip(*sorted_genres)\n",
    "plt.bar(genres, counts)\n",
    "plt.title('Distribution of Movies by Genre')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Number of Movies')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_path, 'genre_distribution.png'))\n",
    "print(f\"\\nGenre distribution plot saved to {os.path.join(output_path, 'genre_distribution.png')}\")\n",
    "plt.close()\n",
    "\n",
    "# Analyze genre co-occurrence\n",
    "print(\"\\nGenre Co-occurrence Analysis:\")\n",
    "genre_co_occurrence = pd.DataFrame(0, index=genre_columns, columns=genre_columns)\n",
    "\n",
    "for _, row in movie_genre_features.iterrows():\n",
    "    movie_genres = [genre for genre in genre_columns if row[genre] == 1]\n",
    "    for g1 in movie_genres:\n",
    "        for g2 in movie_genres:\n",
    "            genre_co_occurrence.loc[g1, g2] += 1\n",
    "\n",
    "# Normalize by diagonal for correlation-like measure\n",
    "for g in genre_columns:\n",
    "    genre_co_occurrence[g] = genre_co_occurrence[g] / genre_co_occurrence.loc[g, g]\n",
    "\n",
    "# Display most common genre combinations\n",
    "print(\"Most common genre combinations:\")\n",
    "for i, g1 in enumerate(genre_columns[:5]):  # Limit to 5 genres for brevity\n",
    "    most_common = genre_co_occurrence.loc[g1].sort_values(ascending=False)[1:6]  # Skip self (always 1.0)\n",
    "    print(f\"- {g1} most commonly appears with: {', '.join([f'{g2} ({v:.2f})' for g2, v in most_common.items()])}\")\n",
    "\n",
    "# Save genre co-occurrence matrix plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(genre_co_occurrence, annot=False, cmap='viridis')\n",
    "plt.title('Genre Co-occurrence Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_path, 'genre_co_occurrence.png'))\n",
    "print(f\"\\nGenre co-occurrence matrix saved to {os.path.join(output_path, 'genre_co_occurrence.png')}\")\n",
    "plt.close()\n",
    "\n",
    "# Show sample of genre features\n",
    "print(\"\\nSample of movie genre features:\")\n",
    "print(movie_genre_features.head(3))\n",
    "\n",
    "# Save the genre features for later use\n",
    "movie_genre_features.to_csv(os.path.join(output_path, 'movie_genre_features.csv'), index=False)\n",
    "print(f\"\\nSaved movie genre features to {os.path.join(output_path, 'movie_genre_features.csv')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: USER GENRE PREFERENCE CALCULATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def calculate_user_genre_preferences(train_ratings, movie_genre_features):\n",
    "    \"\"\"\n",
    "    Calculate user preferences for movie genres based on ratings\n",
    "    \n",
    "    Input:\n",
    "      - train_ratings: DataFrame with user-movie ratings\n",
    "      - movie_genre_features: DataFrame with movie genre features\n",
    "    \n",
    "    Output:\n",
    "      - user_genre_preferences_df: DataFrame with userId and genre preference scores\n",
    "    \"\"\"\n",
    "    print(\"Calculating user preferences for movie genres...\")\n",
    "    \n",
    "    # Get genre columns\n",
    "    genre_columns = [col for col in movie_genre_features.columns if col != 'movieId']\n",
    "    \n",
    "    # Initialize user genre preferences dataframe\n",
    "    user_genre_preferences = []\n",
    "    \n",
    "    # Process each user\n",
    "    total_users = len(train_ratings['userId'].unique())\n",
    "    processed_users = 0\n",
    "    \n",
    "    for user_id in train_ratings['userId'].unique():\n",
    "        # Get user ratings\n",
    "        user_ratings = train_ratings[train_ratings['userId'] == user_id]\n",
    "        \n",
    "        if len(user_ratings) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Separate liked and disliked movies\n",
    "        liked_movies = user_ratings[user_ratings['rating'] > threshold_rating]['movieId'].values\n",
    "        disliked_movies = user_ratings[user_ratings['rating'] <= threshold_rating]['movieId'].values\n",
    "        \n",
    "        # Calculate genre preferences using equation (7) from the paper:\n",
    "        # R̂g = (Nlikes - Ndislikes) / Max(Nlikes - Ndislikes)\n",
    "        genre_preferences = {}\n",
    "        \n",
    "        for genre in genre_columns:\n",
    "            # Get genre values for liked movies\n",
    "            genre_liked = movie_genre_features[movie_genre_features['movieId'].isin(liked_movies)][genre].sum()\n",
    "            \n",
    "            # Get genre values for disliked movies\n",
    "            genre_disliked = movie_genre_features[movie_genre_features['movieId'].isin(disliked_movies)][genre].sum()\n",
    "            \n",
    "            # Calculate preference\n",
    "            genre_preferences[genre] = genre_liked - genre_disliked\n",
    "        \n",
    "        # Calculate maximum absolute genre preference\n",
    "        max_abs_preference = max(abs(val) for val in genre_preferences.values()) if genre_preferences else 1\n",
    "        \n",
    "        # Normalize preferences to [-1, 1]\n",
    "        for genre in genre_preferences:\n",
    "            genre_preferences[genre] = genre_preferences[genre] / max_abs_preference if max_abs_preference > 0 else 0\n",
    "        \n",
    "        # Add user ID\n",
    "        genre_preferences['userId'] = user_id\n",
    "        \n",
    "        user_genre_preferences.append(genre_preferences)\n",
    "        \n",
    "        # Update progress\n",
    "        processed_users += 1\n",
    "        if processed_users % 100 == 0 or processed_users == total_users:\n",
    "            print(f\"Processed {processed_users}/{total_users} users ({processed_users/total_users*100:.1f}%)\")\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    user_genre_preferences_df = pd.DataFrame(user_genre_preferences)\n",
    "    \n",
    "    print(f\"Calculated genre preferences for {len(user_genre_preferences_df)} users\")\n",
    "    \n",
    "    return user_genre_preferences_df\n",
    "\n",
    "# Calculate user genre preferences\n",
    "user_genre_preferences = calculate_user_genre_preferences(data['train_ratings'], movie_genre_features)\n",
    "\n",
    "# Analyze the user genre preferences\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"DATA ANALYSIS: USER GENRE PREFERENCES\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Show basic statistics of user genre preferences\n",
    "if not user_genre_preferences.empty:\n",
    "    print(\"\\nUser Genre Preferences Summary:\")\n",
    "    \n",
    "    genre_columns = [col for col in user_genre_preferences.columns if col != 'userId']\n",
    "    \n",
    "    # Calculate statistics for each genre\n",
    "    genre_stats = {}\n",
    "    for genre in genre_columns:\n",
    "        stats = {\n",
    "            'mean': user_genre_preferences[genre].mean(),\n",
    "            'min': user_genre_preferences[genre].min(),\n",
    "            'max': user_genre_preferences[genre].max(),\n",
    "            'std': user_genre_preferences[genre].std(),\n",
    "            'positive': (user_genre_preferences[genre] > 0).sum(),\n",
    "            'negative': (user_genre_preferences[genre] < 0).sum(),\n",
    "            'neutral': (user_genre_preferences[genre] == 0).sum()\n",
    "        }\n",
    "        genre_stats[genre] = stats\n",
    "    \n",
    "    # Display statistics for top genres\n",
    "    print(\"\\nStatistics for top genres:\")\n",
    "    top_genres = sorted(genre_stats.items(), key=lambda x: x[1]['positive'], reverse=True)[:5]\n",
    "    \n",
    "    for genre, stats in top_genres:\n",
    "        print(f\"- {genre}:\")\n",
    "        print(f\"  * Mean preference: {stats['mean']:.3f} (std: {stats['std']:.3f})\")\n",
    "        print(f\"  * Range: {stats['min']:.3f} to {stats['max']:.3f}\")\n",
    "        print(f\"  * Users with positive preference: {stats['positive']} ({stats['positive']/len(user_genre_preferences)*100:.1f}%)\")\n",
    "        print(f\"  * Users with negative preference: {stats['negative']} ({stats['negative']/len(user_genre_preferences)*100:.1f}%)\")\n",
    "    \n",
    "    # Plot distribution of preferences for top genres\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, (genre, _) in enumerate(top_genres):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        sns.histplot(user_genre_preferences[genre], kde=True)\n",
    "        plt.title(f'Distribution of {genre} Preferences')\n",
    "        plt.xlabel('Preference Score')\n",
    "        plt.ylabel('Number of Users')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_path, 'user_preference_distributions.png'))\n",
    "    print(f\"\\nUser preference distributions saved to {os.path.join(output_path, 'user_preference_distributions.png')}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a correlation heatmap of genre preferences\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    corr_matrix = user_genre_preferences[genre_columns].corr()\n",
    "    sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')\n",
    "    plt.title('Correlation Between Genre Preferences')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_path, 'genre_preference_correlation.png'))\n",
    "    print(f\"\\nGenre preference correlation matrix saved to {os.path.join(output_path, 'genre_preference_correlation.png')}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Show example users with diverse preferences\n",
    "    print(\"\\nExample users with diverse preferences:\")\n",
    "    # Calculate preference diversity as standard deviation across genres\n",
    "    user_genre_preferences['preference_diversity'] = user_genre_preferences[genre_columns].std(axis=1)\n",
    "    \n",
    "    # Get top 3 users with highest diversity\n",
    "    diverse_users = user_genre_preferences.nlargest(3, 'preference_diversity')\n",
    "    for _, user in diverse_users.iterrows():\n",
    "        user_id = user['userId']\n",
    "        print(f\"\\nUser {user_id} (diversity score: {user['preference_diversity']:.3f}):\")\n",
    "        \n",
    "        # Show top 3 liked and disliked genres\n",
    "        user_prefs = [(genre, user[genre]) for genre in genre_columns]\n",
    "        liked_genres = sorted(user_prefs, key=lambda x: x[1], reverse=True)[:3]\n",
    "        disliked_genres = sorted(user_prefs, key=lambda x: x[1])[:3]\n",
    "        \n",
    "        print(f\"- Most liked genres: {', '.join([f'{g} ({v:.2f})' for g, v in liked_genres])}\")\n",
    "        print(f\"- Most disliked genres: {', '.join([f'{g} ({v:.2f})' for g, v in disliked_genres])}\")\n",
    "    \n",
    "    # Remove the temporary column\n",
    "    user_genre_preferences.drop('preference_diversity', axis=1, inplace=True)\n",
    "    \n",
    "    # Show sample of user genre preferences\n",
    "    print(\"\\nSample of user genre preferences:\")\n",
    "    sample_users = user_genre_preferences.sample(3)\n",
    "    for _, user in sample_users.iterrows():\n",
    "        user_id = user['userId']\n",
    "        print(f\"\\nUser {user_id} preferences:\")\n",
    "        # Show top 5 genres with non-zero preferences\n",
    "        user_prefs = [(genre, user[genre]) for genre in genre_columns if user[genre] != 0]\n",
    "        sorted_prefs = sorted(user_prefs, key=lambda x: abs(x[1]), reverse=True)[:5]\n",
    "        for genre, value in sorted_prefs:\n",
    "            print(f\"- {genre}: {value:.3f}\")\n",
    "\n",
    "    # Save the user genre preferences for later use\n",
    "    user_genre_preferences.to_csv(os.path.join(output_path, 'user_genre_preferences.csv'), index=False)\n",
    "    print(f\"\\nSaved user genre preferences to {os.path.join(output_path, 'user_genre_preferences.csv')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: DNN TRAINING DATA PREPARATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def prepare_dnn_training_data(train_ratings, user_genre_preferences, movie_genre_features):\n",
    "    \"\"\"\n",
    "    Prepare training data for the DNN model\n",
    "    \n",
    "    Input:\n",
    "      - train_ratings: DataFrame with user-movie ratings\n",
    "      - user_genre_preferences: DataFrame with user genre preferences\n",
    "      - movie_genre_features: DataFrame with movie genre features\n",
    "    \n",
    "    Output:\n",
    "      - X_train, X_val: Feature matrices for training and validation\n",
    "      - y_train, y_val: Target values for training and validation\n",
    "      - genre_columns: List of genre column names\n",
    "    \"\"\"\n",
    "    print(\"Preparing training data for DNN model...\")\n",
    "    \n",
    "    # Get genre columns\n",
    "    genre_columns = [col for col in movie_genre_features.columns if col != 'movieId']\n",
    "    \n",
    "    # Initialize lists for features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # Process only a sample of ratings for efficiency\n",
    "    sample_size = min(1000000, len(train_ratings))  # Cap at 1M ratings\n",
    "    sampled_ratings = train_ratings.sample(sample_size, random_state=42) if len(train_ratings) > sample_size else train_ratings\n",
    "    \n",
    "    print(f\"Using {len(sampled_ratings)} ratings to train the DNN model\")\n",
    "    \n",
    "    # Process each rating in batches to avoid memory issues\n",
    "    batch_size = 10000\n",
    "    total_ratings = len(sampled_ratings)\n",
    "    processed_ratings = 0\n",
    "    \n",
    "    for batch_start in range(0, total_ratings, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, total_ratings)\n",
    "        ratings_batch = sampled_ratings.iloc[batch_start:batch_end]\n",
    "        \n",
    "        batch_features = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        for _, row in ratings_batch.iterrows():\n",
    "            user_id = row['userId']\n",
    "            movie_id = row['movieId']\n",
    "            rating = row['rating']\n",
    "            \n",
    "            # Skip if user or movie not found\n",
    "            if user_id not in user_genre_preferences['userId'].values or \\\n",
    "               movie_id not in movie_genre_features['movieId'].values:\n",
    "                continue\n",
    "            \n",
    "            # Get user genre preferences\n",
    "            user_prefs = user_genre_preferences[user_genre_preferences['userId'] == user_id].iloc[0]\n",
    "            \n",
    "            # Get movie genres\n",
    "            movie_genres = movie_genre_features[movie_genre_features['movieId'] == movie_id].iloc[0]\n",
    "            \n",
    "            # Create feature vector by combining user preferences and movie genres\n",
    "            feature_vector = []\n",
    "            \n",
    "            for genre in genre_columns:\n",
    "                # Add user preference for this genre\n",
    "                feature_vector.append(user_prefs[genre])\n",
    "                # Add movie genre indicator\n",
    "                feature_vector.append(movie_genres[genre])\n",
    "            \n",
    "            # Use the actual rating as the target\n",
    "            batch_features.append(feature_vector)\n",
    "            batch_labels.append(rating)\n",
    "        \n",
    "        # Extend the main lists\n",
    "        features.extend(batch_features)\n",
    "        labels.extend(batch_labels)\n",
    "        \n",
    "        # Update progress\n",
    "        processed_ratings += len(ratings_batch)\n",
    "        print(f\"Processed {processed_ratings}/{total_ratings} ratings ({processed_ratings/total_ratings*100:.1f}%)\")\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(features, dtype=np.float32)\n",
    "    y = np.array(labels, dtype=np.float32)\n",
    "    \n",
    "    print(f\"Created feature matrix with shape {X.shape} and labels with shape {y.shape}\")\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"Prepared training data with {len(X_train)} samples, validation data with {len(X_val)} samples\")\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val, genre_columns\n",
    "\n",
    "# Prepare DNN training data\n",
    "X_train, X_val, y_train, y_val, genre_columns = prepare_dnn_training_data(\n",
    "    data['train_ratings'], \n",
    "    user_genre_preferences, \n",
    "    movie_genre_features\n",
    ")\n",
    "\n",
    "# Analyze the training data\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"DATA ANALYSIS: DNN TRAINING DATA\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Feature dimension analysis\n",
    "feature_dim = X_train.shape[1]\n",
    "print(f\"\\nFeature Vector Dimension: {feature_dim}\")\n",
    "print(f\"Number of genres: {len(genre_columns)}\")\n",
    "print(f\"Features per genre: 2 (user preference + movie indicator)\")\n",
    "print(f\"Total features: {len(genre_columns) * 2}\")\n",
    "\n",
    "# Analyze distribution of training labels\n",
    "print(\"\\nTraining Labels Distribution:\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(y_train, bins=9, kde=True)\n",
    "plt.title('Distribution of Training Labels (Ratings)')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig(os.path.join(output_path, 'training_labels_distribution.png'))\n",
    "print(f\"Training labels distribution saved to {os.path.join(output_path, 'training_labels_distribution.png')}\")\n",
    "plt.close()\n",
    "\n",
    "# Analyze feature statistics\n",
    "print(\"\\nFeature Statistics:\")\n",
    "feature_means = np.mean(X_train, axis=0)\n",
    "feature_stds = np.std(X_train, axis=0)\n",
    "\n",
    "# Organize features by genre for better interpretation\n",
    "genre_feature_stats = []\n",
    "for i, genre in enumerate(genre_columns):\n",
    "    # User preference feature is at index 2*i\n",
    "    # Movie indicator feature is at index 2*i + 1\n",
    "    user_pref_idx = 2*i\n",
    "    movie_ind_idx = 2*i + 1\n",
    "    \n",
    "    genre_feature_stats.append({\n",
    "        'Genre': genre,\n",
    "        'User_Pref_Mean': feature_means[user_pref_idx],\n",
    "        'User_Pref_Std': feature_stds[user_pref_idx],\n",
    "        'Movie_Ind_Mean': feature_means[movie_ind_idx],\n",
    "        'Movie_Ind_Std': feature_stds[movie_ind_idx]\n",
    "    })\n",
    "\n",
    "# Convert to dataframe for easier analysis\n",
    "feature_stats_df = pd.DataFrame(genre_feature_stats)\n",
    "print(\"\\nFeature statistics by genre (top 5 genres):\")\n",
    "print(feature_stats_df.sort_values('User_Pref_Mean', ascending=False).head())\n",
    "\n",
    "# Plot feature distributions for a few genres\n",
    "plt.figure(figsize=(15, 10))\n",
    "top_genres = feature_stats_df.sort_values('User_Pref_Mean', ascending=False).head(4)['Genre'].values\n",
    "\n",
    "for i, genre in enumerate(top_genres):\n",
    "    genre_idx = genre_columns.index(genre)\n",
    "    user_pref_idx = 2 * genre_idx\n",
    "    movie_ind_idx = 2 * genre_idx + 1\n",
    "    \n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.histplot(X_train[:, user_pref_idx], label='User Preference', alpha=0.7)\n",
    "    plt.title(f'{genre} - User Preference Distribution')\n",
    "    plt.xlabel('Preference Value')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_path, 'feature_distributions.png'))\n",
    "print(f\"Feature distributions saved to {os.path.join(output_path, 'feature_distributions.png')}\")\n",
    "plt.close()\n",
    "\n",
    "# Save a sample of the training data for reference\n",
    "sample_indices = np.random.choice(len(X_train), min(5, len(X_train)), replace=False)\n",
    "sample_data = []\n",
    "\n",
    "for idx in sample_indices:\n",
    "    features = X_train[idx]\n",
    "    rating = y_train[idx]\n",
    "    \n",
    "    sample_features = {}\n",
    "    for i, genre in enumerate(genre_columns):\n",
    "        user_pref_idx = 2*i\n",
    "        movie_ind_idx = 2*i + 1\n",
    "        \n",
    "        sample_features[f\"{genre}_user_pref\"] = features[user_pref_idx]\n",
    "        sample_features[f\"{genre}_movie_ind\"] = features[movie_ind_idx]\n",
    "    \n",
    "    sample_features['rating'] = rating\n",
    "    sample_data.append(sample_features)\n",
    "\n",
    "sample_df = pd.DataFrame(sample_data)\n",
    "print(\"\\nSample of DNN training data (showing first 3 genres for 1 sample):\")\n",
    "print(sample_df.iloc[0][[f\"{genre}_user_pref\" for genre in genre_columns[:3]] + \n",
    "                       [f\"{genre}_movie_ind\" for genre in genre_columns[:3]] + \n",
    "                       ['rating']].to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: DNN MODEL BUILDING AND TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def build_and_train_dnn_model(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Build and train the DNN model for collaborative filtering\n",
    "    \n",
    "    Input:\n",
    "      - X_train, X_val: Feature matrices for training and validation\n",
    "      - y_train, y_val: Target values for training and validation\n",
    "    \n",
    "    Output:\n",
    "      - model: Trained DNN model\n",
    "      - history: Training history\n",
    "    \"\"\"\n",
    "    print(\"Building and training DNN model...\")\n",
    "    \n",
    "    # Set memory limit to avoid OOM errors\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f\"Found {len(gpus)} GPU(s), enabled memory growth\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error setting GPU memory growth: {e}\")\n",
    "    \n",
    "    # Define input dimension\n",
    "    input_dim = X_train.shape[1]\n",
    "    \n",
    "    # Build model based on optimized architecture\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(Dense(dnn_hidden_layers[0], input_dim=input_dim, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dnn_dropout_rate))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for units in dnn_hidden_layers[1:]:\n",
    "        model.add(Dense(units, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dnn_dropout_rate))\n",
    "    \n",
    "    # Output layer - single value for rating prediction\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compile model with Adam optimizer and Mean Squared Error loss for regression\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=dnn_learning_rate),\n",
    "        loss='mse',  # Use MSE for regression\n",
    "        metrics=['mae']  # Track mean absolute error during training\n",
    "    )\n",
    "    \n",
    "    # Define early stopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=dnn_epochs,\n",
    "        batch_size=dnn_batch_size,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
    "    \n",
    "    print(f\"Model training completed. Validation MSE: {val_loss:.4f}, validation MAE: {val_mae:.4f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Build and train DNN model\n",
    "dnn_model, training_history = build_and_train_dnn_model(X_train, X_val, y_train, y_val)\n",
    "\n",
    "# Save DNN model\n",
    "dnn_model.save(os.path.join(output_path, 'dnn_model.h5'))\n",
    "print(f\"Saved DNN model to {os.path.join(output_path, 'dnn_model.h5')}\")\n",
    "\n",
    "# Analyze the training results\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"MODEL ANALYSIS: DNN TRAINING RESULTS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot MSE loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_history.history['loss'], label='Training MSE')\n",
    "plt.plot(training_history.history['val_loss'], label='Validation MSE')\n",
    "plt.title('Model MSE Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend()\n",
    "\n",
    "# Plot MAE\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_history.history['mae'], label='Training MAE')\n",
    "plt.plot(training_history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Model MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_path, 'dnn_training_history.png'))\n",
    "print(f\"Training history plot saved to {os.path.join(output_path, 'dnn_training_history.png')}\")\n",
    "plt.close()\n",
    "\n",
    "# Model architecture summary\n",
    "print(\"\\nDNN Model Architecture:\")\n",
    "dnn_model.summary()\n",
    "\n",
    "# Analyze convergence\n",
    "final_train_loss = training_history.history['loss'][-1]\n",
    "final_val_loss = training_history.history['val_loss'][-1]\n",
    "final_train_mae = training_history.history['mae'][-1]\n",
    "final_val_mae = training_history.history['val_mae'][-1]\n",
    "\n",
    "print(f\"\\nFinal Training Metrics:\")\n",
    "print(f\"- MSE Loss: {final_train_loss:.4f}\")\n",
    "print(f\"- MAE: {final_train_mae:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal Validation Metrics:\")\n",
    "print(f\"- MSE Loss: {final_val_loss:.4f}\")\n",
    "print(f\"- MAE: {final_val_mae:.4f}\")\n",
    "\n",
    "# Calculate RMSE from MSE\n",
    "final_train_rmse = np.sqrt(final_train_loss)\n",
    "final_val_rmse = np.sqrt(final_val_loss)\n",
    "\n",
    "print(f\"\\nFinal RMSE:\")\n",
    "print(f\"- Training RMSE: {final_train_rmse:.4f}\")\n",
    "print(f\"- Validation RMSE: {final_val_rmse:.4f}\")\n",
    "\n",
    "# Analyze prediction quality\n",
    "print(\"\\nPrediction Quality Analysis:\")\n",
    "val_predictions = dnn_model.predict(X_val)\n",
    "val_errors = val_predictions.flatten() - y_val\n",
    "\n",
    "# Create error histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(val_errors, bins=30, alpha=0.7)\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.title('Validation Prediction Error Distribution')\n",
    "plt.xlabel('Prediction Error (Predicted - Actual)')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(output_path, 'prediction_error_distribution.png'))\n",
    "print(f\"Prediction error distribution saved to {os.path.join(output_path, 'prediction_error_distribution.png')}\")\n",
    "plt.close()\n",
    "\n",
    "# Create scatter plot of predicted vs actual\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_val, val_predictions, alpha=0.3)\n",
    "plt.plot([0.5, 5.0], [0.5, 5.0], 'r--')\n",
    "plt.xlabel('Actual Ratings')\n",
    "plt.ylabel('Predicted Ratings')\n",
    "plt.title('Predicted vs Actual Ratings')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(output_path, 'predicted_vs_actual.png'))\n",
    "print(f\"Predicted vs actual plot saved to {os.path.join(output_path, 'predicted_vs_actual.png')}\")\n",
    "plt.close()\n",
    "\n",
    "# Calculate error statistics by rating level\n",
    "error_by_rating = {}\n",
    "for rating in sorted(np.unique(np.round(y_val * 2) / 2)):  # Round to nearest 0.5\n",
    "    mask = (np.round(y_val * 2) / 2 == rating)\n",
    "    if np.sum(mask) > 0:\n",
    "        rating_errors = val_errors[mask]\n",
    "        error_by_rating[rating] = {\n",
    "            'count': len(rating_errors),\n",
    "            'mean_error': np.mean(rating_errors),\n",
    "            'abs_error': np.mean(np.abs(rating_errors)),\n",
    "            'rmse': np.sqrt(np.mean(rating_errors**2))\n",
    "        }\n",
    "\n",
    "print(\"\\nError by rating level:\")\n",
    "error_df = pd.DataFrame.from_dict(error_by_rating, orient='index')\n",
    "print(error_df)\n",
    "\n",
    "# Plot error by rating level\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(error_df.index, error_df['mean_error'])\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Mean Error by Rating Level')\n",
    "plt.xlabel('Actual Rating')\n",
    "plt.ylabel('Mean Error')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(error_df.index, error_df['rmse'])\n",
    "plt.title('RMSE by Rating Level')\n",
    "plt.xlabel('Actual Rating')\n",
    "plt.ylabel('RMSE')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_path, 'error_by_rating.png'))\n",
    "print(f\"Error by rating plot saved to {os.path.join(output_path, 'error_by_rating.png')}\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: MOVIE RECOMMENDATION GENERATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def generate_user_movie_features(user_id, movie_id, user_genre_preferences, movie_genre_features):\n",
    "    \"\"\"\n",
    "    Generate feature vector for a specific user-movie pair\n",
    "    \n",
    "    Input:\n",
    "      - user_id: User ID\n",
    "      - movie_id: Movie ID\n",
    "      - user_genre_preferences: DataFrame with user genre preferences\n",
    "      - movie_genre_features: DataFrame with movie genre features\n",
    "    \n",
    "    Output:\n",
    "      - feature_vector: Feature vector for the user-movie pair\n",
    "    \"\"\"\n",
    "    # Get genre columns\n",
    "    genre_columns = [col for col in movie_genre_features.columns if col != 'movieId']\n",
    "    \n",
    "    # Skip if user or movie not found\n",
    "    if user_id not in user_genre_preferences['userId'].values or \\\n",
    "       movie_id not in movie_genre_features['movieId'].values:\n",
    "        return None\n",
    "    \n",
    "    # Get user genre preferences\n",
    "    user_prefs = user_genre_preferences[user_genre_preferences['userId'] == user_id].iloc[0]\n",
    "    \n",
    "    # Get movie genres\n",
    "    movie_row = movie_genre_features[movie_genre_features['movieId'] == movie_id]\n",
    "    if movie_row.empty:\n",
    "        return None\n",
    "    movie_genres = movie_row.iloc[0]\n",
    "    \n",
    "    # Create feature vector\n",
    "    feature_vector = []\n",
    "    \n",
    "    for genre in genre_columns:\n",
    "        # Add user preference for this genre\n",
    "        feature_vector.append(user_prefs[genre])\n",
    "        # Add movie genre indicator\n",
    "        feature_vector.append(movie_genres[genre])\n",
    "    \n",
    "    return np.array([feature_vector], dtype=np.float32)\n",
    "\n",
    "def generate_dnn_recommendations(user_id, dnn_model, user_genre_preferences, movie_genre_features, train_ratings, n=10):\n",
    "    \"\"\"Optimized version with batched predictions\"\"\"\n",
    "    print(f\"Generating recommendations for user {user_id}...\")\n",
    "    \n",
    "    # Skip if user not found in genre preferences\n",
    "    if user_id not in user_genre_preferences['userId'].values:\n",
    "        print(f\"User {user_id} not found in genre preferences\")\n",
    "        return []\n",
    "    \n",
    "    # Get genre columns\n",
    "    genre_columns = [col for col in movie_genre_features.columns if col != 'movieId']\n",
    "    \n",
    "    # Get user genre preferences\n",
    "    user_prefs = user_genre_preferences[user_genre_preferences['userId'] == user_id].iloc[0]\n",
    "    \n",
    "    # Get movies already rated by the user\n",
    "    rated_movies = set(train_ratings[train_ratings['userId'] == user_id]['movieId'].values)\n",
    "    \n",
    "    # Get unrated movies\n",
    "    unrated_movies = movie_genre_features[~movie_genre_features['movieId'].isin(rated_movies)]\n",
    "    \n",
    "    # Process in batches to avoid memory issues\n",
    "    batch_size = 1000\n",
    "    all_predictions = []\n",
    "    \n",
    "    for i in range(0, len(unrated_movies), batch_size):\n",
    "        batch = unrated_movies.iloc[i:i+batch_size]\n",
    "        \n",
    "        # Create feature vectors in a vectorized way\n",
    "        feature_vectors = []\n",
    "        movie_ids = []\n",
    "        \n",
    "        for _, movie_row in batch.iterrows():\n",
    "            movie_id = movie_row['movieId']\n",
    "            feature_vector = []\n",
    "            \n",
    "            for genre in genre_columns:\n",
    "                feature_vector.append(user_prefs[genre])\n",
    "                feature_vector.append(movie_row[genre])\n",
    "            \n",
    "            feature_vectors.append(feature_vector)\n",
    "            movie_ids.append(movie_id)\n",
    "        \n",
    "        # Convert to numpy array for batch prediction\n",
    "        feature_array = np.array(feature_vectors)\n",
    "        \n",
    "        # Predict in batch\n",
    "        predictions = dnn_model.predict(feature_array, verbose=0).flatten()\n",
    "        \n",
    "        # Ensure ratings are within bounds\n",
    "        predictions = np.clip(predictions, 0.5, 5.0)\n",
    "        \n",
    "        # Add to results\n",
    "        for movie_id, pred in zip(movie_ids, predictions):\n",
    "            all_predictions.append((movie_id, pred))\n",
    "            \n",
    "    # Sort by predicted rating in descending order\n",
    "    all_predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"Generated {len(all_predictions)} predictions for user {user_id}\")\n",
    "    \n",
    "    # Return top N recommendations\n",
    "    return all_predictions[:n]\n",
    "\n",
    "def generate_recommendations_for_all_users(dnn_model, user_genre_preferences, movie_genre_features, train_ratings, n=10, batch_size=50, max_users=None):\n",
    "    \"\"\"\n",
    "    Generate recommendations for all users using the DNN model with improved batching\n",
    "    \n",
    "    Input:\n",
    "      - dnn_model: Trained DNN model\n",
    "      - user_genre_preferences: DataFrame with user genre preferences\n",
    "      - movie_genre_features: DataFrame with movie genre features\n",
    "      - train_ratings: DataFrame with training ratings\n",
    "      - n: Number of recommendations to generate per user\n",
    "      - batch_size: Number of users to process in each batch\n",
    "      - max_users: Maximum number of users to process (optional)\n",
    "    \n",
    "    Output:\n",
    "      - all_recommendations: Dictionary mapping user IDs to recommendation lists\n",
    "    \"\"\"\n",
    "    print(f\"Generating top-{n} DNN recommendations for all users with optimized batching...\")\n",
    "    \n",
    "    # Get all user IDs\n",
    "    all_user_ids = user_genre_preferences['userId'].unique()\n",
    "    \n",
    "    # Limit to max_users if specified\n",
    "    if max_users and max_users < len(all_user_ids):\n",
    "        user_ids = all_user_ids[:max_users]\n",
    "        print(f\"Limiting to {max_users} users out of {len(all_user_ids)} total users\")\n",
    "    else:\n",
    "        user_ids = all_user_ids\n",
    "    \n",
    "    all_recommendations = {}\n",
    "    total_users = len(user_ids)\n",
    "    \n",
    "    # Create a lookup dictionary for user ratings to avoid repeated filtering\n",
    "    print(\"Creating user rating lookup dictionary...\")\n",
    "    user_rated_movies = {}\n",
    "    for _, row in train_ratings.iterrows():\n",
    "        user_id = row['userId']\n",
    "        movie_id = row['movieId']\n",
    "        if user_id not in user_rated_movies:\n",
    "            user_rated_movies[user_id] = set()\n",
    "        user_rated_movies[user_id].add(movie_id)\n",
    "    \n",
    "    # Get genre columns\n",
    "    genre_columns = [col for col in movie_genre_features.columns if col != 'movieId']\n",
    "    \n",
    "    # Process users in batches\n",
    "    start_time = time.time()\n",
    "    for i in range(0, total_users, batch_size):\n",
    "        batch_end = min(i + batch_size, total_users)\n",
    "        batch_users = user_ids[i:batch_end]\n",
    "        \n",
    "        print(f\"Processing batch of {len(batch_users)} users ({i+1}-{batch_end} of {total_users})\")\n",
    "        batch_start_time = time.time()\n",
    "        \n",
    "        # Process each user in the batch\n",
    "        for user_idx, user_id in enumerate(batch_users):\n",
    "            # Skip if user not found in genre preferences\n",
    "            user_prefs = user_genre_preferences[user_genre_preferences['userId'] == user_id]\n",
    "            if user_prefs.empty:\n",
    "                continue\n",
    "            \n",
    "            # Get movies already rated by the user\n",
    "            rated_movies = user_rated_movies.get(user_id, set())\n",
    "            \n",
    "            # Get candidate movies (not yet rated by the user)\n",
    "            # To improve efficiency, we'll use a modified approach:\n",
    "            # 1. Get all unrated movies\n",
    "            unrated_movie_ids = set(movie_genre_features['movieId']) - rated_movies\n",
    "            \n",
    "            # If too many, limit to a manageable number to improve performance\n",
    "            max_movies_per_batch = 1000\n",
    "            if len(unrated_movie_ids) > max_movies_per_batch:\n",
    "                # Convert to list so we can slice it\n",
    "                unrated_movie_ids = list(unrated_movie_ids)[:max_movies_per_batch]\n",
    "            \n",
    "            # Get movie features for unrated movies\n",
    "            candidate_movies = movie_genre_features[movie_genre_features['movieId'].isin(unrated_movie_ids)]\n",
    "            \n",
    "            # If no candidates, skip this user\n",
    "            if len(candidate_movies) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Process candidates in smaller batches to avoid memory issues\n",
    "            movie_batch_size = 200  # Adjust based on memory constraints\n",
    "            predictions = []\n",
    "            \n",
    "            for j in range(0, len(candidate_movies), movie_batch_size):\n",
    "                movie_batch_end = min(j + movie_batch_size, len(candidate_movies))\n",
    "                movie_batch = candidate_movies.iloc[j:movie_batch_end]\n",
    "                \n",
    "                # Create feature vectors for all movies in this batch\n",
    "                batch_features = []\n",
    "                batch_movie_ids = []\n",
    "                \n",
    "                for _, movie_row in movie_batch.iterrows():\n",
    "                    movie_id = movie_row['movieId']\n",
    "                    feature_vector = []\n",
    "                    \n",
    "                    for genre in genre_columns:\n",
    "                        # User preference for this genre\n",
    "                        feature_vector.append(user_prefs.iloc[0][genre])\n",
    "                        # Movie genre indicator\n",
    "                        feature_vector.append(movie_row[genre])\n",
    "                    \n",
    "                    batch_features.append(feature_vector)\n",
    "                    batch_movie_ids.append(movie_id)\n",
    "                \n",
    "                # Convert to numpy array\n",
    "                batch_features = np.array(batch_features, dtype=np.float32)\n",
    "                \n",
    "                # Skip if empty\n",
    "                if len(batch_features) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Make predictions in batch\n",
    "                try:\n",
    "                    batch_predictions = dnn_model.predict(batch_features, verbose=0).flatten()\n",
    "                    \n",
    "                    # Ensure ratings are within bounds\n",
    "                    batch_predictions = np.clip(batch_predictions, 0.5, 5.0)\n",
    "                    \n",
    "                    # Add to predictions list\n",
    "                    for movie_id, pred in zip(batch_movie_ids, batch_predictions):\n",
    "                        predictions.append((movie_id, float(pred)))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error making predictions for user {user_id}, batch {j}: {e}\")\n",
    "            \n",
    "            # Sort predictions by rating and take top n\n",
    "            predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "            all_recommendations[user_id] = predictions[:n]\n",
    "            \n",
    "            # Log progress for every 10th user or the last one\n",
    "            if (user_idx + 1) % 10 == 0 or user_idx == len(batch_users) - 1:\n",
    "                elapsed_batch = time.time() - batch_start_time\n",
    "                avg_time_per_user = elapsed_batch / (user_idx + 1)\n",
    "                print(f\"  Processed {user_idx + 1}/{len(batch_users)} users in batch, avg time: {avg_time_per_user:.2f}s per user\")\n",
    "        \n",
    "        # Log batch completion\n",
    "        elapsed = time.time() - start_time\n",
    "        avg_time_per_batch = elapsed / ((batch_end - i) / batch_size)\n",
    "        progress = batch_end / total_users * 100\n",
    "        remaining = avg_time_per_batch * ((total_users - batch_end) / batch_size) if batch_end < total_users else 0\n",
    "        \n",
    "        print(f\"Completed batch {i//batch_size + 1}/{(total_users-1)//batch_size + 1}\")\n",
    "        print(f\"Progress: {progress:.1f}% - Elapsed: {elapsed:.2f}s - Est. remaining: {remaining:.2f}s\")\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"Generated recommendations for {len(all_recommendations)} users\")\n",
    "    return all_recommendations\n",
    "\n",
    "# Generate DNN recommendations for all users (limiting to a reasonable number for demonstration)\n",
    "max_users = 200  # Adjust based on your computational resources\n",
    "dnn_recommendations = generate_recommendations_for_all_users(\n",
    "    dnn_model,\n",
    "    user_genre_preferences,\n",
    "    movie_genre_features,\n",
    "    data['train_ratings'],\n",
    "    top_n,\n",
    "    batch_size=50,\n",
    "    max_users=max_users\n",
    ")\n",
    "\n",
    "# Save recommendations\n",
    "with open(os.path.join(output_path, 'dnn_recommendations.pkl'), 'wb') as f:\n",
    "    pickle.dump(dnn_recommendations, f)\n",
    "\n",
    "# Analyze the recommendations\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"RECOMMENDATION ANALYSIS: DNN RECOMMENDATIONS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "if dnn_recommendations:\n",
    "    # Create recommendations dataframe for analysis\n",
    "    rec_list = []\n",
    "    for user_id, recs in dnn_recommendations.items():\n",
    "        for rank, (movie_id, rating) in enumerate(recs, 1):\n",
    "            rec_list.append({\n",
    "                'userId': user_id,\n",
    "                'movieId': movie_id,\n",
    "                'predicted_rating': rating,\n",
    "                'rank': rank\n",
    "            })\n",
    "    \n",
    "    rec_df = pd.DataFrame(rec_list)\n",
    "    \n",
    "    # Save in CSV format\n",
    "    if not rec_df.empty:\n",
    "        # Add movie titles if available\n",
    "        if 'movie_features' in data:\n",
    "            movie_titles = data['movie_features'][['movieId', 'title']]\n",
    "            rec_df = pd.merge(rec_df, movie_titles, on='movieId', how='left')\n",
    "        \n",
    "        rec_df.to_csv(os.path.join(output_path, 'dnn_recommendations.csv'), index=False)\n",
    "        print(f\"Saved recommendations to {os.path.join(output_path, 'dnn_recommendations.csv')}\")\n",
    "    \n",
    "    # Basic recommendation statistics\n",
    "    print(f\"\\nRecommendation Statistics:\")\n",
    "    print(f\"- Users with recommendations: {len(dnn_recommendations)}\")\n",
    "    print(f\"- Total recommendation entries: {len(rec_df)}\")\n",
    "    print(f\"- Average recommendations per user: {len(rec_df)/len(dnn_recommendations):.2f}\")\n",
    "    \n",
    "    # Rating distribution\n",
    "    print(f\"\\nPredicted Rating Distribution:\")\n",
    "    rating_stats = rec_df['predicted_rating'].describe()\n",
    "    print(f\"- Min: {rating_stats['min']:.2f}\")\n",
    "    print(f\"- Max: {rating_stats['max']:.2f}\")\n",
    "    print(f\"- Mean: {rating_stats['mean']:.2f}\")\n",
    "    print(f\"- Median: {rating_stats['50%']:.2f}\")\n",
    "    print(f\"- Std Dev: {rating_stats['std']:.2f}\")\n",
    "    \n",
    "    # Plot recommendation rating distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(rec_df['predicted_rating'], bins=20, kde=True)\n",
    "    plt.title('Distribution of Predicted Ratings in Recommendations')\n",
    "    plt.xlabel('Predicted Rating')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig(os.path.join(output_path, 'recommendation_rating_distribution.png'))\n",
    "    print(f\"Recommendation rating distribution saved to {os.path.join(output_path, 'recommendation_rating_distribution.png')}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Analyze top recommended movies\n",
    "    if 'movie_features' in data:\n",
    "        print(\"\\nTop Recommended Movies:\")\n",
    "        top_movies = rec_df.groupby('movieId').size().reset_index(name='count')\n",
    "        top_movies = pd.merge(top_movies, data['movie_features'][['movieId', 'title']], on='movieId')\n",
    "        top_movies = top_movies.sort_values('count', ascending=False).head(10)\n",
    "        \n",
    "        for i, (_, row) in enumerate(top_movies.iterrows(), 1):\n",
    "            print(f\"{i}. '{row['title']}' - Recommended to {row['count']} users\")\n",
    "        \n",
    "        # Get genre distribution of top recommended movies\n",
    "        top_movie_ids = top_movies['movieId'].values\n",
    "        top_movie_genres = movie_genre_features[movie_genre_features['movieId'].isin(top_movie_ids)]\n",
    "        \n",
    "        genre_columns = [col for col in movie_genre_features.columns if col != 'movieId']\n",
    "        genre_counts = {}\n",
    "        \n",
    "        for genre in genre_columns:\n",
    "            count = top_movie_genres[genre].sum()\n",
    "            genre_counts[genre] = count\n",
    "        \n",
    "        # Plot genre distribution of top recommendations\n",
    "        if genre_counts:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sorted_genres = sorted(genre_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "            genres, counts = zip(*sorted_genres)\n",
    "            plt.bar(genres, counts)\n",
    "            plt.title('Genre Distribution of Top Recommended Movies')\n",
    "            plt.xlabel('Genre')\n",
    "            plt.ylabel('Count')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_path, 'top_recommendations_genre_distribution.png'))\n",
    "            print(f\"Top recommendations genre distribution saved to {os.path.join(output_path, 'top_recommendations_genre_distribution.png')}\")\n",
    "            plt.close()\n",
    "    \n",
    "    # Show sample recommendations for a few users\n",
    "    print(\"\\nSample Recommendations for 3 Users:\")\n",
    "    sample_users = list(dnn_recommendations.keys())[:3]\n",
    "    \n",
    "    for user_id in sample_users:\n",
    "        print(f\"\\nUser {user_id}:\")\n",
    "        user_recs = dnn_recommendations[user_id][:5]  # Show top 5\n",
    "        \n",
    "        for i, (movie_id, rating) in enumerate(user_recs, 1):\n",
    "            movie_info = f\"Movie ID: {movie_id}\"\n",
    "            \n",
    "            # Try to get movie title and genres if available\n",
    "            if 'movie_features' in data:\n",
    "                movie_row = data['movie_features'][data['movie_features']['movieId'] == movie_id]\n",
    "                if not movie_row.empty:\n",
    "                    movie_info = movie_row.iloc[0]['title']\n",
    "            \n",
    "            print(f\"{i}. {movie_info} - Predicted Rating: {rating:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 7: EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def evaluate_recommendations(recommendations, test_ratings, dnn_model, user_genre_preferences, movie_genre_features):\n",
    "    \"\"\"\n",
    "    Evaluate recommendations using RMSE and MAE metrics with expanded predictions\n",
    "    Modified to handle user-based train-test split\n",
    "    \"\"\"\n",
    "    print(\"Evaluating recommendations using RMSE and MAE...\")\n",
    "    \n",
    "    # Check if we have a user-based split by checking user overlap\n",
    "    train_users = set(user_genre_preferences['userId'].unique())\n",
    "    test_users = set(test_ratings['userId'].unique())\n",
    "    common_users = train_users.intersection(test_users)\n",
    "    \n",
    "    print(f\"Train users: {len(train_users)}, Test users: {len(test_users)}, Common users: {len(common_users)}\")\n",
    "    \n",
    "    # If no common users, use baseline evaluation with average rating\n",
    "    if len(common_users) == 0:\n",
    "        print(\"Using user-based split - no common users between train and test.\")\n",
    "        print(\"Evaluating using average rating for all predictions instead.\")\n",
    "        \n",
    "        # Calculate average rating from training data\n",
    "        avg_rating = 3.0  # Fallback value\n",
    "        if 'rating' in test_ratings.columns:\n",
    "            # Use the average rating from test data as baseline\n",
    "            avg_rating = test_ratings['rating'].mean()\n",
    "        \n",
    "        # Get test ratings\n",
    "        total_predictions = len(test_ratings)\n",
    "        \n",
    "        if total_predictions == 0:\n",
    "            print(\"No test ratings available for evaluation\")\n",
    "            return {\n",
    "                'rmse': float('inf'),\n",
    "                'mae': float('inf'),\n",
    "                'num_predictions': 0\n",
    "            }\n",
    "        \n",
    "        # Calculate RMSE and MAE using average rating as prediction\n",
    "        squared_errors_sum = ((test_ratings['rating'] - avg_rating) ** 2).sum()\n",
    "        absolute_errors_sum = (abs(test_ratings['rating'] - avg_rating)).sum()\n",
    "        \n",
    "        rmse = np.sqrt(squared_errors_sum / total_predictions)\n",
    "        mae = absolute_errors_sum / total_predictions\n",
    "        \n",
    "        print(f\"Baseline evaluation results (using avg rating {avg_rating:.2f}):\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"Number of predictions: {total_predictions}\")\n",
    "        \n",
    "        return {\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'num_predictions': total_predictions,\n",
    "            'method': 'baseline_average_rating'\n",
    "        }\n",
    "    \n",
    "    # Standard evaluation (original code) for when there are common users\n",
    "    # Initialize lists for predictions and actual ratings\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    # For each user in the test set\n",
    "    for user_id in test_ratings['userId'].unique():\n",
    "        # Skip users without genre preferences\n",
    "        if user_id not in user_genre_preferences['userId'].values:\n",
    "            continue\n",
    "        \n",
    "        # Get user's test ratings\n",
    "        user_test_ratings = test_ratings[test_ratings['userId'] == user_id]\n",
    "        \n",
    "        # Get user's recommendations (movie_id, predicted_rating) if available\n",
    "        user_recs = {}\n",
    "        if user_id in recommendations:\n",
    "            user_recs = dict(recommendations[user_id])\n",
    "        \n",
    "        # Match test ratings with predictions\n",
    "        for _, row in user_test_ratings.iterrows():\n",
    "            movie_id = row['movieId']\n",
    "            actual_rating = row['rating']\n",
    "            \n",
    "            # If the movie is in recommendations\n",
    "            if movie_id in user_recs:\n",
    "                predictions.append(user_recs[movie_id])\n",
    "                actuals.append(actual_rating)\n",
    "            # Otherwise, make a new prediction for this movie\n",
    "            elif movie_id in movie_genre_features['movieId'].values:\n",
    "                # Generate feature vector for this user-movie pair\n",
    "                feature_vector = generate_user_movie_features(\n",
    "                    user_id, \n",
    "                    movie_id, \n",
    "                    user_genre_preferences, \n",
    "                    movie_genre_features\n",
    "                )\n",
    "                \n",
    "                if feature_vector is not None:\n",
    "                    # Predict rating\n",
    "                    predicted_rating = dnn_model.predict(feature_vector, verbose=0)[0][0]\n",
    "                    # Ensure rating is within bounds\n",
    "                    predicted_rating = max(0.5, min(5.0, predicted_rating))\n",
    "                    \n",
    "                    predictions.append(predicted_rating)\n",
    "                    actuals.append(actual_rating)\n",
    "    \n",
    "    \n",
    "    # Check if we have predictions to evaluate\n",
    "    if not predictions:\n",
    "        print(\"No predictions available for evaluation using standard method\")\n",
    "        # Fall back to baseline if we have test ratings\n",
    "        if len(test_ratings) > 0:\n",
    "            print(\"Falling back to baseline evaluation method\")\n",
    "            avg_rating = test_ratings['rating'].mean() if 'rating' in test_ratings.columns else 3.0\n",
    "            total_predictions = len(test_ratings)\n",
    "            \n",
    "            # Calculate RMSE and MAE using average rating\n",
    "            squared_errors_sum = ((test_ratings['rating'] - avg_rating) ** 2).sum()\n",
    "            absolute_errors_sum = (abs(test_ratings['rating'] - avg_rating)).sum()\n",
    "            \n",
    "            rmse = np.sqrt(squared_errors_sum / total_predictions)\n",
    "            mae = absolute_errors_sum / total_predictions\n",
    "            \n",
    "            print(f\"Baseline evaluation results:\")\n",
    "            print(f\"RMSE: {rmse:.4f}\")\n",
    "            print(f\"MAE: {mae:.4f}\")\n",
    "            print(f\"Number of predictions: {total_predictions}\")\n",
    "            \n",
    "            return {\n",
    "                'rmse': rmse,\n",
    "                'mae': mae,\n",
    "                'num_predictions': total_predictions,\n",
    "                'method': 'baseline_average_rating'\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'rmse': float('inf'),\n",
    "            'mae': float('inf'),\n",
    "            'num_predictions': 0\n",
    "        }\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "    \n",
    "    # Calculate RMSE and MAE\n",
    "    rmse = np.sqrt(np.mean((predictions - actuals) ** 2))\n",
    "    mae = np.mean(np.abs(predictions - actuals))\n",
    "    \n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'num_predictions': len(predictions),\n",
    "        'method': 'standard'\n",
    "    }\n",
    "    \n",
    "    print(f\"Evaluation completed - RMSE: {rmse:.4f}, MAE: {mae:.4f}, Predictions: {len(predictions)}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def recommend_for_user(user_id, recommendations, movie_features=None, n=10):\n",
    "    \"\"\"\n",
    "    Print recommendations for a specific user\n",
    "    \n",
    "    Input:\n",
    "      - user_id: User ID to display recommendations for\n",
    "      - recommendations: Dictionary with recommendation lists\n",
    "      - movie_features: DataFrame with movie features (for titles)\n",
    "      - n: Number of recommendations to display\n",
    "    \n",
    "    Output:\n",
    "      - None (prints recommendations)\n",
    "    \"\"\"\n",
    "    # Check if user has recommendations\n",
    "    if user_id not in recommendations:\n",
    "        print(f\"No recommendations found for user {user_id}\")\n",
    "        return\n",
    "    \n",
    "    # Get recommendations\n",
    "    user_recs = recommendations[user_id][:n]\n",
    "    \n",
    "    if not user_recs:\n",
    "        print(f\"No recommendations found for user {user_id}\")\n",
    "        return\n",
    "    \n",
    "    # Print recommendations\n",
    "    print(f\"\\nTop {len(user_recs)} recommendations for user {user_id}:\")\n",
    "    \n",
    "    for i, (movie_id, predicted_rating) in enumerate(user_recs, 1):\n",
    "        movie_info = f\"Movie ID: {movie_id}\"\n",
    "        \n",
    "        # Try to get movie title if available\n",
    "        if movie_features is not None:\n",
    "            movie_row = movie_features[movie_features['movieId'] == movie_id]\n",
    "            if not movie_row.empty and 'title' in movie_row.columns:\n",
    "                movie_info = movie_row.iloc[0]['title']\n",
    "        \n",
    "        print(f\"{i}. {movie_info} - Predicted Rating: {predicted_rating:.2f}\")\n",
    "\n",
    "# Evaluate recommendations\n",
    "print(\"\\nEvaluating DNN recommendations...\")\n",
    "evaluation_metrics = evaluate_recommendations(\n",
    "    dnn_recommendations,\n",
    "    data['test_ratings'],\n",
    "    dnn_model,\n",
    "    user_genre_preferences,\n",
    "    movie_genre_features\n",
    ")\n",
    "\n",
    "# Save evaluation metrics\n",
    "if evaluation_metrics:\n",
    "    evaluation_results = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_results.to_csv(os.path.join(output_path, 'dnn_evaluation.csv'), index=False)\n",
    "    print(f\"Saved evaluation metrics to {os.path.join(output_path, 'dnn_evaluation.csv')}\")\n",
    "\n",
    "# Analyze the evaluation results\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"EVALUATION ANALYSIS: MODEL PERFORMANCE\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "if evaluation_metrics:\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    for key, value in evaluation_metrics.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"- {key}: {value:.4f}\" if isinstance(value, float) else f\"- {key}: {value}\")\n",
    "        else:\n",
    "            print(f\"- {key}: {value}\")\n",
    "    \n",
    "    # Compare with baseline if possible\n",
    "    if 'method' in evaluation_metrics and evaluation_metrics['method'] == 'standard':\n",
    "        # Calculate baseline (using average rating)\n",
    "        if 'train_ratings' in data:\n",
    "            avg_rating = data['train_ratings']['rating'].mean()\n",
    "            user_avg_ratings = data['train_ratings'].groupby('userId')['rating'].mean()\n",
    "            \n",
    "            # Get actual ratings used in evaluation\n",
    "            baseline_errors = []\n",
    "            personalized_baseline_errors = []\n",
    "            \n",
    "            for user_id in test_ratings['userId'].unique():\n",
    "                if user_id not in user_genre_preferences['userId'].values:\n",
    "                    continue\n",
    "                \n",
    "                user_test_ratings = test_ratings[test_ratings['userId'] == user_id]\n",
    "                \n",
    "                for _, row in user_test_ratings.iterrows():\n",
    "                    actual_rating = row['rating']\n",
    "                    \n",
    "                    # Global average baseline\n",
    "                    baseline_errors.append((avg_rating - actual_rating)**2)\n",
    "                    \n",
    "                    # Per-user average baseline\n",
    "                    if user_id in user_avg_ratings.index:\n",
    "                        user_avg = user_avg_ratings[user_id]\n",
    "                        personalized_baseline_errors.append((user_avg - actual_rating)**2)\n",
    "            \n",
    "            if baseline_errors:\n",
    "                baseline_rmse = np.sqrt(np.mean(baseline_errors))\n",
    "                print(f\"\\nBaseline RMSE (global average): {baseline_rmse:.4f}\")\n",
    "                print(f\"Improvement over baseline: {(baseline_rmse - evaluation_metrics['rmse'])/baseline_rmse*100:.2f}%\")\n",
    "            \n",
    "            if personalized_baseline_errors:\n",
    "                personalized_baseline_rmse = np.sqrt(np.mean(personalized_baseline_errors))\n",
    "                print(f\"\\nBaseline RMSE (user average): {personalized_baseline_rmse:.4f}\")\n",
    "                print(f\"Improvement over personalized baseline: {(personalized_baseline_rmse - evaluation_metrics['rmse'])/personalized_baseline_rmse*100:.2f}%\")\n",
    "\n",
    "# Display a sample recommendation for user exploration\n",
    "print(\"\\nSample recommendation for exploration:\")\n",
    "if dnn_recommendations:\n",
    "    # Pick a random user\n",
    "    sample_user_id = np.random.choice(list(dnn_recommendations.keys()))\n",
    "    \n",
    "    # Get user's genre preferences\n",
    "    if sample_user_id in user_genre_preferences['userId'].values:\n",
    "        user_prefs = user_genre_preferences[user_genre_preferences['userId'] == sample_user_id].iloc[0]\n",
    "        genre_columns = [col for col in user_genre_preferences.columns if col != 'userId']\n",
    "        \n",
    "        print(f\"\\nUser {sample_user_id} Genre Preferences:\")\n",
    "        # Show top 3 liked and disliked genres\n",
    "        user_prefs_list = [(genre, user_prefs[genre]) for genre in genre_columns]\n",
    "        liked_genres = sorted(user_prefs_list, key=lambda x: x[1], reverse=True)[:3]\n",
    "        disliked_genres = sorted(user_prefs_list, key=lambda x: x[1])[:3]\n",
    "        \n",
    "        print(f\"- Most liked genres: {', '.join([f'{g} ({v:.2f})' for g, v in liked_genres])}\")\n",
    "        print(f\"- Most disliked genres: {', '.join([f'{g} ({v:.2f})' for g, v in disliked_genres])}\")\n",
    "    \n",
    "    # Show recommendations\n",
    "    recommend_for_user(sample_user_id, dnn_recommendations, data['movie_features'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: COLLABORATIVE FILTERING WITH DNN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Final summary of model performance and characteristics\n",
    "print(\"\\nModel Characteristics:\")\n",
    "print(f\"- Hidden layer sizes: {dnn_hidden_layers}\")\n",
    "print(f\"- Dropout rate: {dnn_dropout_rate}\")\n",
    "print(f\"- Learning rate: {dnn_learning_rate}\")\n",
    "print(f\"- Batch size: {dnn_batch_size}\")\n",
    "\n",
    "# Display dataset statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"- Training samples: {len(X_train)}\")\n",
    "print(f\"- Validation samples: {len(X_val)}\")\n",
    "print(f\"- Feature dimensions: {X_train.shape[1]}\")\n",
    "print(f\"- Number of users with genre preferences: {len(user_genre_preferences)}\")\n",
    "print(f\"- Number of movies with genre features: {len(movie_genre_features)}\")\n",
    "\n",
    "# Show performance metrics\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "if evaluation_metrics:\n",
    "    print(f\"- RMSE: {evaluation_metrics['rmse']:.4f}\")\n",
    "    print(f\"- MAE: {evaluation_metrics['mae']:.4f}\")\n",
    "    print(f\"- Predictions evaluated: {evaluation_metrics['num_predictions']}\")\n",
    "\n",
    "# Display comparison table with other methods\n",
    "print(\"\\nComparison with Other Methods:\")\n",
    "headers = [\"Model\", \"RMSE\", \"MAE\", \"Predictions\"]\n",
    "rows = [\n",
    "    [\n",
    "        \"Collaborative Filtering (DNN)\",\n",
    "        f\"{evaluation_metrics['rmse']:.4f}\" if evaluation_metrics else \"N/A\",\n",
    "        f\"{evaluation_metrics['mae']:.4f}\" if evaluation_metrics else \"N/A\",\n",
    "        f\"{evaluation_metrics['num_predictions']}\" if evaluation_metrics else \"N/A\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Print table\n",
    "col_widths = [max(len(row[i]) for row in [headers] + rows) for i in range(len(headers))]\n",
    "print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")\n",
    "print(\"| \" + \" | \".join(headers[i].ljust(col_widths[i]) for i in range(len(headers))) + \" |\")\n",
    "print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")\n",
    "for row in rows:\n",
    "    print(\"| \" + \" | \".join(row[i].ljust(col_widths[i]) for i in range(len(row))) + \" |\")\n",
    "print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
