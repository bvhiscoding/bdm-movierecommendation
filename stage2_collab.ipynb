{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 10:35:13,575 : INFO : Loading processed data from stage1.py...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COLLABORATIVE FILTERING WITH DEEP NEURAL NETWORK\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 10:35:13,829 : INFO : Loaded features for 27278 movies\n",
      "2025-04-16 10:35:18,317 : INFO : Loaded 20000263 normalized ratings\n",
      "2025-04-16 10:35:39,101 : INFO : Split ratings into 15945812 training and 4054451 testing samples\n",
      "2025-04-16 10:35:39,670 : INFO : Extracting genre features for movies...\n",
      "2025-04-16 10:35:39,670 : INFO : Extracted 20 genre features for 27278 movies\n",
      "2025-04-16 10:35:39,670 : INFO : Calculating user preferences for movie genres...\n",
      "2025-04-16 12:01:50,337 : INFO : Calculated genre preferences for 138493 users\n",
      "2025-04-16 12:01:50,409 : INFO : Preparing training data for DNN model...\n",
      "2025-04-16 12:10:20,917 : INFO : Created feature matrix with shape (1000000, 40) and labels with shape (1000000,)\n",
      "2025-04-16 12:10:21,047 : INFO : Prepared training data with 800000 samples, validation data with 200000 samples\n",
      "2025-04-16 12:10:21,524 : INFO : Building and training DNN model...\n",
      "c:\\Users\\NCPC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1ms/step - loss: 2.1955 - mae: 1.0863 - val_loss: 0.8724 - val_mae: 0.7166\n",
      "Epoch 2/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 987us/step - loss: 0.9147 - mae: 0.7408 - val_loss: 0.8573 - val_mae: 0.7127\n",
      "Epoch 3/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8890 - mae: 0.7274 - val_loss: 0.8568 - val_mae: 0.7119\n",
      "Epoch 4/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8771 - mae: 0.7220 - val_loss: 0.8486 - val_mae: 0.7074\n",
      "Epoch 5/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8775 - mae: 0.7217 - val_loss: 0.8467 - val_mae: 0.7052\n",
      "Epoch 6/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 996us/step - loss: 0.8740 - mae: 0.7203 - val_loss: 0.8440 - val_mae: 0.7051\n",
      "Epoch 7/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8721 - mae: 0.7198 - val_loss: 0.8441 - val_mae: 0.7038\n",
      "Epoch 8/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8749 - mae: 0.7203 - val_loss: 0.8457 - val_mae: 0.7097\n",
      "Epoch 9/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8734 - mae: 0.7197 - val_loss: 0.8415 - val_mae: 0.7035\n",
      "Epoch 10/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8723 - mae: 0.7193 - val_loss: 0.8430 - val_mae: 0.7042\n",
      "Epoch 11/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8717 - mae: 0.7188 - val_loss: 0.8410 - val_mae: 0.7015\n",
      "Epoch 12/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8700 - mae: 0.7184 - val_loss: 0.8416 - val_mae: 0.7000\n",
      "Epoch 13/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8683 - mae: 0.7175 - val_loss: 0.8385 - val_mae: 0.7021\n",
      "Epoch 14/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8684 - mae: 0.7180 - val_loss: 0.8462 - val_mae: 0.7064\n",
      "Epoch 15/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8669 - mae: 0.7177 - val_loss: 0.8411 - val_mae: 0.7017\n",
      "Epoch 16/20\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8665 - mae: 0.7168 - val_loss: 0.8414 - val_mae: 0.7049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 12:13:48,601 : INFO : Model training completed. Validation MSE: 0.8385, validation MAE: 0.7021\n",
      "2025-04-16 12:13:48,602 : WARNING : You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "2025-04-16 12:13:48,636 : INFO : Saved DNN model to ./rec/collaborative-recommendations\\dnn_model.h5\n",
      "2025-04-16 12:13:48,911 : INFO : Saved training history plot to ./rec/collaborative-recommendations\\dnn_training_history.png\n",
      "2025-04-16 12:13:48,915 : INFO : Generating top-50 DNN recommendations for all users...\n",
      "2025-04-16 12:13:48,919 : INFO : Processing batch of 138493 users (1-138493 of 138493)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set paths\n",
    "input_path = \"./processed/\"  # Current directory where stage1.py saved the files\n",
    "output_path = \"./rec/collaborative-recommendations\"\n",
    "top_n = 50\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Model parameters\n",
    "dnn_hidden_layers = [64, 32, 16]  # Optimized architecture \n",
    "dnn_dropout_rate = 0.2\n",
    "dnn_learning_rate = 0.001\n",
    "dnn_batch_size = 64   # Increased batch size for faster training\n",
    "dnn_epochs = 20       # Reduced epochs with early stopping\n",
    "threshold_rating = 3.0  # Rating threshold to classify as \"like\"\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load processed data from stage1.py\n",
    "    \n",
    "    Input: None (reads from files)\n",
    "    Output: Dictionary containing DataFrames for movie features and ratings\n",
    "    \"\"\"\n",
    "    logger.info(\"Loading processed data from stage1.py...\")\n",
    "    \n",
    "    # Data containers\n",
    "    data = {}\n",
    "    \n",
    "    # Load movie features\n",
    "    movie_features_path = os.path.join(input_path, 'processed_movie_features.csv')\n",
    "    if os.path.exists(movie_features_path):\n",
    "        data['movie_features'] = pd.read_csv(movie_features_path)\n",
    "        logger.info(f\"Loaded features for {len(data['movie_features'])} movies\")\n",
    "    else:\n",
    "        logger.error(f\"Movie features not found at {movie_features_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Load normalized ratings\n",
    "    ratings_path = os.path.join(input_path, 'normalized_ratings.csv')\n",
    "    if os.path.exists(ratings_path):\n",
    "        data['ratings'] = pd.read_csv(ratings_path)\n",
    "        logger.info(f\"Loaded {len(data['ratings'])} normalized ratings\")\n",
    "    else:\n",
    "        logger.error(f\"Normalized ratings not found at {ratings_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Create training and testing sets with 80-20 split\n",
    "    if 'ratings' in data:\n",
    "        # Sort by timestamp if available to ensure reproducibility\n",
    "        if 'timestamp' in data['ratings'].columns:\n",
    "            data['ratings'] = data['ratings'].sort_values('timestamp')\n",
    "        \n",
    "        # Group by user to ensure each user has both training and testing data\n",
    "        user_groups = data['ratings'].groupby('userId')\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        \n",
    "        for _, group in user_groups:\n",
    "            n = len(group)\n",
    "            split_idx = int(n * 0.8)\n",
    "            train_data.append(group.iloc[:split_idx])\n",
    "            test_data.append(group.iloc[split_idx:])\n",
    "        \n",
    "        data['train_ratings'] = pd.concat(train_data).reset_index(drop=True)\n",
    "        data['test_ratings'] = pd.concat(test_data).reset_index(drop=True)\n",
    "        \n",
    "        logger.info(f\"Split ratings into {len(data['train_ratings'])} training and {len(data['test_ratings'])} testing samples\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def extract_genre_features(movie_features):\n",
    "    \"\"\"\n",
    "    Extract genre features for each movie\n",
    "    \n",
    "    Input: \n",
    "      - movie_features: DataFrame with movie features including genre columns\n",
    "    \n",
    "    Output:\n",
    "      - movie_genre_features: DataFrame with movieId and genre columns only\n",
    "    \"\"\"\n",
    "    logger.info(\"Extracting genre features for movies...\")\n",
    "    \n",
    "    # Get all genre columns (assuming they're already one-hot encoded)\n",
    "    genre_columns = [col for col in movie_features.columns if col not in \n",
    "                     ['movieId', 'title', 'tokens', 'token_count', 'top_keywords']]\n",
    "    \n",
    "    if not genre_columns:\n",
    "        logger.error(\"No genre columns found in movie features\")\n",
    "        return None\n",
    "    \n",
    "    # Create genre feature matrix\n",
    "    movie_genre_features = movie_features[['movieId'] + genre_columns].copy()\n",
    "    \n",
    "    logger.info(f\"Extracted {len(genre_columns)} genre features for {len(movie_features)} movies\")\n",
    "    \n",
    "    return movie_genre_features\n",
    "\n",
    "def calculate_user_genre_preferences(train_ratings, movie_genre_features):\n",
    "    \"\"\"\n",
    "    Calculate user preferences for movie genres based on ratings\n",
    "    \n",
    "    Input:\n",
    "      - train_ratings: DataFrame with user-movie ratings\n",
    "      - movie_genre_features: DataFrame with movie genre features\n",
    "    \n",
    "    Output:\n",
    "      - user_genre_preferences_df: DataFrame with userId and genre preference scores\n",
    "    \"\"\"\n",
    "    logger.info(\"Calculating user preferences for movie genres...\")\n",
    "    \n",
    "    # Get genre columns\n",
    "    genre_columns = [col for col in movie_genre_features.columns if col != 'movieId']\n",
    "    \n",
    "    # Initialize user genre preferences dataframe\n",
    "    user_genre_preferences = []\n",
    "    \n",
    "    # Process each user\n",
    "    for user_id in train_ratings['userId'].unique():\n",
    "        # Get user ratings\n",
    "        user_ratings = train_ratings[train_ratings['userId'] == user_id]\n",
    "        \n",
    "        if len(user_ratings) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Separate liked and disliked movies\n",
    "        liked_movies = user_ratings[user_ratings['rating'] > threshold_rating]['movieId'].values\n",
    "        disliked_movies = user_ratings[user_ratings['rating'] <= threshold_rating]['movieId'].values\n",
    "        \n",
    "        # Calculate genre preferences using equation (7) from the paper:\n",
    "        # R̂g = (Nlikes - Ndislikes) / Max(Nlikes - Ndislikes)\n",
    "        genre_preferences = {}\n",
    "        \n",
    "        for genre in genre_columns:\n",
    "            # Get genre values for liked movies\n",
    "            genre_liked = movie_genre_features[movie_genre_features['movieId'].isin(liked_movies)][genre].sum()\n",
    "            \n",
    "            # Get genre values for disliked movies\n",
    "            genre_disliked = movie_genre_features[movie_genre_features['movieId'].isin(disliked_movies)][genre].sum()\n",
    "            \n",
    "            # Calculate preference\n",
    "            genre_preferences[genre] = genre_liked - genre_disliked\n",
    "        \n",
    "        # Calculate maximum absolute genre preference\n",
    "        max_abs_preference = max(abs(val) for val in genre_preferences.values()) if genre_preferences else 1\n",
    "        \n",
    "        # Normalize preferences to [-1, 1]\n",
    "        for genre in genre_preferences:\n",
    "            genre_preferences[genre] = genre_preferences[genre] / max_abs_preference if max_abs_preference > 0 else 0\n",
    "        \n",
    "        # Add user ID\n",
    "        genre_preferences['userId'] = user_id\n",
    "        \n",
    "        user_genre_preferences.append(genre_preferences)\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    user_genre_preferences_df = pd.DataFrame(user_genre_preferences)\n",
    "    \n",
    "    logger.info(f\"Calculated genre preferences for {len(user_genre_preferences_df)} users\")\n",
    "    \n",
    "    return user_genre_preferences_df\n",
    "\n",
    "def prepare_dnn_training_data(train_ratings, user_genre_preferences, movie_genre_features):\n",
    "    \"\"\"\n",
    "    Prepare training data for the DNN model\n",
    "    \n",
    "    Input:\n",
    "      - train_ratings: DataFrame with user-movie ratings\n",
    "      - user_genre_preferences: DataFrame with user genre preferences\n",
    "      - movie_genre_features: DataFrame with movie genre features\n",
    "    \n",
    "    Output:\n",
    "      - X_train, X_val: Feature matrices for training and validation\n",
    "      - y_train, y_val: Target values for training and validation\n",
    "      - genre_columns: List of genre column names\n",
    "    \"\"\"\n",
    "    logger.info(\"Preparing training data for DNN model...\")\n",
    "    \n",
    "    # Get genre columns\n",
    "    genre_columns = [col for col in movie_genre_features.columns if col != 'movieId']\n",
    "    \n",
    "    # Initialize lists for features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # Process only a sample of ratings for efficiency\n",
    "    sample_size = min(1000000, len(train_ratings))  # Cap at 1M ratings\n",
    "    sampled_ratings = train_ratings.sample(sample_size, random_state=42) if len(train_ratings) > sample_size else train_ratings\n",
    "    \n",
    "    # Process each rating\n",
    "    for _, row in sampled_ratings.iterrows():\n",
    "        user_id = row['userId']\n",
    "        movie_id = row['movieId']\n",
    "        rating = row['rating']\n",
    "        \n",
    "        # Skip if user or movie not found\n",
    "        if user_id not in user_genre_preferences['userId'].values or \\\n",
    "           movie_id not in movie_genre_features['movieId'].values:\n",
    "            continue\n",
    "        \n",
    "        # Get user genre preferences\n",
    "        user_prefs = user_genre_preferences[user_genre_preferences['userId'] == user_id].iloc[0]\n",
    "        \n",
    "        # Get movie genres\n",
    "        movie_genres = movie_genre_features[movie_genre_features['movieId'] == movie_id].iloc[0]\n",
    "        \n",
    "        # Create feature vector by combining user preferences and movie genres\n",
    "        feature_vector = []\n",
    "        \n",
    "        for genre in genre_columns:\n",
    "            # Add user preference for this genre\n",
    "            feature_vector.append(user_prefs[genre])\n",
    "            # Add movie genre indicator\n",
    "            feature_vector.append(movie_genres[genre])\n",
    "        \n",
    "        # Use the actual rating as the target\n",
    "        features.append(feature_vector)\n",
    "        labels.append(rating)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(features, dtype=np.float32)\n",
    "    y = np.array(labels, dtype=np.float32)\n",
    "    \n",
    "    logger.info(f\"Created feature matrix with shape {X.shape} and labels with shape {y.shape}\")\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    logger.info(f\"Prepared training data with {len(X_train)} samples, validation data with {len(X_val)} samples\")\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val, genre_columns\n",
    "\n",
    "def build_and_train_dnn_model(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Build and train the DNN model for collaborative filtering\n",
    "    \n",
    "    Input:\n",
    "      - X_train, X_val: Feature matrices for training and validation\n",
    "      - y_train, y_val: Target values for training and validation\n",
    "    \n",
    "    Output:\n",
    "      - model: Trained DNN model\n",
    "      - history: Training history\n",
    "    \"\"\"\n",
    "    logger.info(\"Building and training DNN model...\")\n",
    "    \n",
    "    # Set memory limit to avoid OOM errors\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logger.info(f\"Found {len(gpus)} GPU(s), enabled memory growth\")\n",
    "        except RuntimeError as e:\n",
    "            logger.warning(f\"Error setting GPU memory growth: {e}\")\n",
    "    \n",
    "    # Define input dimension\n",
    "    input_dim = X_train.shape[1]\n",
    "    \n",
    "    # Build model based on optimized architecture\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(Dense(dnn_hidden_layers[0], input_dim=input_dim, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dnn_dropout_rate))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for units in dnn_hidden_layers[1:]:\n",
    "        model.add(Dense(units, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dnn_dropout_rate))\n",
    "    \n",
    "    # Output layer - single value for rating prediction\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compile model with Adam optimizer and Mean Squared Error loss for regression\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=dnn_learning_rate),\n",
    "        loss='mse',  # Use MSE for regression\n",
    "        metrics=['mae']  # Track mean absolute error during training\n",
    "    )\n",
    "    \n",
    "    # Define early stopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=dnn_epochs,\n",
    "        batch_size=dnn_batch_size,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
    "    \n",
    "    logger.info(f\"Model training completed. Validation MSE: {val_loss:.4f}, validation MAE: {val_mae:.4f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def generate_user_movie_features(user_id, movie_id, user_genre_preferences, movie_genre_features):\n",
    "    \"\"\"\n",
    "    Generate feature vector for a specific user-movie pair\n",
    "    \n",
    "    Input:\n",
    "      - user_id: User ID\n",
    "      - movie_id: Movie ID\n",
    "      - user_genre_preferences: DataFrame with user genre preferences\n",
    "      - movie_genre_features: DataFrame with movie genre features\n",
    "    \n",
    "    Output:\n",
    "      - feature_vector: Feature vector for the user-movie pair\n",
    "    \"\"\"\n",
    "    # Get genre columns\n",
    "    genre_columns = [col for col in movie_genre_features.columns if col != 'movieId']\n",
    "    \n",
    "    # Skip if user or movie not found\n",
    "    if user_id not in user_genre_preferences['userId'].values or \\\n",
    "       movie_id not in movie_genre_features['movieId'].values:\n",
    "        return None\n",
    "    \n",
    "    # Get user genre preferences\n",
    "    user_prefs = user_genre_preferences[user_genre_preferences['userId'] == user_id].iloc[0]\n",
    "    \n",
    "    # Get movie genres\n",
    "    movie_row = movie_genre_features[movie_genre_features['movieId'] == movie_id]\n",
    "    if movie_row.empty:\n",
    "        return None\n",
    "    movie_genres = movie_row.iloc[0]\n",
    "    \n",
    "    # Create feature vector\n",
    "    feature_vector = []\n",
    "    \n",
    "    for genre in genre_columns:\n",
    "        # Add user preference for this genre\n",
    "        feature_vector.append(user_prefs[genre])\n",
    "        # Add movie genre indicator\n",
    "        feature_vector.append(movie_genres[genre])\n",
    "    \n",
    "    return np.array([feature_vector], dtype=np.float32)\n",
    "\n",
    "def generate_dnn_recommendations(user_id, dnn_model, user_genre_preferences, movie_genre_features, train_ratings, n=10):\n",
    "    \"\"\"Optimized version with batched predictions\"\"\"\n",
    "    # Skip if user not found in genre preferences\n",
    "    if user_id not in user_genre_preferences['userId'].values:\n",
    "        logger.warning(f\"User {user_id} not found in genre preferences\")\n",
    "        return []\n",
    "    \n",
    "    # Get genre columns\n",
    "    genre_columns = [col for col in movie_genre_features.columns if col != 'movieId']\n",
    "    \n",
    "    # Get user genre preferences\n",
    "    user_prefs = user_genre_preferences[user_genre_preferences['userId'] == user_id].iloc[0]\n",
    "    \n",
    "    # Get movies already rated by the user\n",
    "    rated_movies = set(train_ratings[train_ratings['userId'] == user_id]['movieId'].values)\n",
    "    \n",
    "    # Get unrated movies\n",
    "    unrated_movies = movie_genre_features[~movie_genre_features['movieId'].isin(rated_movies)]\n",
    "    \n",
    "    # Process in batches to avoid memory issues\n",
    "    batch_size = 1000\n",
    "    all_predictions = []\n",
    "    \n",
    "    for i in range(0, len(unrated_movies), batch_size):\n",
    "        batch = unrated_movies.iloc[i:i+batch_size]\n",
    "        \n",
    "        # Create feature vectors in a vectorized way\n",
    "        feature_vectors = []\n",
    "        movie_ids = []\n",
    "        \n",
    "        for _, movie_row in batch.iterrows():\n",
    "            movie_id = movie_row['movieId']\n",
    "            feature_vector = []\n",
    "            \n",
    "            for genre in genre_columns:\n",
    "                feature_vector.append(user_prefs[genre])\n",
    "                feature_vector.append(movie_row[genre])\n",
    "            \n",
    "            feature_vectors.append(feature_vector)\n",
    "            movie_ids.append(movie_id)\n",
    "        \n",
    "        # Convert to numpy array for batch prediction\n",
    "        feature_array = np.array(feature_vectors)\n",
    "        \n",
    "        # Predict in batch\n",
    "        predictions = dnn_model.predict(feature_array, verbose=0).flatten()\n",
    "        \n",
    "        # Ensure ratings are within bounds\n",
    "        predictions = np.clip(predictions, 0.5, 5.0)\n",
    "        \n",
    "        # Add to results\n",
    "        for movie_id, pred in zip(movie_ids, predictions):\n",
    "            all_predictions.append((movie_id, pred))\n",
    "            \n",
    "    # Sort by predicted rating in descending order\n",
    "    all_predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top N recommendations\n",
    "    return all_predictions[:n]\n",
    "\n",
    "def generate_recommendations_for_all_users(dnn_model, user_genre_preferences, movie_genre_features, train_ratings, n=10, batch_size=50):\n",
    "    \"\"\"\n",
    "    Generate recommendations for all users using the DNN model with improved batching\n",
    "    \n",
    "    Input:\n",
    "      - dnn_model: Trained DNN model\n",
    "      - user_genre_preferences: DataFrame with user genre preferences\n",
    "      - movie_genre_features: DataFrame with movie genre features\n",
    "      - train_ratings: DataFrame with training ratings\n",
    "      - n: Number of recommendations to generate per user\n",
    "      - batch_size: Number of users to process in each batch\n",
    "    \n",
    "    Output:\n",
    "      - all_recommendations: Dictionary mapping user IDs to recommendation lists\n",
    "    \"\"\"\n",
    "    logger.info(f\"Generating top-{n} DNN recommendations for all users...\")\n",
    "    \n",
    "    # Get all user IDs\n",
    "    all_user_ids = user_genre_preferences['userId'].unique()\n",
    "    \n",
    "    # Limit to max_users if specified\n",
    "    if max_users and max_users < len(all_user_ids):\n",
    "        user_ids = all_user_ids[:max_users]\n",
    "    else:\n",
    "        user_ids = all_user_ids\n",
    "    \n",
    "    all_recommendations = {}\n",
    "    total_users = len(user_ids)\n",
    "    \n",
    "    # Process users in batches\n",
    "    for i in range(0, total_users, batch_size):\n",
    "        batch_end = min(i + batch_size, total_users)\n",
    "        batch_users = user_ids[i:batch_end]\n",
    "        \n",
    "        logger.info(f\"Processing batch of {len(batch_users)} users ({i+1}-{batch_end} of {total_users})\")\n",
    "        \n",
    "        for user_id in batch_users:\n",
    "            try:\n",
    "                # Set a timeout for each user's recommendation generation (optional)\n",
    "                recommendations = generate_dnn_recommendations(\n",
    "                    user_id, \n",
    "                    dnn_model, \n",
    "                    user_genre_preferences, \n",
    "                    movie_genre_features, \n",
    "                    train_ratings, \n",
    "                    n\n",
    "                )\n",
    "                \n",
    "                if recommendations:\n",
    "                    all_recommendations[user_id] = recommendations\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating recommendations for user {user_id}: {str(e)}\")\n",
    "        \n",
    "        # Log progress at each batch\n",
    "        logger.info(f\"Generated recommendations for {batch_end}/{total_users} users ({batch_end/total_users*100:.1f}%)\")\n",
    "    \n",
    "    return all_recommendations\n",
    "\n",
    "def evaluate_recommendations(recommendations, test_ratings, dnn_model, user_genre_preferences, movie_genre_features):\n",
    "    \"\"\"\n",
    "    Evaluate recommendations using RMSE and MAE metrics with expanded predictions\n",
    "    \"\"\"\n",
    "    logger.info(\"Evaluating recommendations using RMSE and MAE...\")\n",
    "    \n",
    "    # Initialize lists for predictions and actual ratings\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    # For each user in the test set\n",
    "    for user_id in test_ratings['userId'].unique():\n",
    "        # Skip users without genre preferences\n",
    "        if user_id not in user_genre_preferences['userId'].values:\n",
    "            continue\n",
    "        \n",
    "        # Get user's test ratings\n",
    "        user_test_ratings = test_ratings[test_ratings['userId'] == user_id]\n",
    "        \n",
    "        # Get user's recommendations (movie_id, predicted_rating) if available\n",
    "        user_recs = {}\n",
    "        if user_id in recommendations:\n",
    "            user_recs = dict(recommendations[user_id])\n",
    "        \n",
    "        # Match test ratings with predictions\n",
    "        for _, row in user_test_ratings.iterrows():\n",
    "            movie_id = row['movieId']\n",
    "            actual_rating = row['rating']\n",
    "            \n",
    "            # If the movie is in recommendations\n",
    "            if movie_id in user_recs:\n",
    "                predictions.append(user_recs[movie_id])\n",
    "                actuals.append(actual_rating)\n",
    "            # Otherwise, make a new prediction for this movie\n",
    "            elif movie_id in movie_genre_features['movieId'].values:\n",
    "                # Generate feature vector for this user-movie pair\n",
    "                feature_vector = generate_user_movie_features(\n",
    "                    user_id, \n",
    "                    movie_id, \n",
    "                    user_genre_preferences, \n",
    "                    movie_genre_features\n",
    "                )\n",
    "                \n",
    "                if feature_vector is not None:\n",
    "                    # Predict rating\n",
    "                    predicted_rating = dnn_model.predict(feature_vector, verbose=0)[0][0]\n",
    "                    # Ensure rating is within bounds\n",
    "                    predicted_rating = max(0.5, min(5.0, predicted_rating))\n",
    "                    \n",
    "                    predictions.append(predicted_rating)\n",
    "                    actuals.append(actual_rating)\n",
    "    \n",
    "    \n",
    "    # Check if we have predictions to evaluate\n",
    "    if not predictions:\n",
    "        logger.warning(\"No predictions to evaluate\")\n",
    "        return {\n",
    "            'rmse': float('inf'),\n",
    "            'mae': float('inf'),\n",
    "            'num_predictions': 0\n",
    "        }\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "    \n",
    "    # Calculate RMSE and MAE\n",
    "    rmse = np.sqrt(np.mean((predictions - actuals) ** 2))\n",
    "    mae = np.mean(np.abs(predictions - actuals))\n",
    "    \n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'num_predictions': len(predictions)\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Evaluation completed - RMSE: {rmse:.4f}, MAE: {mae:.4f}, Predictions: {len(predictions)}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def recommend_for_user(user_id, recommendations, movie_features=None, n=10):\n",
    "    \"\"\"\n",
    "    Print recommendations for a specific user\n",
    "    \n",
    "    Input:\n",
    "      - user_id: User ID to display recommendations for\n",
    "      - recommendations: Dictionary with recommendation lists\n",
    "      - movie_features: DataFrame with movie features (for titles)\n",
    "      - n: Number of recommendations to display\n",
    "    \n",
    "    Output:\n",
    "      - None (prints recommendations)\n",
    "    \"\"\"\n",
    "    # Check if user has recommendations\n",
    "    if user_id not in recommendations:\n",
    "        print(f\"No recommendations found for user {user_id}\")\n",
    "        return\n",
    "    \n",
    "    # Get recommendations\n",
    "    user_recs = recommendations[user_id][:n]\n",
    "    \n",
    "    if not user_recs:\n",
    "        print(f\"No recommendations found for user {user_id}\")\n",
    "        return\n",
    "    \n",
    "    # Print recommendations\n",
    "    print(f\"\\nTop {len(user_recs)} recommendations for user {user_id}:\")\n",
    "    \n",
    "    for i, (movie_id, predicted_rating) in enumerate(user_recs, 1):\n",
    "        movie_info = f\"Movie ID: {movie_id}\"\n",
    "        \n",
    "        # Try to get movie title if available\n",
    "        if movie_features is not None:\n",
    "            movie_row = movie_features[movie_features['movieId'] == movie_id]\n",
    "            if not movie_row.empty and 'title' in movie_row.columns:\n",
    "                movie_info = movie_row.iloc[0]['title']\n",
    "        \n",
    "        print(f\"{i}. {movie_info} - Predicted Rating: {predicted_rating:.2f}\")\n",
    "\n",
    "# Main execution flow\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COLLABORATIVE FILTERING WITH DEEP NEURAL NETWORK\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Load Data\n",
    "    data = load_data()\n",
    "    if data is None:\n",
    "        logger.error(\"Failed to load required data\")\n",
    "        exit(1)\n",
    "\n",
    "    # Step 2: Extract Genre Features\n",
    "    movie_genre_features = extract_genre_features(data['movie_features'])\n",
    "    if movie_genre_features is None:\n",
    "        logger.error(\"Failed to extract genre features\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Step 3: Calculate User Genre Preferences\n",
    "    user_genre_preferences = calculate_user_genre_preferences(data['train_ratings'], movie_genre_features)\n",
    "    \n",
    "    # Step 4: Prepare Training Data for DNN\n",
    "    X_train, X_val, y_train, y_val, genre_columns = prepare_dnn_training_data(\n",
    "        data['train_ratings'], \n",
    "        user_genre_preferences, \n",
    "        movie_genre_features\n",
    "    )\n",
    "    \n",
    "    # Step 5: Build and Train DNN Model\n",
    "    dnn_model, training_history = build_and_train_dnn_model(X_train, X_val, y_train, y_val)\n",
    "    \n",
    "    # Save DNN model\n",
    "    dnn_model.save(os.path.join(output_path, 'dnn_model.h5'))\n",
    "    logger.info(f\"Saved DNN model to {os.path.join(output_path, 'dnn_model.h5')}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot MSE loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(training_history.history['loss'], label='Training MSE')\n",
    "    plt.plot(training_history.history['val_loss'], label='Validation MSE')\n",
    "    plt.title('Model MSE Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot MAE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(training_history.history['mae'], label='Training MAE')\n",
    "    plt.plot(training_history.history['val_mae'], label='Validation MAE')\n",
    "    plt.title('Model MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_path, 'dnn_training_history.png'))\n",
    "    logger.info(f\"Saved training history plot to {os.path.join(output_path, 'dnn_training_history.png')}\")\n",
    "    \n",
    "    # Step 6: Generate Recommendations for a Subset of Users\n",
    "    # Limit to 100 users for demonstration (can be increased)\n",
    "    max_users = len(user_genre_preferences['userId'].unique())\n",
    "    dnn_recommendations = generate_recommendations_for_all_users(\n",
    "        dnn_model,\n",
    "        user_genre_preferences,\n",
    "        movie_genre_features,\n",
    "        data['train_ratings'],\n",
    "        top_n,\n",
    "        max_users\n",
    "    )\n",
    "    \n",
    "    # Save recommendations\n",
    "    with open(os.path.join(output_path, 'dnn_recommendations.pkl'), 'wb') as f:\n",
    "        pickle.dump(dnn_recommendations, f)\n",
    "    \n",
    "    # Also save in a more readable CSV format\n",
    "    recommendations_list = []\n",
    "    \n",
    "    for user_id, recs in dnn_recommendations.items():\n",
    "        for rank, (movie_id, predicted_rating) in enumerate(recs, 1):\n",
    "            movie_title = \"Unknown\"\n",
    "            movie_row = data['movie_features'][data['movie_features']['movieId'] == movie_id]\n",
    "            if not movie_row.empty and 'title' in movie_row.columns:\n",
    "                movie_title = movie_row.iloc[0]['title']\n",
    "                    \n",
    "            recommendations_list.append({\n",
    "                'userId': user_id,\n",
    "                'movieId': movie_id,\n",
    "                'title': movie_title,\n",
    "                'rank': rank,\n",
    "                'predicted_rating': predicted_rating\n",
    "            })\n",
    "    \n",
    "    if recommendations_list:\n",
    "        recommendations_df = pd.DataFrame(recommendations_list)\n",
    "        recommendations_df.to_csv(os.path.join(output_path, 'dnn_recommendations.csv'), index=False)\n",
    "    \n",
    "    logger.info(f\"Generated DNN recommendations for {len(dnn_recommendations)} users\")\n",
    "    \n",
    "    # Step 7: Evaluate Recommendations\n",
    "    evaluation_metrics = evaluate_recommendations(\n",
    "        dnn_recommendations,\n",
    "        data['test_ratings'],\n",
    "        dnn_model,\n",
    "        user_genre_preferences,\n",
    "        movie_genre_features\n",
    "    )\n",
    "    \n",
    "    # Save metrics\n",
    "    evaluation_results = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_results.to_csv(os.path.join(output_path, 'dnn_evaluation.csv'), index=False)\n",
    "    \n",
    "    # Display evaluation metrics\n",
    "    print(\"\\nDNN Evaluation Results:\")\n",
    "    print(f\"RMSE: {evaluation_metrics['rmse']:.4f}\")\n",
    "    print(f\"MAE: {evaluation_metrics['mae']:.4f}\")\n",
    "    print(f\"Number of predictions evaluated: {evaluation_metrics['num_predictions']}\")\n",
    "    \n",
    "    # Display sample recommendations for a few users\n",
    "    if dnn_recommendations:\n",
    "        sample_user_id = next(iter(dnn_recommendations.keys()))\n",
    "        recommend_for_user(sample_user_id, dnn_recommendations, data['movie_features'])\n",
    "    \n",
    "    # Final Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY: COLLABORATIVE FILTERING WITH DNN\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Display model architecture\n",
    "    print(\"\\nDNN Model Architecture:\")\n",
    "    dnn_model.summary(print_fn=print)\n",
    "    print(f\"\\nNumber of layers: {len(dnn_model.layers)}\")\n",
    "    print(f\"Hidden layer sizes: {dnn_hidden_layers}\")\n",
    "    print(f\"Dropout rate: {dnn_dropout_rate}\")\n",
    "    print(f\"Learning rate: {dnn_learning_rate}\")\n",
    "    print(f\"Batch size: {dnn_batch_size}\")\n",
    "    \n",
    "    # Display performance metrics\n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    headers = [\"Model\", \"RMSE\", \"MAE\", \"Predictions Evaluated\"]\n",
    "    rows = [\n",
    "        [\n",
    "            \"Collaborative Filtering (DNN)\",\n",
    "            f\"{evaluation_metrics['rmse']:.4f}\",\n",
    "            f\"{evaluation_metrics['mae']:.4f}\",\n",
    "            str(evaluation_metrics['num_predictions'])\n",
    "        ]\n",
    "    ]\n",
    "    \n",
    "    # Print table\n",
    "    col_widths = [max(len(row[i]) for row in [headers] + rows) for i in range(len(headers))]\n",
    "    print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")\n",
    "    print(\"| \" + \" | \".join(headers[i].ljust(col_widths[i]) for i in range(len(headers))) + \" |\")\n",
    "    print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")\n",
    "    for row in rows:\n",
    "        print(\"| \" + \" | \".join(row[i].ljust(col_widths[i]) for i in range(len(row))) + \" |\")\n",
    "    print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")\n",
    "    \n",
    "    # Recommendations statistics\n",
    "    if dnn_recommendations:\n",
    "        avg_recs = sum(len(recs) for recs in dnn_recommendations.values()) / len(dnn_recommendations)\n",
    "        print(f\"\\nRecommendation Statistics:\")\n",
    "        print(f\"- Users with recommendations: {len(dnn_recommendations)}\")\n",
    "        print(f\"- Average recommendations per user: {avg_recs:.2f}\")\n",
    "    \n",
    "    \n",
    "    # Save batch file to run recommendations for any user\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "# Define path to model and data\n",
    "recommendation_path = \"{output_path}\"\n",
    "model_path = os.path.join(recommendation_path, 'dnn_model.h5')\n",
    "\n",
    "# Check if model exists\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"Error: Model not found at\", model_path)\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = load_model(model_path)\n",
    "print(\"Model loaded successfully from\", model_path)\n",
    "\n",
    "# Load other"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
