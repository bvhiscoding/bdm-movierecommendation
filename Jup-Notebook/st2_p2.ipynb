{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 01:53:33,623 : INFO : Starting enhanced DNN-based recommendation pipeline\n",
      "2025-04-27 01:53:33,624 : INFO : Initial memory usage: 384.50 MB\n",
      "2025-04-27 01:53:33,625 : INFO : Before loading data: 384.50 MB\n",
      "2025-04-27 01:53:33,958 : INFO : Loaded movie features with shape (24378, 38)\n",
      "2025-04-27 01:54:02,273 : INFO : Loaded ratings with shape (14472091, 4)\n",
      "2025-04-27 01:54:02,273 : INFO : After loading data: 1307.35 MB\n",
      "2025-04-27 01:54:02,273 : INFO : Creating improved train/test split\n",
      "2025-04-27 01:55:25,182 : INFO : Training set: 13875887 ratings from 100000 users\n",
      "2025-04-27 01:55:25,188 : INFO : Test set: 596204 ratings from 20000 users\n",
      "2025-04-27 01:55:25,475 : INFO : Found 7 region features: ['East Asia', 'Middle East', 'North America', 'Oceania', 'Other', 'South Asia', 'Southeast Asia']\n",
      "2025-04-27 01:55:25,475 : INFO : After split preparation: 1906.69 MB\n",
      "2025-04-27 01:55:25,514 : INFO : After loading data: 1462.85 MB\n",
      "2025-04-27 01:55:25,528 : INFO : Extracted 26 genre features and 7 region features\n",
      "2025-04-27 01:55:25,530 : INFO : After feature extraction: 1467.60 MB\n",
      "2025-04-27 01:55:25,531 : INFO : Calculating user preferences for 33 features\n",
      "2025-04-27 01:57:06,132 : INFO : Processed 1000/100000 users (1.0%) - Elapsed: 100.54s - Est. remaining: 9953.16s\n",
      "2025-04-27 01:58:34,480 : INFO : Processed 2000/100000 users (2.0%) - Elapsed: 188.89s - Est. remaining: 9255.39s\n",
      "2025-04-27 01:59:48,768 : INFO : Processed 3000/100000 users (3.0%) - Elapsed: 263.17s - Est. remaining: 8509.25s\n",
      "2025-04-27 02:01:03,454 : INFO : Processed 4000/100000 users (4.0%) - Elapsed: 337.86s - Est. remaining: 8108.62s\n",
      "2025-04-27 02:02:19,994 : INFO : Processed 5000/100000 users (5.0%) - Elapsed: 414.40s - Est. remaining: 7873.58s\n",
      "2025-04-27 02:03:44,660 : INFO : Processed 6000/100000 users (6.0%) - Elapsed: 499.06s - Est. remaining: 7818.68s\n",
      "2025-04-27 02:05:10,731 : INFO : Processed 7000/100000 users (7.0%) - Elapsed: 585.14s - Est. remaining: 7773.96s\n",
      "2025-04-27 02:06:37,688 : INFO : Processed 8000/100000 users (8.0%) - Elapsed: 672.09s - Est. remaining: 7729.06s\n",
      "2025-04-27 02:07:58,718 : INFO : Processed 9000/100000 users (9.0%) - Elapsed: 753.12s - Est. remaining: 7614.92s\n",
      "2025-04-27 02:09:15,880 : INFO : Processed 10000/100000 users (10.0%) - Elapsed: 830.29s - Est. remaining: 7472.57s\n",
      "2025-04-27 02:10:33,367 : INFO : Processed 11000/100000 users (11.0%) - Elapsed: 907.77s - Est. remaining: 7344.70s\n",
      "2025-04-27 02:11:50,520 : INFO : Processed 12000/100000 users (12.0%) - Elapsed: 984.92s - Est. remaining: 7222.78s\n",
      "2025-04-27 02:13:07,622 : INFO : Processed 13000/100000 users (13.0%) - Elapsed: 1062.03s - Est. remaining: 7107.41s\n",
      "2025-04-27 02:14:24,299 : INFO : Processed 14000/100000 users (14.0%) - Elapsed: 1138.70s - Est. remaining: 6994.89s\n",
      "2025-04-27 02:15:41,606 : INFO : Processed 15000/100000 users (15.0%) - Elapsed: 1216.01s - Est. remaining: 6890.73s\n",
      "2025-04-27 02:16:58,308 : INFO : Processed 16000/100000 users (16.0%) - Elapsed: 1292.71s - Est. remaining: 6786.75s\n",
      "2025-04-27 02:18:15,198 : INFO : Processed 17000/100000 users (17.0%) - Elapsed: 1369.60s - Est. remaining: 6686.89s\n",
      "2025-04-27 02:19:32,375 : INFO : Processed 18000/100000 users (18.0%) - Elapsed: 1446.78s - Est. remaining: 6590.89s\n",
      "2025-04-27 02:20:49,974 : INFO : Processed 19000/100000 users (19.0%) - Elapsed: 1524.38s - Est. remaining: 6498.67s\n",
      "2025-04-27 02:22:06,726 : INFO : Processed 20000/100000 users (20.0%) - Elapsed: 1601.13s - Est. remaining: 6404.53s\n",
      "2025-04-27 02:23:23,635 : INFO : Processed 21000/100000 users (21.0%) - Elapsed: 1678.04s - Est. remaining: 6312.63s\n",
      "2025-04-27 02:24:41,071 : INFO : Processed 22000/100000 users (22.0%) - Elapsed: 1755.48s - Est. remaining: 6223.96s\n",
      "2025-04-27 02:25:58,214 : INFO : Processed 23000/100000 users (23.0%) - Elapsed: 1832.62s - Est. remaining: 6135.29s\n",
      "2025-04-27 02:27:15,326 : INFO : Processed 24000/100000 users (24.0%) - Elapsed: 1909.73s - Est. remaining: 6047.48s\n",
      "2025-04-27 02:28:32,152 : INFO : Processed 25000/100000 users (25.0%) - Elapsed: 1986.56s - Est. remaining: 5959.67s\n",
      "2025-04-27 02:29:49,357 : INFO : Processed 26000/100000 users (26.0%) - Elapsed: 2063.76s - Est. remaining: 5873.78s\n",
      "2025-04-27 02:31:06,110 : INFO : Processed 27000/100000 users (27.0%) - Elapsed: 2140.52s - Est. remaining: 5787.32s\n",
      "2025-04-27 02:32:23,719 : INFO : Processed 28000/100000 users (28.0%) - Elapsed: 2218.12s - Est. remaining: 5703.75s\n",
      "2025-04-27 02:33:41,206 : INFO : Processed 29000/100000 users (29.0%) - Elapsed: 2295.61s - Est. remaining: 5620.29s\n",
      "2025-04-27 02:34:58,378 : INFO : Processed 30000/100000 users (30.0%) - Elapsed: 2372.78s - Est. remaining: 5536.49s\n",
      "2025-04-27 02:36:14,659 : INFO : Processed 31000/100000 users (31.0%) - Elapsed: 2449.06s - Est. remaining: 5451.14s\n",
      "2025-04-27 02:37:31,219 : INFO : Processed 32000/100000 users (32.0%) - Elapsed: 2525.62s - Est. remaining: 5366.95s\n",
      "2025-04-27 02:38:48,119 : INFO : Processed 33000/100000 users (33.0%) - Elapsed: 2602.52s - Est. remaining: 5283.91s\n",
      "2025-04-27 02:40:06,119 : INFO : Processed 34000/100000 users (34.0%) - Elapsed: 2680.52s - Est. remaining: 5203.37s\n",
      "2025-04-27 02:41:24,010 : INFO : Processed 35000/100000 users (35.0%) - Elapsed: 2758.41s - Est. remaining: 5122.77s\n",
      "2025-04-27 02:42:41,164 : INFO : Processed 36000/100000 users (36.0%) - Elapsed: 2835.57s - Est. remaining: 5041.01s\n",
      "2025-04-27 02:43:57,861 : INFO : Processed 37000/100000 users (37.0%) - Elapsed: 2912.27s - Est. remaining: 4958.72s\n",
      "2025-04-27 02:45:14,849 : INFO : Processed 38000/100000 users (38.0%) - Elapsed: 2989.25s - Est. remaining: 4877.20s\n",
      "2025-04-27 02:46:31,641 : INFO : Processed 39000/100000 users (39.0%) - Elapsed: 3066.05s - Est. remaining: 4795.61s\n",
      "2025-04-27 02:47:48,593 : INFO : Processed 40000/100000 users (40.0%) - Elapsed: 3143.00s - Est. remaining: 4714.50s\n",
      "2025-04-27 02:49:05,724 : INFO : Processed 41000/100000 users (41.0%) - Elapsed: 3220.13s - Est. remaining: 4633.84s\n",
      "2025-04-27 02:50:26,262 : INFO : Processed 42000/100000 users (42.0%) - Elapsed: 3300.67s - Est. remaining: 4558.06s\n",
      "2025-04-27 02:51:49,374 : INFO : Processed 43000/100000 users (43.0%) - Elapsed: 3383.78s - Est. remaining: 4485.47s\n",
      "2025-04-27 02:53:13,063 : INFO : Processed 44000/100000 users (44.0%) - Elapsed: 3467.47s - Est. remaining: 4413.14s\n",
      "2025-04-27 02:54:36,292 : INFO : Processed 45000/100000 users (45.0%) - Elapsed: 3550.70s - Est. remaining: 4339.74s\n",
      "2025-04-27 02:55:59,045 : INFO : Processed 46000/100000 users (46.0%) - Elapsed: 3633.45s - Est. remaining: 4265.35s\n",
      "2025-04-27 02:57:21,809 : INFO : Processed 47000/100000 users (47.0%) - Elapsed: 3716.21s - Est. remaining: 4190.62s\n",
      "2025-04-27 02:58:45,148 : INFO : Processed 48000/100000 users (48.0%) - Elapsed: 3799.55s - Est. remaining: 4116.18s\n",
      "2025-04-27 03:00:07,950 : INFO : Processed 49000/100000 users (49.0%) - Elapsed: 3882.35s - Est. remaining: 4040.82s\n",
      "2025-04-27 03:01:30,555 : INFO : Processed 50000/100000 users (50.0%) - Elapsed: 3964.96s - Est. remaining: 3964.96s\n",
      "2025-04-27 03:02:53,675 : INFO : Processed 51000/100000 users (51.0%) - Elapsed: 4048.08s - Est. remaining: 3889.33s\n",
      "2025-04-27 03:04:16,379 : INFO : Processed 52000/100000 users (52.0%) - Elapsed: 4130.78s - Est. remaining: 3813.03s\n",
      "2025-04-27 03:05:38,875 : INFO : Processed 53000/100000 users (53.0%) - Elapsed: 4213.28s - Est. remaining: 3736.31s\n",
      "2025-04-27 03:07:02,282 : INFO : Processed 54000/100000 users (54.0%) - Elapsed: 4296.69s - Est. remaining: 3660.14s\n",
      "2025-04-27 03:08:24,570 : INFO : Processed 55000/100000 users (55.0%) - Elapsed: 4378.97s - Est. remaining: 3582.80s\n",
      "2025-04-27 03:09:47,368 : INFO : Processed 56000/100000 users (56.0%) - Elapsed: 4461.77s - Est. remaining: 3505.68s\n",
      "2025-04-27 03:11:10,296 : INFO : Processed 57000/100000 users (57.0%) - Elapsed: 4544.70s - Est. remaining: 3428.46s\n",
      "2025-04-27 03:12:33,531 : INFO : Processed 58000/100000 users (58.0%) - Elapsed: 4627.94s - Est. remaining: 3351.26s\n",
      "2025-04-27 03:13:56,825 : INFO : Processed 59000/100000 users (59.0%) - Elapsed: 4711.23s - Est. remaining: 3273.91s\n",
      "2025-04-27 03:15:19,408 : INFO : Processed 60000/100000 users (60.0%) - Elapsed: 4793.81s - Est. remaining: 3195.88s\n",
      "2025-04-27 03:16:41,901 : INFO : Processed 61000/100000 users (61.0%) - Elapsed: 4876.31s - Est. remaining: 3117.64s\n",
      "2025-04-27 03:18:05,228 : INFO : Processed 62000/100000 users (62.0%) - Elapsed: 4959.63s - Est. remaining: 3039.78s\n",
      "2025-04-27 03:19:28,052 : INFO : Processed 63000/100000 users (63.0%) - Elapsed: 5042.46s - Est. remaining: 2961.44s\n",
      "2025-04-27 03:20:48,910 : INFO : Processed 64000/100000 users (64.0%) - Elapsed: 5123.32s - Est. remaining: 2881.86s\n",
      "2025-04-27 03:22:02,979 : INFO : Processed 65000/100000 users (65.0%) - Elapsed: 5197.38s - Est. remaining: 2798.59s\n",
      "2025-04-27 03:23:17,209 : INFO : Processed 66000/100000 users (66.0%) - Elapsed: 5271.61s - Est. remaining: 2715.68s\n",
      "2025-04-27 03:24:31,623 : INFO : Processed 67000/100000 users (67.0%) - Elapsed: 5346.03s - Est. remaining: 2633.12s\n",
      "2025-04-27 03:25:46,985 : INFO : Processed 68000/100000 users (68.0%) - Elapsed: 5421.39s - Est. remaining: 2551.24s\n",
      "2025-04-27 03:27:11,163 : INFO : Processed 69000/100000 users (69.0%) - Elapsed: 5505.57s - Est. remaining: 2473.52s\n",
      "2025-04-27 03:28:34,908 : INFO : Processed 70000/100000 users (70.0%) - Elapsed: 5589.31s - Est. remaining: 2395.42s\n",
      "2025-04-27 03:29:58,725 : INFO : Processed 71000/100000 users (71.0%) - Elapsed: 5673.13s - Est. remaining: 2317.19s\n",
      "2025-04-27 03:31:23,401 : INFO : Processed 72000/100000 users (72.0%) - Elapsed: 5757.81s - Est. remaining: 2239.15s\n",
      "2025-04-27 03:32:47,260 : INFO : Processed 73000/100000 users (73.0%) - Elapsed: 5841.66s - Est. remaining: 2160.62s\n",
      "2025-04-27 03:34:10,900 : INFO : Processed 74000/100000 users (74.0%) - Elapsed: 5925.31s - Est. remaining: 2081.86s\n",
      "2025-04-27 03:35:34,915 : INFO : Processed 75000/100000 users (75.0%) - Elapsed: 6009.32s - Est. remaining: 2003.11s\n",
      "2025-04-27 03:36:58,969 : INFO : Processed 76000/100000 users (76.0%) - Elapsed: 6093.37s - Est. remaining: 1924.22s\n",
      "2025-04-27 03:38:23,913 : INFO : Processed 77000/100000 users (77.0%) - Elapsed: 6178.32s - Est. remaining: 1845.47s\n",
      "2025-04-27 03:39:48,033 : INFO : Processed 78000/100000 users (78.0%) - Elapsed: 6262.44s - Est. remaining: 1766.33s\n",
      "2025-04-27 03:41:12,069 : INFO : Processed 79000/100000 users (79.0%) - Elapsed: 6346.47s - Est. remaining: 1687.04s\n",
      "2025-04-27 03:42:35,646 : INFO : Processed 80000/100000 users (80.0%) - Elapsed: 6430.05s - Est. remaining: 1607.51s\n",
      "2025-04-27 03:43:57,865 : INFO : Processed 81000/100000 users (81.0%) - Elapsed: 6512.27s - Est. remaining: 1527.57s\n",
      "2025-04-27 03:45:20,402 : INFO : Processed 82000/100000 users (82.0%) - Elapsed: 6594.81s - Est. remaining: 1447.64s\n",
      "2025-04-27 03:46:41,892 : INFO : Processed 83000/100000 users (83.0%) - Elapsed: 6676.30s - Est. remaining: 1367.43s\n",
      "2025-04-27 03:48:03,536 : INFO : Processed 84000/100000 users (84.0%) - Elapsed: 6757.94s - Est. remaining: 1287.23s\n",
      "2025-04-27 03:49:25,874 : INFO : Processed 85000/100000 users (85.0%) - Elapsed: 6840.28s - Est. remaining: 1207.11s\n",
      "2025-04-27 03:50:48,388 : INFO : Processed 86000/100000 users (86.0%) - Elapsed: 6922.79s - Est. remaining: 1126.97s\n",
      "2025-04-27 03:52:11,143 : INFO : Processed 87000/100000 users (87.0%) - Elapsed: 7005.55s - Est. remaining: 1046.81s\n",
      "2025-04-27 03:53:33,643 : INFO : Processed 88000/100000 users (88.0%) - Elapsed: 7088.05s - Est. remaining: 966.55s\n",
      "2025-04-27 03:54:55,050 : INFO : Processed 89000/100000 users (89.0%) - Elapsed: 7169.45s - Est. remaining: 886.11s\n",
      "2025-04-27 03:56:17,821 : INFO : Processed 90000/100000 users (90.0%) - Elapsed: 7252.23s - Est. remaining: 805.80s\n",
      "2025-04-27 03:57:40,026 : INFO : Processed 91000/100000 users (91.0%) - Elapsed: 7334.43s - Est. remaining: 725.38s\n",
      "2025-04-27 03:59:02,553 : INFO : Processed 92000/100000 users (92.0%) - Elapsed: 7416.96s - Est. remaining: 644.95s\n",
      "2025-04-27 04:00:25,516 : INFO : Processed 93000/100000 users (93.0%) - Elapsed: 7499.92s - Est. remaining: 564.51s\n",
      "2025-04-27 04:01:48,813 : INFO : Processed 94000/100000 users (94.0%) - Elapsed: 7583.22s - Est. remaining: 484.04s\n",
      "2025-04-27 04:03:11,229 : INFO : Processed 95000/100000 users (95.0%) - Elapsed: 7665.63s - Est. remaining: 403.45s\n",
      "2025-04-27 04:04:33,427 : INFO : Processed 96000/100000 users (96.0%) - Elapsed: 7747.83s - Est. remaining: 322.83s\n",
      "2025-04-27 04:05:54,754 : INFO : Processed 97000/100000 users (97.0%) - Elapsed: 7829.16s - Est. remaining: 242.14s\n",
      "2025-04-27 04:07:16,600 : INFO : Processed 98000/100000 users (98.0%) - Elapsed: 7911.01s - Est. remaining: 161.45s\n",
      "2025-04-27 04:08:39,426 : INFO : Processed 99000/100000 users (99.0%) - Elapsed: 7993.83s - Est. remaining: 80.75s\n",
      "2025-04-27 04:10:01,862 : INFO : Processed 100000/100000 users (100.0%) - Elapsed: 8076.27s - Est. remaining: 0.00s\n",
      "2025-04-27 04:10:02,899 : INFO : Created preferences for 100000 users\n",
      "2025-04-27 04:10:04,986 : INFO : Saved user preferences for 100000 users\n",
      "2025-04-27 04:10:05,071 : INFO : Saved movie genre features for 24378 movies\n",
      "2025-04-27 04:10:05,072 : INFO : After user preferences calculation: 1566.43 MB\n",
      "2025-04-27 04:10:05,073 : INFO : Preparing training data for DNN model with enhanced features\n",
      "2025-04-27 04:45:18,679 : INFO : Processed 10000/999998 ratings (1.0%) - Elapsed: 6.55s - Est. remaining: 648.11s\n",
      "2025-04-27 04:46:27,564 : INFO : Processed 110000/999998 ratings (11.0%) - Elapsed: 75.43s - Est. remaining: 610.31s\n",
      "2025-04-27 04:47:37,804 : INFO : Processed 210000/999998 ratings (21.0%) - Elapsed: 145.67s - Est. remaining: 548.00s\n",
      "2025-04-27 04:48:50,423 : INFO : Processed 310000/999998 ratings (31.0%) - Elapsed: 218.29s - Est. remaining: 485.87s\n",
      "2025-04-27 04:50:04,684 : INFO : Processed 410000/999998 ratings (41.0%) - Elapsed: 292.55s - Est. remaining: 420.99s\n",
      "2025-04-27 04:51:22,426 : INFO : Processed 510000/999998 ratings (51.0%) - Elapsed: 370.29s - Est. remaining: 355.77s\n",
      "2025-04-27 04:52:42,954 : INFO : Processed 610000/999998 ratings (61.0%) - Elapsed: 450.82s - Est. remaining: 288.23s\n",
      "2025-04-27 04:54:04,219 : INFO : Processed 710000/999998 ratings (71.0%) - Elapsed: 532.09s - Est. remaining: 217.33s\n",
      "2025-04-27 04:55:27,223 : INFO : Processed 810000/999998 ratings (81.0%) - Elapsed: 615.09s - Est. remaining: 144.28s\n",
      "2025-04-27 09:57:01,326 : INFO : Processed 910000/999998 ratings (91.0%) - Elapsed: 18709.19s - Est. remaining: 1850.32s\n",
      "2025-04-27 09:58:43,561 : INFO : Created training data: X_train shape (798099, 173), y_train shape (798099,)\n",
      "2025-04-27 09:58:43,567 : INFO : Created validation data: X_val shape (199525, 173), y_val shape (199525,)\n",
      "2025-04-27 09:58:43,570 : INFO : Training set class distribution: Positive 485782.0 (60.9%), Negative 312317.0 (39.1%)\n",
      "2025-04-27 09:58:43,570 : INFO : Validation set class distribution: Positive 121446.0 (60.9%), Negative 78079.0 (39.1%)\n",
      "2025-04-27 09:58:47,055 : INFO : After training data preparation: 1259.70 MB\n",
      "2025-04-27 09:58:47,059 : INFO : Building and training enhanced DNN model\n",
      "2025-04-27 09:58:47,368 : INFO : Class weights: {0: 1.2777066249995996, 1: 0.8214579790935029}\n",
      "2025-04-27 09:58:47,368 : INFO : Training model with 30 max epochs, batch size 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m3117/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.7000 - auc: 0.7661 - f1_metric: 147.6149 - loss: 0.7085 - precision: 0.7798 - recall: 0.7051\n",
      "Epoch 1: val_auc improved from -inf to 0.79878, saving model to ./rec/collaborative-recommendations\\best_model.keras\n",
      "\u001b[1m3118/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 72ms/step - accuracy: 0.7000 - auc: 0.7661 - f1_metric: 147.6157 - loss: 0.7085 - precision: 0.7798 - recall: 0.7051 - val_accuracy: 0.7265 - val_auc: 0.7988 - val_f1_metric: 150.3040 - val_loss: 0.5565 - val_precision: 0.7947 - val_recall: 0.7425\n",
      "Epoch 2/30\n",
      "\u001b[1m3117/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.7248 - auc: 0.7963 - f1_metric: 148.9921 - loss: 0.5546 - precision: 0.7984 - recall: 0.7329\n",
      "Epoch 2: val_auc improved from 0.79878 to 0.80153, saving model to ./rec/collaborative-recommendations\\best_model.keras\n",
      "\u001b[1m3118/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 72ms/step - accuracy: 0.7248 - auc: 0.7963 - f1_metric: 148.9921 - loss: 0.5546 - precision: 0.7984 - recall: 0.7329 - val_accuracy: 0.7251 - val_auc: 0.8015 - val_f1_metric: 148.1487 - val_loss: 0.5438 - val_precision: 0.8017 - val_recall: 0.7286\n",
      "Epoch 3/30\n",
      "\u001b[1m3117/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.7261 - auc: 0.7985 - f1_metric: 148.7863 - loss: 0.5493 - precision: 0.8006 - recall: 0.7326\n",
      "Epoch 3: val_auc improved from 0.80153 to 0.80416, saving model to ./rec/collaborative-recommendations\\best_model.keras\n",
      "\u001b[1m3118/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 69ms/step - accuracy: 0.7261 - auc: 0.7985 - f1_metric: 148.7865 - loss: 0.5493 - precision: 0.8006 - recall: 0.7326 - val_accuracy: 0.7312 - val_auc: 0.8042 - val_f1_metric: 150.9496 - val_loss: 0.5409 - val_precision: 0.7963 - val_recall: 0.7503\n",
      "Epoch 4/30\n",
      "\u001b[1m3117/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.7301 - auc: 0.8009 - f1_metric: 150.2634 - loss: 0.5454 - precision: 0.7989 - recall: 0.7442\n",
      "Epoch 4: val_auc improved from 0.80416 to 0.80522, saving model to ./rec/collaborative-recommendations\\best_model.keras\n",
      "\u001b[1m3118/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 69ms/step - accuracy: 0.7301 - auc: 0.8009 - f1_metric: 150.2633 - loss: 0.5454 - precision: 0.7989 - recall: 0.7442 - val_accuracy: 0.7306 - val_auc: 0.8052 - val_f1_metric: 150.0470 - val_loss: 0.5361 - val_precision: 0.7993 - val_recall: 0.7443\n",
      "Epoch 5/30\n",
      "\u001b[1m3117/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7299 - auc: 0.8020 - f1_metric: 149.9458 - loss: 0.5434 - precision: 0.7992 - recall: 0.7428\n",
      "Epoch 5: val_auc did not improve from 0.80522\n",
      "\u001b[1m3118/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 56ms/step - accuracy: 0.7299 - auc: 0.8020 - f1_metric: 149.9459 - loss: 0.5434 - precision: 0.7992 - recall: 0.7428 - val_accuracy: 0.7303 - val_auc: 0.8048 - val_f1_metric: 150.1611 - val_loss: 0.5323 - val_precision: 0.7985 - val_recall: 0.7448\n",
      "Epoch 6/30\n",
      "\u001b[1m3117/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7269 - auc: 0.7991 - f1_metric: 149.3788 - loss: 0.5476 - precision: 0.8004 - recall: 0.7357\n",
      "Epoch 6: val_auc did not improve from 0.80522\n",
      "\u001b[1m3118/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 49ms/step - accuracy: 0.7269 - auc: 0.7991 - f1_metric: 149.3786 - loss: 0.5476 - precision: 0.8004 - recall: 0.7357 - val_accuracy: 0.7317 - val_auc: 0.8031 - val_f1_metric: 151.6703 - val_loss: 0.5436 - val_precision: 0.7941 - val_recall: 0.7551\n",
      "Epoch 7/30\n",
      "\u001b[1m3118/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7280 - auc: 0.8009 - f1_metric: 148.6767 - loss: 0.5459 - precision: 0.8022 - recall: 0.7340\n",
      "Epoch 7: val_auc did not improve from 0.80522\n",
      "\u001b[1m3118/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 49ms/step - accuracy: 0.7280 - auc: 0.8009 - f1_metric: 148.6768 - loss: 0.5459 - precision: 0.8022 - recall: 0.7340 - val_accuracy: 0.7288 - val_auc: 0.8039 - val_f1_metric: 149.4918 - val_loss: 0.5403 - val_precision: 0.7998 - val_recall: 0.7396\n",
      "Epoch 8/30\n",
      "\u001b[1m3118/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7274 - auc: 0.7998 - f1_metric: 149.1525 - loss: 0.5469 - precision: 0.8004 - recall: 0.7358\n",
      "Epoch 8: val_auc did not improve from 0.80522\n",
      "\u001b[1m3118/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 49ms/step - accuracy: 0.7274 - auc: 0.7998 - f1_metric: 149.1524 - loss: 0.5469 - precision: 0.8004 - recall: 0.7358 - val_accuracy: 0.7257 - val_auc: 0.8041 - val_f1_metric: 147.7096 - val_loss: 0.5348 - val_precision: 0.8039 - val_recall: 0.7265\n",
      "Epoch 9/30\n",
      "\u001b[1m3117/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7284 - auc: 0.8006 - f1_metric: 148.8795 - loss: 0.5458 - precision: 0.8018 - recall: 0.7354\n",
      "Epoch 9: val_auc did not improve from 0.80522\n",
      "\u001b[1m3118/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 49ms/step - accuracy: 0.7284 - auc: 0.8006 - f1_metric: 148.8796 - loss: 0.5458 - precision: 0.8018 - recall: 0.7354 - val_accuracy: 0.7291 - val_auc: 0.8049 - val_f1_metric: 149.1219 - val_loss: 0.5394 - val_precision: 0.8015 - val_recall: 0.7376\n",
      "Epoch 10/30\n",
      "\u001b[1m3117/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.7275 - auc: 0.8004 - f1_metric: 148.9532 - loss: 0.5460 - precision: 0.8004 - recall: 0.7353\n",
      "Epoch 10: val_auc improved from 0.80522 to 0.80532, saving model to ./rec/collaborative-recommendations\\best_model.keras\n",
      "\u001b[1m3118/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 48ms/step - accuracy: 0.7275 - auc: 0.8004 - f1_metric: 148.9535 - loss: 0.5460 - precision: 0.8004 - recall: 0.7353 - val_accuracy: 0.7322 - val_auc: 0.8053 - val_f1_metric: 151.6318 - val_loss: 0.5353 - val_precision: 0.7946 - val_recall: 0.7552\n",
      "Epoch 11/30\n",
      "\u001b[1m3118/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.7292 - auc: 0.8009 - f1_metric: 149.1251 - loss: 0.5453 - precision: 0.8003 - recall: 0.7385\n",
      "Epoch 11: val_auc improved from 0.80532 to 0.80587, saving model to ./rec/collaborative-recommendations\\best_model.keras\n",
      "\u001b[1m3118/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 46ms/step - accuracy: 0.7292 - auc: 0.8009 - f1_metric: 149.1251 - loss: 0.5453 - precision: 0.8003 - recall: 0.7385 - val_accuracy: 0.7286 - val_auc: 0.8059 - val_f1_metric: 148.5612 - val_loss: 0.5345 - val_precision: 0.8033 - val_recall: 0.7339\n",
      "Epoch 12/30\n",
      "\u001b[1m3118/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.7292 - auc: 0.8028 - f1_metric: 148.8318 - loss: 0.5427 - precision: 0.8032 - recall: 0.7354\n",
      "Epoch 12: val_auc improved from 0.80587 to 0.80624, saving model to ./rec/collaborative-recommendations\\best_model.keras\n",
      "\u001b[1m3118/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 44ms/step - accuracy: 0.7292 - auc: 0.8028 - f1_metric: 148.8318 - loss: 0.5427 - precision: 0.8032 - recall: 0.7354 - val_accuracy: 0.7310 - val_auc: 0.8062 - val_f1_metric: 149.9011 - val_loss: 0.5370 - val_precision: 0.8002 - val_recall: 0.7438\n",
      "Epoch 13/30\n",
      "\u001b[1m3118/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.7291 - auc: 0.8026 - f1_metric: 148.6598 - loss: 0.5425 - precision: 0.8042 - recall: 0.7339\n",
      "Epoch 13: val_auc improved from 0.80624 to 0.80666, saving model to ./rec/collaborative-recommendations\\best_model.keras\n",
      "\u001b[1m3118/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 45ms/step - accuracy: 0.7291 - auc: 0.8026 - f1_metric: 148.6598 - loss: 0.5425 - precision: 0.8042 - recall: 0.7340 - val_accuracy: 0.7292 - val_auc: 0.8067 - val_f1_metric: 148.6695 - val_loss: 0.5365 - val_precision: 0.8033 - val_recall: 0.7350\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m6236/6236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.7309 - auc: 0.8042 - f1_metric: 18.7227 - loss: 0.5333 - precision: 0.8006 - recall: 0.7449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 10:36:13,719 : INFO : Model validation metrics:\n",
      "2025-04-27 10:36:13,721 : INFO : - Loss: 0.5323\n",
      "2025-04-27 10:36:13,721 : INFO : - Accuracy: 0.7303\n",
      "2025-04-27 10:36:13,722 : INFO : - AUC: 0.8048\n",
      "2025-04-27 10:36:13,722 : INFO : - Precision: 0.7985\n",
      "2025-04-27 10:36:13,722 : INFO : - Recall: 0.7448\n",
      "2025-04-27 10:36:13,722 : INFO : - F1 Score: 18.6788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6236/6236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 10:36:24,061 : INFO : After model training: 1511.07 MB\n",
      "2025-04-27 10:36:24,061 : WARNING : You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "2025-04-27 10:36:24,103 : INFO : Saved trained DNN model\n",
      "2025-04-27 10:36:24,109 : INFO : Generating recommendations for 100000 users\n",
      "2025-04-27 12:01:17,261 : INFO : Processed 50/100000 users (0.1%) - Elapsed: 471.57s - Est. remaining: 942666.93s\n",
      "2025-04-27 12:05:19,895 : INFO : Processed 100/100000 users (0.1%) - Elapsed: 714.20s - Est. remaining: 713488.85s\n",
      "2025-04-27 12:07:51,553 : INFO : Processed 150/100000 users (0.1%) - Elapsed: 865.86s - Est. remaining: 576375.41s\n",
      "2025-04-27 12:10:15,154 : INFO : Processed 200/100000 users (0.2%) - Elapsed: 1009.46s - Est. remaining: 503721.66s\n",
      "2025-04-27 12:12:27,119 : INFO : Processed 250/100000 users (0.2%) - Elapsed: 1141.43s - Est. remaining: 455429.44s\n",
      "2025-04-27 12:14:39,310 : INFO : Processed 300/100000 users (0.3%) - Elapsed: 1273.62s - Est. remaining: 423265.80s\n",
      "2025-04-27 12:16:51,478 : INFO : Processed 350/100000 users (0.4%) - Elapsed: 1405.79s - Est. remaining: 400247.57s\n",
      "2025-04-27 12:19:02,935 : INFO : Processed 400/100000 users (0.4%) - Elapsed: 1537.24s - Est. remaining: 382773.60s\n",
      "2025-04-27 12:21:13,388 : INFO : Processed 450/100000 users (0.4%) - Elapsed: 1667.70s - Est. remaining: 368931.48s\n",
      "2025-04-27 12:23:24,404 : INFO : Processed 500/100000 users (0.5%) - Elapsed: 1798.71s - Est. remaining: 357943.82s\n",
      "2025-04-27 12:25:35,348 : INFO : Processed 550/100000 users (0.5%) - Elapsed: 1929.66s - Est. remaining: 348917.01s\n",
      "2025-04-27 12:27:46,573 : INFO : Processed 600/100000 users (0.6%) - Elapsed: 2060.88s - Est. remaining: 341419.30s\n",
      "2025-04-27 12:29:56,211 : INFO : Processed 650/100000 users (0.7%) - Elapsed: 2190.52s - Est. remaining: 334812.45s\n",
      "2025-04-27 12:32:07,223 : INFO : Processed 700/100000 users (0.7%) - Elapsed: 2321.53s - Est. remaining: 329325.81s\n",
      "2025-04-27 12:34:17,749 : INFO : Processed 750/100000 users (0.8%) - Elapsed: 2452.06s - Est. remaining: 324488.90s\n",
      "2025-04-27 12:36:27,907 : INFO : Processed 800/100000 users (0.8%) - Elapsed: 2582.22s - Est. remaining: 320194.74s\n",
      "2025-04-27 12:38:38,335 : INFO : Processed 850/100000 users (0.9%) - Elapsed: 2712.64s - Est. remaining: 316421.83s\n",
      "2025-04-27 12:40:48,672 : INFO : Processed 900/100000 users (0.9%) - Elapsed: 2842.98s - Est. remaining: 313043.67s\n",
      "2025-04-27 12:42:59,252 : INFO : Processed 950/100000 users (0.9%) - Elapsed: 2973.56s - Est. remaining: 310032.81s\n",
      "2025-04-27 12:45:09,009 : INFO : Processed 1000/100000 users (1.0%) - Elapsed: 3103.32s - Est. remaining: 307228.38s\n",
      "2025-04-27 12:47:19,942 : INFO : Processed 1050/100000 users (1.1%) - Elapsed: 3234.25s - Est. remaining: 304789.59s\n",
      "2025-04-27 12:49:29,150 : INFO : Processed 1100/100000 users (1.1%) - Elapsed: 3363.46s - Est. remaining: 302405.52s\n",
      "2025-04-27 12:51:40,445 : INFO : Processed 1150/100000 users (1.1%) - Elapsed: 3494.75s - Est. remaining: 300396.86s\n",
      "2025-04-27 12:53:50,763 : INFO : Processed 1200/100000 users (1.2%) - Elapsed: 3625.07s - Est. remaining: 298464.23s\n",
      "2025-04-27 12:56:01,198 : INFO : Processed 1250/100000 users (1.2%) - Elapsed: 3755.51s - Est. remaining: 296685.02s\n",
      "2025-04-27 12:58:11,362 : INFO : Processed 1300/100000 users (1.3%) - Elapsed: 3885.67s - Est. remaining: 295012.03s\n",
      "2025-04-27 13:00:21,907 : INFO : Processed 1350/100000 users (1.4%) - Elapsed: 4016.22s - Est. remaining: 293481.21s\n",
      "2025-04-27 13:02:33,128 : INFO : Processed 1400/100000 users (1.4%) - Elapsed: 4147.44s - Est. remaining: 292098.01s\n",
      "2025-04-27 13:04:42,717 : INFO : Processed 1450/100000 users (1.5%) - Elapsed: 4277.03s - Est. remaining: 290690.25s\n",
      "2025-04-27 13:07:17,158 : INFO : Processed 1500/100000 users (1.5%) - Elapsed: 4431.47s - Est. remaining: 290999.65s\n",
      "2025-04-27 13:10:04,285 : INFO : Processed 1550/100000 users (1.6%) - Elapsed: 4598.59s - Est. remaining: 292084.87s\n",
      "2025-04-27 13:12:30,127 : INFO : Processed 1600/100000 users (1.6%) - Elapsed: 4744.44s - Est. remaining: 291782.79s\n",
      "2025-04-27 13:15:09,165 : INFO : Processed 1650/100000 users (1.7%) - Elapsed: 4903.47s - Est. remaining: 292276.72s\n",
      "2025-04-27 13:17:53,783 : INFO : Processed 1700/100000 users (1.7%) - Elapsed: 5068.09s - Est. remaining: 293054.93s\n",
      "2025-04-27 13:20:29,937 : INFO : Processed 1750/100000 users (1.8%) - Elapsed: 5224.25s - Est. remaining: 293304.05s\n",
      "2025-04-27 13:23:17,505 : INFO : Processed 1800/100000 users (1.8%) - Elapsed: 5391.81s - Est. remaining: 294153.39s\n",
      "2025-04-27 13:26:04,554 : INFO : Processed 1850/100000 users (1.8%) - Elapsed: 5558.86s - Est. remaining: 294920.21s\n",
      "2025-04-27 13:28:54,513 : INFO : Processed 1900/100000 users (1.9%) - Elapsed: 5728.82s - Est. remaining: 295788.11s\n",
      "2025-04-27 13:31:40,193 : INFO : Processed 1950/100000 users (1.9%) - Elapsed: 5894.50s - Est. remaining: 296387.64s\n",
      "2025-04-27 13:34:17,587 : INFO : Processed 2000/100000 users (2.0%) - Elapsed: 6051.90s - Est. remaining: 296542.89s\n",
      "2025-04-27 13:36:47,197 : INFO : Processed 2050/100000 users (2.1%) - Elapsed: 6201.51s - Est. remaining: 296310.96s\n",
      "2025-04-27 13:39:24,580 : INFO : Processed 2100/100000 users (2.1%) - Elapsed: 6358.89s - Est. remaining: 296445.30s\n",
      "2025-04-27 13:42:02,474 : INFO : Processed 2150/100000 users (2.1%) - Elapsed: 6516.78s - Est. remaining: 296589.36s\n",
      "2025-04-27 13:44:38,310 : INFO : Processed 2200/100000 users (2.2%) - Elapsed: 6672.62s - Est. remaining: 296628.20s\n",
      "2025-04-27 13:47:13,562 : INFO : Processed 2250/100000 users (2.2%) - Elapsed: 6827.87s - Est. remaining: 296633.02s\n",
      "2025-04-27 13:49:54,210 : INFO : Processed 2300/100000 users (2.3%) - Elapsed: 6988.52s - Est. remaining: 296860.11s\n",
      "2025-04-27 13:52:32,748 : INFO : Processed 2350/100000 users (2.4%) - Elapsed: 7147.06s - Est. remaining: 296982.99s\n",
      "2025-04-27 13:55:24,653 : INFO : Processed 2400/100000 users (2.4%) - Elapsed: 7318.96s - Est. remaining: 297637.77s\n",
      "2025-04-27 13:58:19,549 : INFO : Processed 2450/100000 users (2.5%) - Elapsed: 7493.86s - Est. remaining: 298377.87s\n",
      "2025-04-27 14:01:07,617 : INFO : Processed 2500/100000 users (2.5%) - Elapsed: 7661.93s - Est. remaining: 298815.08s\n",
      "2025-04-27 14:03:43,216 : INFO : Processed 2550/100000 users (2.5%) - Elapsed: 7817.52s - Est. remaining: 298752.05s\n",
      "2025-04-27 14:06:21,838 : INFO : Processed 2600/100000 users (2.6%) - Elapsed: 7976.15s - Est. remaining: 298798.72s\n",
      "2025-04-27 14:08:40,023 : INFO : Processed 2650/100000 users (2.6%) - Elapsed: 8114.33s - Est. remaining: 298086.84s\n",
      "2025-04-27 14:10:57,833 : INFO : Processed 2700/100000 users (2.7%) - Elapsed: 8252.14s - Est. remaining: 297382.73s\n",
      "2025-04-27 14:13:09,438 : INFO : Processed 2750/100000 users (2.8%) - Elapsed: 8383.75s - Est. remaining: 296479.76s\n",
      "2025-04-27 14:15:24,744 : INFO : Processed 2800/100000 users (2.8%) - Elapsed: 8519.05s - Est. remaining: 295732.82s\n",
      "2025-04-27 14:18:29,603 : INFO : Processed 2850/100000 users (2.9%) - Elapsed: 8703.91s - Est. remaining: 296696.48s\n",
      "2025-04-27 14:20:43,143 : INFO : Processed 2900/100000 users (2.9%) - Elapsed: 8837.45s - Est. remaining: 295902.25s\n",
      "2025-04-27 14:23:05,169 : INFO : Processed 2950/100000 users (2.9%) - Elapsed: 8979.48s - Est. remaining: 295409.60s\n",
      "2025-04-27 14:25:15,915 : INFO : Processed 3000/100000 users (3.0%) - Elapsed: 9110.22s - Est. remaining: 294563.88s\n",
      "2025-04-27 14:27:34,410 : INFO : Processed 3050/100000 users (3.0%) - Elapsed: 9248.72s - Est. remaining: 293987.96s\n",
      "2025-04-27 14:29:47,916 : INFO : Processed 3100/100000 users (3.1%) - Elapsed: 9382.22s - Est. remaining: 293270.19s\n",
      "2025-04-27 14:32:04,496 : INFO : Processed 3150/100000 users (3.1%) - Elapsed: 9518.80s - Est. remaining: 292665.48s\n",
      "2025-04-27 14:34:23,352 : INFO : Processed 3200/100000 users (3.2%) - Elapsed: 9657.66s - Est. remaining: 292144.22s\n",
      "2025-04-27 14:36:35,706 : INFO : Processed 3250/100000 users (3.2%) - Elapsed: 9790.01s - Est. remaining: 291441.21s\n",
      "2025-04-27 14:39:38,711 : INFO : Processed 3300/100000 users (3.3%) - Elapsed: 9973.02s - Est. remaining: 292239.70s\n",
      "2025-04-27 14:41:53,299 : INFO : Processed 3350/100000 users (3.4%) - Elapsed: 10107.61s - Est. remaining: 291612.01s\n",
      "2025-04-27 14:44:17,632 : INFO : Processed 3400/100000 users (3.4%) - Elapsed: 10251.94s - Est. remaining: 291275.71s\n",
      "2025-04-27 14:46:35,003 : INFO : Processed 3450/100000 users (3.5%) - Elapsed: 10389.31s - Est. remaining: 290750.15s\n",
      "2025-04-27 14:48:47,254 : INFO : Processed 3500/100000 users (3.5%) - Elapsed: 10521.56s - Est. remaining: 290094.51s\n",
      "2025-04-27 14:50:56,226 : INFO : Processed 3550/100000 users (3.5%) - Elapsed: 10650.53s - Est. remaining: 289364.52s\n",
      "2025-04-27 14:53:04,352 : INFO : Processed 3600/100000 users (3.6%) - Elapsed: 10778.66s - Est. remaining: 288628.57s\n",
      "2025-04-27 14:55:11,063 : INFO : Processed 3650/100000 users (3.6%) - Elapsed: 10905.37s - Est. remaining: 287871.93s\n",
      "2025-04-27 14:57:23,727 : INFO : Processed 3700/100000 users (3.7%) - Elapsed: 11038.04s - Est. remaining: 287287.26s\n",
      "2025-04-27 15:00:29,963 : INFO : Processed 3750/100000 users (3.8%) - Elapsed: 11224.27s - Est. remaining: 288089.62s\n",
      "2025-04-27 15:03:54,310 : INFO : Processed 3800/100000 users (3.8%) - Elapsed: 11428.62s - Est. remaining: 289324.50s\n",
      "2025-04-27 15:06:50,488 : INFO : Processed 3850/100000 users (3.9%) - Elapsed: 11604.80s - Est. remaining: 289818.45s\n",
      "2025-04-27 15:10:03,630 : INFO : Processed 3900/100000 users (3.9%) - Elapsed: 11797.94s - Est. remaining: 290713.30s\n",
      "2025-04-27 15:13:22,831 : INFO : Processed 3950/100000 users (4.0%) - Elapsed: 11997.14s - Est. remaining: 291727.91s\n",
      "2025-04-27 15:16:31,283 : INFO : Processed 4000/100000 users (4.0%) - Elapsed: 12185.59s - Est. remaining: 292454.19s\n",
      "2025-04-27 15:20:06,939 : INFO : Processed 4050/100000 users (4.0%) - Elapsed: 12401.25s - Est. remaining: 293802.39s\n",
      "2025-04-27 15:22:37,866 : INFO : Processed 4100/100000 users (4.1%) - Elapsed: 12552.17s - Est. remaining: 293598.42s\n",
      "2025-04-27 15:25:17,025 : INFO : Processed 4150/100000 users (4.2%) - Elapsed: 12711.33s - Est. remaining: 293585.86s\n",
      "2025-04-27 15:27:43,343 : INFO : Processed 4200/100000 users (4.2%) - Elapsed: 12857.65s - Est. remaining: 293276.92s\n",
      "2025-04-27 15:30:14,765 : INFO : Processed 4250/100000 users (4.2%) - Elapsed: 13009.07s - Est. remaining: 293086.77s\n",
      "2025-04-27 15:33:15,219 : INFO : Processed 4300/100000 users (4.3%) - Elapsed: 13189.53s - Est. remaining: 293543.68s\n",
      "2025-04-27 15:35:50,009 : INFO : Processed 4350/100000 users (4.3%) - Elapsed: 13344.32s - Est. remaining: 293421.60s\n",
      "2025-04-27 15:38:26,703 : INFO : Processed 4400/100000 users (4.4%) - Elapsed: 13501.01s - Est. remaining: 293340.15s\n",
      "2025-04-27 15:40:54,767 : INFO : Processed 4450/100000 users (4.5%) - Elapsed: 13649.08s - Est. remaining: 293071.72s\n",
      "2025-04-27 15:43:30,387 : INFO : Processed 4500/100000 users (4.5%) - Elapsed: 13804.70s - Est. remaining: 292966.32s\n",
      "2025-04-27 15:45:57,611 : INFO : Processed 4550/100000 users (4.5%) - Elapsed: 13951.92s - Est. remaining: 292683.67s\n",
      "2025-04-27 15:48:27,824 : INFO : Processed 4600/100000 users (4.6%) - Elapsed: 14102.13s - Est. remaining: 292465.95s\n",
      "2025-04-27 15:50:34,841 : INFO : Processed 4650/100000 users (4.7%) - Elapsed: 14229.15s - Est. remaining: 291774.06s\n",
      "2025-04-27 15:53:44,912 : INFO : Processed 4700/100000 users (4.7%) - Elapsed: 14419.22s - Est. remaining: 292372.70s\n",
      "2025-04-27 15:56:27,619 : INFO : Processed 4750/100000 users (4.8%) - Elapsed: 14581.93s - Est. remaining: 292406.03s\n",
      "2025-04-27 15:58:54,634 : INFO : Processed 4800/100000 users (4.8%) - Elapsed: 14728.94s - Est. remaining: 292124.02s\n",
      "2025-04-27 16:02:40,183 : INFO : Processed 4850/100000 users (4.9%) - Elapsed: 14954.49s - Est. remaining: 293385.54s\n",
      "2025-04-27 16:05:01,605 : INFO : Processed 4900/100000 users (4.9%) - Elapsed: 15095.91s - Est. remaining: 292983.94s\n",
      "2025-04-27 16:07:34,084 : INFO : Processed 4950/100000 users (5.0%) - Elapsed: 15248.39s - Est. remaining: 292799.93s\n",
      "2025-04-27 16:09:45,261 : INFO : Processed 5000/100000 users (5.0%) - Elapsed: 15379.57s - Est. remaining: 292211.82s\n",
      "2025-04-27 16:12:03,881 : INFO : Processed 5050/100000 users (5.1%) - Elapsed: 15518.19s - Est. remaining: 291772.69s\n",
      "2025-04-27 16:14:23,601 : INFO : Processed 5100/100000 users (5.1%) - Elapsed: 15657.91s - Est. remaining: 291359.93s\n",
      "2025-04-27 16:17:40,946 : INFO : Processed 5150/100000 users (5.1%) - Elapsed: 15855.25s - Est. remaining: 292013.76s\n",
      "2025-04-27 16:20:02,211 : INFO : Processed 5200/100000 users (5.2%) - Elapsed: 15996.52s - Est. remaining: 291628.85s\n",
      "2025-04-27 16:22:27,563 : INFO : Processed 5250/100000 users (5.2%) - Elapsed: 16141.87s - Est. remaining: 291322.34s\n",
      "2025-04-27 16:24:53,416 : INFO : Processed 5300/100000 users (5.3%) - Elapsed: 16287.72s - Est. remaining: 291027.83s\n",
      "2025-04-27 16:27:20,441 : INFO : Processed 5350/100000 users (5.3%) - Elapsed: 16434.75s - Est. remaining: 290756.83s\n",
      "2025-04-27 16:29:59,104 : INFO : Processed 5400/100000 users (5.4%) - Elapsed: 16593.41s - Est. remaining: 290692.00s\n",
      "2025-04-27 16:32:40,282 : INFO : Processed 5450/100000 users (5.5%) - Elapsed: 16754.59s - Est. remaining: 290669.08s\n",
      "2025-04-27 16:35:15,743 : INFO : Processed 5500/100000 users (5.5%) - Elapsed: 16910.05s - Est. remaining: 290545.43s\n",
      "2025-04-27 16:37:51,087 : INFO : Processed 5550/100000 users (5.5%) - Elapsed: 17065.40s - Est. remaining: 290419.21s\n",
      "2025-04-27 16:40:01,388 : INFO : Processed 5600/100000 users (5.6%) - Elapsed: 17195.70s - Est. remaining: 289870.31s\n",
      "2025-04-27 16:42:12,000 : INFO : Processed 5650/100000 users (5.7%) - Elapsed: 17326.31s - Est. remaining: 289334.02s\n",
      "2025-04-27 16:44:20,411 : INFO : Processed 5700/100000 users (5.7%) - Elapsed: 17454.72s - Est. remaining: 288768.42s\n",
      "2025-04-27 16:46:29,870 : INFO : Processed 5750/100000 users (5.8%) - Elapsed: 17584.18s - Est. remaining: 288227.62s\n",
      "2025-04-27 16:48:39,243 : INFO : Processed 5800/100000 users (5.8%) - Elapsed: 17713.55s - Est. remaining: 287692.51s\n",
      "2025-04-27 16:50:47,338 : INFO : Processed 5850/100000 users (5.9%) - Elapsed: 17841.65s - Est. remaining: 287143.76s\n",
      "2025-04-27 16:52:56,736 : INFO : Processed 5900/100000 users (5.9%) - Elapsed: 17971.04s - Est. remaining: 286622.93s\n",
      "2025-04-27 16:55:06,370 : INFO : Processed 5950/100000 users (5.9%) - Elapsed: 18100.68s - Est. remaining: 286112.40s\n",
      "2025-04-27 16:57:14,873 : INFO : Processed 6000/100000 users (6.0%) - Elapsed: 18229.18s - Est. remaining: 285590.51s\n",
      "2025-04-27 16:59:24,893 : INFO : Processed 6050/100000 users (6.0%) - Elapsed: 18359.20s - Est. remaining: 285098.67s\n",
      "2025-04-27 17:01:33,171 : INFO : Processed 6100/100000 users (6.1%) - Elapsed: 18487.48s - Est. remaining: 284585.96s\n",
      "2025-04-27 17:03:42,252 : INFO : Processed 6150/100000 users (6.2%) - Elapsed: 18616.56s - Est. remaining: 284091.74s\n",
      "2025-04-27 17:05:52,717 : INFO : Processed 6200/100000 users (6.2%) - Elapsed: 18747.03s - Est. remaining: 283624.35s\n",
      "2025-04-27 17:08:03,385 : INFO : Processed 6250/100000 users (6.2%) - Elapsed: 18877.69s - Est. remaining: 283165.40s\n",
      "2025-04-27 17:10:13,387 : INFO : Processed 6300/100000 users (6.3%) - Elapsed: 19007.70s - Est. remaining: 282701.76s\n",
      "2025-04-27 17:12:22,683 : INFO : Processed 6350/100000 users (6.3%) - Elapsed: 19136.99s - Est. remaining: 282232.96s\n",
      "2025-04-27 17:14:31,979 : INFO : Processed 6400/100000 users (6.4%) - Elapsed: 19266.29s - Est. remaining: 281769.46s\n",
      "2025-04-27 17:16:40,666 : INFO : Processed 6450/100000 users (6.5%) - Elapsed: 19394.97s - Est. remaining: 281302.30s\n",
      "2025-04-27 17:18:50,787 : INFO : Processed 6500/100000 users (6.5%) - Elapsed: 19525.10s - Est. remaining: 280860.99s\n",
      "2025-04-27 17:21:00,322 : INFO : Processed 6550/100000 users (6.6%) - Elapsed: 19654.63s - Est. remaining: 280416.06s\n",
      "2025-04-27 17:23:09,456 : INFO : Processed 6600/100000 users (6.6%) - Elapsed: 19783.76s - Est. remaining: 279970.24s\n",
      "2025-04-27 17:25:20,804 : INFO : Processed 6650/100000 users (6.7%) - Elapsed: 19915.11s - Est. remaining: 279560.26s\n",
      "2025-04-27 17:27:31,065 : INFO : Processed 6700/100000 users (6.7%) - Elapsed: 20045.37s - Est. remaining: 279139.31s\n",
      "2025-04-27 17:29:39,482 : INFO : Processed 6750/100000 users (6.8%) - Elapsed: 20173.79s - Est. remaining: 278697.18s\n",
      "2025-04-27 17:31:49,316 : INFO : Processed 6800/100000 users (6.8%) - Elapsed: 20303.62s - Est. remaining: 278279.07s\n",
      "2025-04-27 17:33:59,098 : INFO : Processed 6850/100000 users (6.9%) - Elapsed: 20433.41s - Est. remaining: 277864.50s\n",
      "2025-04-27 17:36:08,788 : INFO : Processed 6900/100000 users (6.9%) - Elapsed: 20563.10s - Est. remaining: 277452.80s\n",
      "2025-04-27 17:38:18,790 : INFO : Processed 6950/100000 users (7.0%) - Elapsed: 20693.10s - Est. remaining: 277049.33s\n",
      "2025-04-27 17:40:28,974 : INFO : Processed 7000/100000 users (7.0%) - Elapsed: 20823.28s - Est. remaining: 276652.19s\n",
      "2025-04-27 17:42:38,010 : INFO : Processed 7050/100000 users (7.0%) - Elapsed: 20952.32s - Est. remaining: 276243.69s\n",
      "2025-04-27 17:44:47,178 : INFO : Processed 7100/100000 users (7.1%) - Elapsed: 21081.49s - Est. remaining: 275840.85s\n",
      "2025-04-27 17:47:03,515 : INFO : Processed 7150/100000 users (7.1%) - Elapsed: 21217.82s - Est. remaining: 275534.95s\n",
      "2025-04-27 17:50:23,706 : INFO : Processed 7200/100000 users (7.2%) - Elapsed: 21418.01s - Est. remaining: 276054.41s\n",
      "2025-04-27 17:53:51,898 : INFO : Processed 7250/100000 users (7.2%) - Elapsed: 21626.21s - Est. remaining: 276666.30s\n",
      "2025-04-27 17:56:53,902 : INFO : Processed 7300/100000 users (7.3%) - Elapsed: 21808.21s - Est. remaining: 276934.40s\n",
      "2025-04-27 17:59:36,478 : INFO : Processed 7350/100000 users (7.3%) - Elapsed: 21970.79s - Est. remaining: 276951.48s\n",
      "2025-04-27 18:02:24,009 : INFO : Processed 7400/100000 users (7.4%) - Elapsed: 22138.32s - Est. remaining: 277028.13s\n",
      "2025-04-27 18:05:35,586 : INFO : Processed 7450/100000 users (7.4%) - Elapsed: 22329.89s - Est. remaining: 277400.24s\n",
      "2025-04-27 18:08:46,642 : INFO : Processed 7500/100000 users (7.5%) - Elapsed: 22520.95s - Est. remaining: 277758.39s\n",
      "2025-04-27 18:11:59,293 : INFO : Processed 7550/100000 users (7.5%) - Elapsed: 22713.60s - Est. remaining: 278128.80s\n",
      "2025-04-27 18:15:09,062 : INFO : Processed 7600/100000 users (7.6%) - Elapsed: 22903.37s - Est. remaining: 278456.77s\n",
      "2025-04-27 18:17:54,286 : INFO : Processed 7650/100000 users (7.6%) - Elapsed: 23068.59s - Est. remaining: 278481.66s\n",
      "2025-04-27 18:20:40,916 : INFO : Processed 7700/100000 users (7.7%) - Elapsed: 23235.22s - Est. remaining: 278520.93s\n",
      "2025-04-27 18:23:34,361 : INFO : Processed 7750/100000 users (7.8%) - Elapsed: 23408.67s - Est. remaining: 278638.68s\n",
      "2025-04-27 18:26:07,531 : INFO : Processed 7800/100000 users (7.8%) - Elapsed: 23561.84s - Est. remaining: 278513.03s\n",
      "2025-04-27 18:28:46,334 : INFO : Processed 7850/100000 users (7.8%) - Elapsed: 23720.64s - Est. remaining: 278453.15s\n",
      "2025-04-27 18:31:20,044 : INFO : Processed 7900/100000 users (7.9%) - Elapsed: 23874.35s - Est. remaining: 278332.64s\n",
      "2025-04-27 18:33:38,575 : INFO : Processed 7950/100000 users (8.0%) - Elapsed: 24012.88s - Est. remaining: 278035.96s\n",
      "2025-04-27 18:36:08,198 : INFO : Processed 8000/100000 users (8.0%) - Elapsed: 24162.51s - Est. remaining: 277868.82s\n",
      "2025-04-27 18:38:39,839 : INFO : Processed 8050/100000 users (8.1%) - Elapsed: 24314.15s - Est. remaining: 277724.95s\n",
      "2025-04-27 18:41:14,934 : INFO : Processed 8100/100000 users (8.1%) - Elapsed: 24469.24s - Est. remaining: 277620.17s\n",
      "2025-04-27 18:43:43,964 : INFO : Processed 8150/100000 users (8.2%) - Elapsed: 24618.27s - Est. remaining: 277446.42s\n",
      "2025-04-27 18:46:16,208 : INFO : Processed 8200/100000 users (8.2%) - Elapsed: 24770.52s - Est. remaining: 277308.95s\n",
      "2025-04-27 18:48:46,878 : INFO : Processed 8250/100000 users (8.2%) - Elapsed: 24921.19s - Est. remaining: 277153.80s\n",
      "2025-04-27 18:51:18,517 : INFO : Processed 8300/100000 users (8.3%) - Elapsed: 25072.83s - Est. remaining: 277009.41s\n",
      "2025-04-27 18:53:56,785 : INFO : Processed 8350/100000 users (8.3%) - Elapsed: 25231.09s - Est. remaining: 276937.69s\n",
      "2025-04-27 18:56:34,092 : INFO : Processed 8400/100000 users (8.4%) - Elapsed: 25388.40s - Est. remaining: 276854.46s\n",
      "2025-04-27 18:58:54,252 : INFO : Processed 8450/100000 users (8.5%) - Elapsed: 25528.56s - Est. remaining: 276584.58s\n",
      "2025-04-27 19:01:21,387 : INFO : Processed 8500/100000 users (8.5%) - Elapsed: 25675.70s - Est. remaining: 276391.31s\n",
      "2025-04-27 19:03:53,173 : INFO : Processed 8550/100000 users (8.6%) - Elapsed: 25827.48s - Est. remaining: 276248.32s\n",
      "2025-04-27 19:06:24,502 : INFO : Processed 8600/100000 users (8.6%) - Elapsed: 25978.81s - Est. remaining: 276100.38s\n",
      "2025-04-27 19:08:58,429 : INFO : Processed 8650/100000 users (8.6%) - Elapsed: 26132.74s - Est. remaining: 275979.84s\n",
      "2025-04-27 19:11:33,970 : INFO : Processed 8700/100000 users (8.7%) - Elapsed: 26288.28s - Est. remaining: 275875.84s\n",
      "2025-04-27 19:14:06,556 : INFO : Processed 8750/100000 users (8.8%) - Elapsed: 26440.86s - Est. remaining: 275740.45s\n",
      "2025-04-27 19:16:48,370 : INFO : Processed 8800/100000 users (8.8%) - Elapsed: 26602.68s - Est. remaining: 275700.48s\n",
      "2025-04-27 19:19:23,292 : INFO : Processed 8850/100000 users (8.8%) - Elapsed: 26757.60s - Est. remaining: 275588.17s\n",
      "2025-04-27 19:21:54,423 : INFO : Processed 8900/100000 users (8.9%) - Elapsed: 26908.73s - Est. remaining: 275436.57s\n",
      "2025-04-27 19:24:32,274 : INFO : Processed 8950/100000 users (8.9%) - Elapsed: 27066.58s - Est. remaining: 275353.33s\n",
      "2025-04-27 19:27:08,141 : INFO : Processed 9000/100000 users (9.0%) - Elapsed: 27222.45s - Est. remaining: 275249.22s\n",
      "2025-04-27 19:29:48,044 : INFO : Processed 9050/100000 users (9.0%) - Elapsed: 27382.35s - Est. remaining: 275185.08s\n",
      "2025-04-27 19:33:48,908 : INFO : Processed 9100/100000 users (9.1%) - Elapsed: 27623.22s - Est. remaining: 275928.61s\n",
      "2025-04-27 19:36:37,420 : INFO : Processed 9150/100000 users (9.2%) - Elapsed: 27791.73s - Est. remaining: 275943.00s\n",
      "2025-04-27 19:39:12,451 : INFO : Processed 9200/100000 users (9.2%) - Elapsed: 27946.76s - Est. remaining: 275822.37s\n",
      "2025-04-27 19:42:38,492 : INFO : Processed 9250/100000 users (9.2%) - Elapsed: 28152.80s - Est. remaining: 276201.80s\n",
      "2025-04-27 19:45:41,017 : INFO : Processed 9300/100000 users (9.3%) - Elapsed: 28335.33s - Est. remaining: 276345.59s\n",
      "2025-04-27 19:48:54,393 : INFO : Processed 9350/100000 users (9.3%) - Elapsed: 28528.70s - Est. remaining: 276591.09s\n",
      "2025-04-27 19:52:29,543 : INFO : Processed 9400/100000 users (9.4%) - Elapsed: 28743.85s - Est. remaining: 277041.80s\n",
      "2025-04-27 19:55:04,626 : INFO : Processed 9450/100000 users (9.4%) - Elapsed: 28898.93s - Est. remaining: 276909.90s\n",
      "2025-04-27 19:57:49,010 : INFO : Processed 9500/100000 users (9.5%) - Elapsed: 29063.32s - Est. remaining: 276866.35s\n",
      "2025-04-27 20:00:55,132 : INFO : Processed 9550/100000 users (9.6%) - Elapsed: 29249.44s - Est. remaining: 277027.42s\n",
      "2025-04-27 20:04:00,301 : INFO : Processed 9600/100000 users (9.6%) - Elapsed: 29434.61s - Est. remaining: 277175.91s\n",
      "2025-04-27 20:06:48,771 : INFO : Processed 9650/100000 users (9.7%) - Elapsed: 29603.08s - Est. remaining: 277164.58s\n",
      "2025-04-27 20:09:27,893 : INFO : Processed 9700/100000 users (9.7%) - Elapsed: 29762.20s - Est. remaining: 277064.62s\n",
      "2025-04-27 20:12:19,292 : INFO : Processed 9750/100000 users (9.8%) - Elapsed: 29933.60s - Est. remaining: 277077.68s\n",
      "2025-04-27 20:14:40,751 : INFO : Processed 9800/100000 users (9.8%) - Elapsed: 30075.06s - Est. remaining: 276813.30s\n",
      "2025-04-27 20:17:04,536 : INFO : Processed 9850/100000 users (9.8%) - Elapsed: 30218.84s - Est. remaining: 276571.45s\n",
      "2025-04-27 20:19:45,298 : INFO : Processed 9900/100000 users (9.9%) - Elapsed: 30379.61s - Est. remaining: 276485.10s\n",
      "2025-04-27 20:22:21,293 : INFO : Processed 9950/100000 users (10.0%) - Elapsed: 30535.60s - Est. remaining: 276354.86s\n",
      "2025-04-27 20:24:53,664 : INFO : Processed 10000/100000 users (10.0%) - Elapsed: 30687.97s - Est. remaining: 276191.75s\n",
      "2025-04-27 20:27:21,908 : INFO : Processed 10050/100000 users (10.1%) - Elapsed: 30836.22s - Est. remaining: 275991.81s\n",
      "2025-04-27 20:30:49,125 : INFO : Processed 10100/100000 users (10.1%) - Elapsed: 31043.43s - Est. remaining: 276317.29s\n",
      "2025-04-27 20:33:29,631 : INFO : Processed 10150/100000 users (10.2%) - Elapsed: 31203.94s - Est. remaining: 276224.03s\n",
      "2025-04-27 20:36:18,108 : INFO : Processed 10200/100000 users (10.2%) - Elapsed: 31372.42s - Est. remaining: 276200.29s\n",
      "2025-04-27 20:38:45,334 : INFO : Processed 10250/100000 users (10.2%) - Elapsed: 31519.64s - Est. remaining: 275989.06s\n",
      "2025-04-27 20:41:31,722 : INFO : Processed 10300/100000 users (10.3%) - Elapsed: 31686.03s - Est. remaining: 275945.34s\n",
      "2025-04-27 20:44:31,732 : INFO : Processed 10350/100000 users (10.3%) - Elapsed: 31866.04s - Est. remaining: 276018.41s\n",
      "2025-04-27 20:48:08,835 : INFO : Processed 10400/100000 users (10.4%) - Elapsed: 32083.14s - Est. remaining: 276408.61s\n",
      "2025-04-27 20:51:20,783 : INFO : Processed 10450/100000 users (10.4%) - Elapsed: 32275.09s - Est. remaining: 276577.45s\n",
      "2025-04-27 20:54:45,186 : INFO : Processed 10500/100000 users (10.5%) - Elapsed: 32479.49s - Est. remaining: 276849.02s\n",
      "2025-04-27 20:57:35,040 : INFO : Processed 10550/100000 users (10.5%) - Elapsed: 32649.35s - Est. remaining: 276823.15s\n",
      "2025-04-27 21:00:22,336 : INFO : Processed 10600/100000 users (10.6%) - Elapsed: 32816.64s - Est. remaining: 276774.34s\n",
      "2025-04-27 21:03:24,225 : INFO : Processed 10650/100000 users (10.7%) - Elapsed: 32998.53s - Est. remaining: 276846.85s\n",
      "2025-04-27 21:06:37,116 : INFO : Processed 10700/100000 users (10.7%) - Elapsed: 33191.42s - Est. remaining: 277008.80s\n",
      "2025-04-27 21:09:52,446 : INFO : Processed 10750/100000 users (10.8%) - Elapsed: 33386.75s - Est. remaining: 277187.71s\n",
      "2025-04-27 21:12:48,877 : INFO : Processed 10800/100000 users (10.8%) - Elapsed: 33563.19s - Est. remaining: 277207.05s\n",
      "2025-04-27 21:15:46,834 : INFO : Processed 10850/100000 users (10.8%) - Elapsed: 33741.14s - Est. remaining: 277237.13s\n",
      "2025-04-27 21:18:39,386 : INFO : Processed 10900/100000 users (10.9%) - Elapsed: 33913.69s - Est. remaining: 277221.12s\n",
      "2025-04-27 21:21:33,672 : INFO : Processed 10950/100000 users (10.9%) - Elapsed: 34087.98s - Est. remaining: 277217.78s\n",
      "2025-04-27 21:24:23,169 : INFO : Processed 11000/100000 users (11.0%) - Elapsed: 34257.48s - Est. remaining: 277174.13s\n",
      "2025-04-27 21:27:07,553 : INFO : Processed 11050/100000 users (11.1%) - Elapsed: 34421.86s - Est. remaining: 277088.20s\n",
      "2025-04-27 21:29:46,289 : INFO : Processed 11100/100000 users (11.1%) - Elapsed: 34580.60s - Est. remaining: 276956.32s\n",
      "2025-04-27 21:32:07,161 : INFO : Processed 11150/100000 users (11.2%) - Elapsed: 34721.47s - Est. remaining: 276681.84s\n",
      "2025-04-27 21:34:16,774 : INFO : Processed 11200/100000 users (11.2%) - Elapsed: 34851.08s - Est. remaining: 276319.30s\n",
      "2025-04-27 21:36:41,752 : INFO : Processed 11250/100000 users (11.2%) - Elapsed: 34996.06s - Est. remaining: 276080.03s\n",
      "2025-04-27 21:38:52,284 : INFO : Processed 11300/100000 users (11.3%) - Elapsed: 35126.59s - Est. remaining: 275728.21s\n",
      "2025-04-27 21:41:00,651 : INFO : Processed 11350/100000 users (11.3%) - Elapsed: 35254.96s - Est. remaining: 275361.42s\n",
      "2025-04-27 21:43:07,526 : INFO : Processed 11400/100000 users (11.4%) - Elapsed: 35381.83s - Est. remaining: 274985.14s\n",
      "2025-04-27 21:45:20,968 : INFO : Processed 11450/100000 users (11.5%) - Elapsed: 35515.28s - Est. remaining: 274661.81s\n",
      "2025-04-27 21:47:25,073 : INFO : Processed 11500/100000 users (11.5%) - Elapsed: 35639.38s - Est. remaining: 274268.28s\n",
      "2025-04-27 21:49:37,707 : INFO : Processed 11550/100000 users (11.6%) - Elapsed: 35772.02s - Est. remaining: 273942.40s\n",
      "2025-04-27 21:51:54,998 : INFO : Processed 11600/100000 users (11.6%) - Elapsed: 35909.31s - Est. remaining: 273653.68s\n",
      "2025-04-27 21:54:09,885 : INFO : Processed 11650/100000 users (11.7%) - Elapsed: 36044.19s - Est. remaining: 273348.03s\n",
      "2025-04-27 21:56:24,622 : INFO : Processed 11700/100000 users (11.7%) - Elapsed: 36178.93s - Est. remaining: 273042.70s\n",
      "2025-04-27 21:58:39,646 : INFO : Processed 11750/100000 users (11.8%) - Elapsed: 36313.95s - Est. remaining: 272740.98s\n",
      "2025-04-27 22:00:55,031 : INFO : Processed 11800/100000 users (11.8%) - Elapsed: 36449.34s - Est. remaining: 272443.37s\n",
      "2025-04-27 22:03:09,333 : INFO : Processed 11850/100000 users (11.8%) - Elapsed: 36583.64s - Est. remaining: 272139.07s\n",
      "2025-04-27 22:05:24,689 : INFO : Processed 11900/100000 users (11.9%) - Elapsed: 36719.00s - Est. remaining: 271844.00s\n",
      "2025-04-27 22:07:49,136 : INFO : Processed 11950/100000 users (11.9%) - Elapsed: 36863.44s - Est. remaining: 271617.26s\n",
      "2025-04-27 22:09:53,816 : INFO : Processed 12000/100000 users (12.0%) - Elapsed: 36988.12s - Est. remaining: 271246.25s\n",
      "2025-04-27 22:11:57,000 : INFO : Processed 12050/100000 users (12.0%) - Elapsed: 37111.31s - Est. remaining: 270866.36s\n",
      "2025-04-27 22:14:01,338 : INFO : Processed 12100/100000 users (12.1%) - Elapsed: 37235.65s - Est. remaining: 270496.97s\n",
      "2025-04-27 22:16:14,529 : INFO : Processed 12150/100000 users (12.2%) - Elapsed: 37368.84s - Est. remaining: 270193.61s\n",
      "2025-04-27 22:18:25,756 : INFO : Processed 12200/100000 users (12.2%) - Elapsed: 37500.06s - Est. remaining: 269877.52s\n",
      "2025-04-27 22:20:30,799 : INFO : Processed 12250/100000 users (12.2%) - Elapsed: 37625.11s - Est. remaining: 269518.63s\n",
      "2025-04-27 22:22:35,042 : INFO : Processed 12300/100000 users (12.3%) - Elapsed: 37749.35s - Est. remaining: 269155.94s\n",
      "2025-04-27 22:24:38,928 : INFO : Processed 12350/100000 users (12.3%) - Elapsed: 37873.24s - Est. remaining: 268792.64s\n",
      "2025-04-27 22:26:42,281 : INFO : Processed 12400/100000 users (12.4%) - Elapsed: 37996.59s - Est. remaining: 268427.52s\n",
      "2025-04-27 22:28:45,032 : INFO : Processed 12450/100000 users (12.4%) - Elapsed: 38119.34s - Est. remaining: 268060.10s\n",
      "2025-04-27 22:30:49,560 : INFO : Processed 12500/100000 users (12.5%) - Elapsed: 38243.87s - Est. remaining: 267707.08s\n",
      "2025-04-27 22:32:54,117 : INFO : Processed 12550/100000 users (12.6%) - Elapsed: 38368.43s - Est. remaining: 267356.08s\n",
      "2025-04-27 22:34:59,699 : INFO : Processed 12600/100000 users (12.6%) - Elapsed: 38494.01s - Est. remaining: 267013.99s\n",
      "2025-04-27 22:37:16,576 : INFO : Processed 12650/100000 users (12.7%) - Elapsed: 38630.88s - Est. remaining: 266751.60s\n",
      "2025-04-27 22:39:31,069 : INFO : Processed 12700/100000 users (12.7%) - Elapsed: 38765.38s - Est. remaining: 266473.81s\n",
      "2025-04-27 22:41:42,895 : INFO : Processed 12750/100000 users (12.8%) - Elapsed: 38897.20s - Est. remaining: 266178.90s\n",
      "2025-04-27 22:43:49,896 : INFO : Processed 12800/100000 users (12.8%) - Elapsed: 39024.20s - Est. remaining: 265852.40s\n",
      "2025-04-27 22:45:57,274 : INFO : Processed 12850/100000 users (12.8%) - Elapsed: 39151.58s - Est. remaining: 265530.00s\n",
      "2025-04-27 22:48:04,657 : INFO : Processed 12900/100000 users (12.9%) - Elapsed: 39278.97s - Est. remaining: 265209.14s\n",
      "2025-04-27 22:50:35,323 : INFO : Processed 12950/100000 users (13.0%) - Elapsed: 39429.63s - Est. remaining: 265046.29s\n",
      "2025-04-27 22:52:47,755 : INFO : Processed 13000/100000 users (13.0%) - Elapsed: 39562.06s - Est. remaining: 264761.50s\n",
      "2025-04-27 22:54:59,153 : INFO : Processed 13050/100000 users (13.1%) - Elapsed: 39693.46s - Est. remaining: 264471.00s\n",
      "2025-04-27 22:57:18,390 : INFO : Processed 13100/100000 users (13.1%) - Elapsed: 39832.70s - Est. remaining: 264233.70s\n",
      "2025-04-27 22:59:34,744 : INFO : Processed 13150/100000 users (13.2%) - Elapsed: 39969.05s - Est. remaining: 263978.12s\n",
      "2025-04-27 23:01:44,872 : INFO : Processed 13200/100000 users (13.2%) - Elapsed: 40099.18s - Est. remaining: 263682.49s\n",
      "2025-04-27 23:04:03,836 : INFO : Processed 13250/100000 users (13.2%) - Elapsed: 40238.14s - Est. remaining: 263445.97s\n",
      "2025-04-27 23:06:16,675 : INFO : Processed 13300/100000 users (13.3%) - Elapsed: 40370.98s - Est. remaining: 263170.25s\n",
      "2025-04-27 23:08:30,756 : INFO : Processed 13350/100000 users (13.4%) - Elapsed: 40505.06s - Est. remaining: 262903.66s\n",
      "2025-04-27 23:10:49,325 : INFO : Processed 13400/100000 users (13.4%) - Elapsed: 40643.63s - Est. remaining: 262667.07s\n",
      "2025-04-27 23:13:04,024 : INFO : Processed 13450/100000 users (13.5%) - Elapsed: 40778.33s - Est. remaining: 262406.29s\n",
      "2025-04-27 23:15:12,383 : INFO : Processed 13500/100000 users (13.5%) - Elapsed: 40906.69s - Est. remaining: 262105.83s\n",
      "2025-04-27 23:17:33,949 : INFO : Processed 13550/100000 users (13.6%) - Elapsed: 41048.26s - Est. remaining: 261890.91s\n",
      "2025-04-27 23:20:04,059 : INFO : Processed 13600/100000 users (13.6%) - Elapsed: 41198.37s - Est. remaining: 261730.80s\n",
      "2025-04-27 23:22:29,748 : INFO : Processed 13650/100000 users (13.7%) - Elapsed: 41344.06s - Est. remaining: 261542.81s\n",
      "2025-04-27 23:24:59,218 : INFO : Processed 13700/100000 users (13.7%) - Elapsed: 41493.53s - Est. remaining: 261378.93s\n",
      "2025-04-27 23:27:24,058 : INFO : Processed 13750/100000 users (13.8%) - Elapsed: 41638.37s - Est. remaining: 261186.11s\n",
      "2025-04-27 23:29:44,868 : INFO : Processed 13800/100000 users (13.8%) - Elapsed: 41779.18s - Est. remaining: 260968.48s\n",
      "2025-04-27 23:32:13,930 : INFO : Processed 13850/100000 users (13.9%) - Elapsed: 41928.24s - Est. remaining: 260802.73s\n",
      "2025-04-27 23:34:39,788 : INFO : Processed 13900/100000 users (13.9%) - Elapsed: 42074.10s - Est. remaining: 260617.24s\n",
      "2025-04-27 23:37:14,777 : INFO : Processed 13950/100000 users (14.0%) - Elapsed: 42229.09s - Est. remaining: 260488.37s\n",
      "2025-04-27 23:39:51,880 : INFO : Processed 14000/100000 users (14.0%) - Elapsed: 42386.19s - Est. remaining: 260372.30s\n",
      "2025-04-27 23:42:49,278 : INFO : Processed 14050/100000 users (14.1%) - Elapsed: 42563.59s - Est. remaining: 260380.09s\n",
      "2025-04-27 23:45:44,983 : INFO : Processed 14100/100000 users (14.1%) - Elapsed: 42739.29s - Est. remaining: 260376.25s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Concatenate, Embedding, Flatten, Add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import time\n",
    "import gc\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "input_path = \"./processed/\"\n",
    "output_path = \"./rec/collaborative-recommendations\"\n",
    "top_n = 20\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Optimized hyperparameters based on extensive testing\n",
    "dnn_hidden_layers = [256, 128, 64, 32]  # Deeper network\n",
    "dnn_dropout_rate = 0.35  # Increased dropout for better generalization\n",
    "dnn_l2_reg = 0.0005  # L2 regularization to prevent overfitting\n",
    "dnn_learning_rate = 0.001  # Lower learning rate for more stable convergence\n",
    "dnn_batch_size = 256  # Larger batch size for better gradient estimates\n",
    "dnn_epochs = 30 # More epochs with early stopping\n",
    "threshold_rating = 3  # Rating threshold for binary classification\n",
    "early_stopping_patience = 8  # Wait longer for improvement\n",
    "use_cosine_annealing = True  # Use cosine annealing learning rate schedule\n",
    "use_attention_mechanism = True  # Use attention mechanism for feature interaction\n",
    "\n",
    "def log_memory_usage(message=\"Current memory usage\"):\n",
    "    \"\"\"Log current memory usage\"\"\"\n",
    "    try:\n",
    "        import psutil\n",
    "        process = psutil.Process(os.getpid())\n",
    "        memory_info = process.memory_info()\n",
    "        memory_usage_mb = memory_info.rss / 1024 / 1024\n",
    "        logger.info(f\"{message}: {memory_usage_mb:.2f} MB\")\n",
    "    except ImportError:\n",
    "        logger.warning(\"psutil not available for memory monitoring\")\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load and prepare data for model training with improved memory management\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    log_memory_usage(\"Before loading data\")\n",
    "    \n",
    "    movie_features_path = os.path.join(input_path, 'processed_movie_features.csv')\n",
    "    if os.path.exists(movie_features_path):\n",
    "        # Use optimized loading for large CSV files with appropriate dtypes\n",
    "        data['movie_features'] = pd.read_csv(movie_features_path, \n",
    "                                            dtype={'movieId': int, 'token_count': int})\n",
    "        logger.info(f\"Loaded movie features with shape {data['movie_features'].shape}\")\n",
    "    else:\n",
    "        logger.error(f\"File not found: {movie_features_path}\")\n",
    "        return None\n",
    "    \n",
    "    ratings_path = os.path.join(input_path, 'normalized_ratings.csv')\n",
    "    if os.path.exists(ratings_path):\n",
    "        # Load ratings in chunks to manage memory better\n",
    "        chunk_size = 100000\n",
    "        chunks = []\n",
    "        for chunk in pd.read_csv(ratings_path, chunksize=chunk_size):\n",
    "            chunks.append(chunk)\n",
    "            # Force garbage collection after each chunk\n",
    "            gc.collect()\n",
    "        \n",
    "        data['ratings'] = pd.concat(chunks)\n",
    "        logger.info(f\"Loaded ratings with shape {data['ratings'].shape}\")\n",
    "    else:\n",
    "        logger.error(f\"File not found: {ratings_path}\")\n",
    "        return None\n",
    "    \n",
    "    log_memory_usage(\"After loading data\")\n",
    "    \n",
    "    if 'ratings' in data:\n",
    "        # Create improved train/test split\n",
    "        logger.info(\"Creating improved train/test split\")\n",
    "        \n",
    "        # Set seed for reproducibility\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # First split: separate users for training and testing\n",
    "        all_user_ids = data['ratings']['userId'].unique()\n",
    "        np.random.shuffle(all_user_ids)\n",
    "        \n",
    "        # Use 80% of users for complete training\n",
    "        split_idx = int(len(all_user_ids) * 0.8)\n",
    "        train_users = all_user_ids[:split_idx]\n",
    "        test_users = all_user_ids[split_idx:]\n",
    "        \n",
    "        # Full training data from train users\n",
    "        train_ratings_main = data['ratings'][data['ratings']['userId'].isin(train_users)]\n",
    "        \n",
    "        # For users in test set, split their ratings temporally\n",
    "        test_user_ratings = data['ratings'][data['ratings']['userId'].isin(test_users)]\n",
    "        train_chunks = []\n",
    "        test_chunks = []\n",
    "        \n",
    "        # Process each test user\n",
    "        for user_id in test_users:\n",
    "            user_data = test_user_ratings[test_user_ratings['userId'] == user_id]\n",
    "            \n",
    "            # Skip users with too few ratings\n",
    "            if len(user_data) < 5:\n",
    "                continue\n",
    "                \n",
    "            # Sort by timestamp if available\n",
    "            if 'timestamp' in user_data.columns:\n",
    "                user_data = user_data.sort_values('timestamp')\n",
    "            \n",
    "            # Take first 80% for training, last 20% for testing (temporal split)\n",
    "            split_idx = int(len(user_data) * 0.8)\n",
    "            train_chunks.append(user_data.iloc[:split_idx])\n",
    "            test_chunks.append(user_data.iloc[split_idx:])\n",
    "        \n",
    "        # Combine training data from both sources\n",
    "        data['train_ratings'] = pd.concat([train_ratings_main] + train_chunks) if train_chunks else train_ratings_main\n",
    "        data['test_ratings'] = pd.concat(test_chunks) if test_chunks else pd.DataFrame()\n",
    "        \n",
    "        # Log split statistics\n",
    "        logger.info(f\"Training set: {len(data['train_ratings'])} ratings from {len(data['train_ratings']['userId'].unique())} users\")\n",
    "        logger.info(f\"Test set: {len(data['test_ratings'])} ratings from {len(data['test_ratings']['userId'].unique())} users\")\n",
    "        \n",
    "        # Force garbage collection\n",
    "        del train_ratings_main, test_user_ratings, train_chunks, test_chunks\n",
    "        gc.collect()\n",
    "    \n",
    "    # Extract region columns with better handling\n",
    "    region_columns = [col for col in data['movie_features'].columns \n",
    "                     if col in ['North America', 'Europe', 'East Asia', 'South Asia', \n",
    "                               'Southeast Asia', 'Oceania', 'Middle East', 'Africa', \n",
    "                               'Latin America', 'Other']]\n",
    "    \n",
    "    if region_columns:\n",
    "        logger.info(f\"Found {len(region_columns)} region features: {region_columns}\")\n",
    "        data['region_columns'] = region_columns\n",
    "    \n",
    "    log_memory_usage(\"After split preparation\")\n",
    "    return data\n",
    "\n",
    "def extract_genre_and_region_features(movie_features):\n",
    "    \"\"\"\n",
    "    Extract enhanced genre and region features from movie data\n",
    "    \"\"\"\n",
    "    # Better identification of genre columns\n",
    "    genre_columns = [col for col in movie_features.columns if col not in \n",
    "                     ['movieId', 'title', 'tokens', 'token_count', 'top_keywords'] and\n",
    "                     col not in ['North America', 'Europe', 'East Asia', 'South Asia', \n",
    "                                'Southeast Asia', 'Oceania', 'Middle East', 'Africa', \n",
    "                                'Latin America', 'Other']]\n",
    "    \n",
    "    region_columns = [col for col in movie_features.columns \n",
    "                     if col in ['North America', 'Europe', 'East Asia', 'South Asia', \n",
    "                               'Southeast Asia', 'Oceania', 'Middle East', 'Africa', \n",
    "                               'Latin America', 'Other']]\n",
    "    \n",
    "    if not genre_columns:\n",
    "        logger.error(\"No genre columns found in movie features\")\n",
    "        return None\n",
    "    \n",
    "    # Extract features\n",
    "    movie_genre_features = movie_features[['movieId'] + genre_columns].copy()\n",
    "    \n",
    "    # Add region features if available\n",
    "    if region_columns:\n",
    "        movie_region_features = movie_features[['movieId'] + region_columns].copy()\n",
    "        # Combine with genre features\n",
    "        movie_features_combined = pd.merge(\n",
    "            movie_genre_features,\n",
    "            movie_region_features,\n",
    "            on='movieId',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Fill NaN values with 0\n",
    "        for col in region_columns:\n",
    "            if col in movie_features_combined.columns:\n",
    "                movie_features_combined[col] = movie_features_combined[col].fillna(0).astype(int)\n",
    "                \n",
    "        logger.info(f\"Extracted {len(genre_columns)} genre features and {len(region_columns)} region features\")\n",
    "        return movie_features_combined, genre_columns, region_columns\n",
    "    \n",
    "    logger.info(f\"Extracted {len(genre_columns)} genre features (no region features found)\")\n",
    "    return movie_genre_features, genre_columns, []\n",
    "\n",
    "def calculate_user_preferences(train_ratings, movie_features, feature_columns, rating_threshold=3):\n",
    "    \"\"\"\n",
    "    Calculate enhanced user preferences with improved weighting scheme\n",
    "    \"\"\"\n",
    "    logger.info(f\"Calculating user preferences for {len(feature_columns)} features\")\n",
    "    \n",
    "    user_preferences = []\n",
    "    \n",
    "    total_users = len(train_ratings['userId'].unique())\n",
    "    processed_users = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Calculate global average rating\n",
    "    global_avg_rating = train_ratings['rating'].mean()\n",
    "    \n",
    "    # Process users in batches to manage memory\n",
    "    user_batch_size = 1000\n",
    "    user_batches = np.array_split(train_ratings['userId'].unique(), \n",
    "                                max(1, total_users // user_batch_size))\n",
    "    \n",
    "    for batch_idx, user_batch in enumerate(user_batches):\n",
    "        batch_preferences = []\n",
    "        \n",
    "        for user_id in user_batch:\n",
    "            user_ratings = train_ratings[train_ratings['userId'] == user_id]\n",
    "            \n",
    "            if len(user_ratings) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate user's average rating and bias\n",
    "            user_avg_rating = user_ratings['rating'].mean()\n",
    "            user_bias = user_avg_rating - global_avg_rating\n",
    "            \n",
    "            # Split into liked and disliked movies with more granular approach\n",
    "            # Use original unbiased ratings\n",
    "            strongly_liked_movies = user_ratings[user_ratings['rating'] >= rating_threshold + 0.5]['movieId'].values\n",
    "            liked_movies = user_ratings[(user_ratings['rating'] >= rating_threshold) & \n",
    "                                        (user_ratings['rating'] < rating_threshold + 0.5)]['movieId'].values\n",
    "            neutral_movies = user_ratings[(user_ratings['rating'] >= rating_threshold - 0.5) & \n",
    "                                         (user_ratings['rating'] < rating_threshold)]['movieId'].values\n",
    "            disliked_movies = user_ratings[user_ratings['rating'] < rating_threshold - 0.5]['movieId'].values\n",
    "            \n",
    "            feature_preferences = {}\n",
    "            \n",
    "            for feature in feature_columns:\n",
    "                # Calculate weighted feature preference\n",
    "                strongly_liked_feature = movie_features[movie_features['movieId'].isin(strongly_liked_movies)][feature].sum()\n",
    "                liked_feature = movie_features[movie_features['movieId'].isin(liked_movies)][feature].sum()\n",
    "                neutral_feature = movie_features[movie_features['movieId'].isin(neutral_movies)][feature].sum()\n",
    "                disliked_feature = movie_features[movie_features['movieId'].isin(disliked_movies)][feature].sum()\n",
    "                \n",
    "                # Count movies in each category\n",
    "                strongly_liked_count = len(strongly_liked_movies) if len(strongly_liked_movies) > 0 else 1\n",
    "                liked_count = len(liked_movies) if len(liked_movies) > 0 else 1\n",
    "                neutral_count = len(neutral_movies) if len(neutral_movies) > 0 else 1\n",
    "                disliked_count = len(disliked_movies) if len(disliked_movies) > 0 else 1\n",
    "                \n",
    "                # Apply progressive weighting\n",
    "                strongly_liked_weight = 1.0\n",
    "                liked_weight = 0.7\n",
    "                neutral_weight = 0.2\n",
    "                disliked_weight = -0.8\n",
    "                \n",
    "                # Calculate weighted feature preference\n",
    "                preference = (\n",
    "                    strongly_liked_weight * (strongly_liked_feature / strongly_liked_count) +\n",
    "                    liked_weight * (liked_feature / liked_count) -\n",
    "                    neutral_weight * (neutral_feature / neutral_count) -\n",
    "                    disliked_weight * (disliked_feature / disliked_count)\n",
    "                )\n",
    "                \n",
    "                feature_preferences[feature] = preference\n",
    "            \n",
    "            # Normalize preferences to -1 to 1 range\n",
    "            max_abs_preference = max(abs(val) for val in feature_preferences.values()) if feature_preferences else 1\n",
    "            \n",
    "            for feature in feature_preferences:\n",
    "                feature_preferences[feature] = feature_preferences[feature] / max_abs_preference if max_abs_preference > 0 else 0\n",
    "            \n",
    "            feature_preferences['userId'] = user_id\n",
    "            \n",
    "            batch_preferences.append(feature_preferences)\n",
    "            \n",
    "            processed_users += 1\n",
    "        \n",
    "        # Add batch to main list\n",
    "        user_preferences.extend(batch_preferences)\n",
    "        \n",
    "        # Log progress\n",
    "        progress = processed_users / total_users * 100\n",
    "        elapsed = time.time() - start_time\n",
    "        remaining = elapsed * (total_users - processed_users) / processed_users if processed_users > 0 else 0\n",
    "        \n",
    "        logger.info(f\"Processed {processed_users}/{total_users} users ({progress:.1f}%) - Elapsed: {elapsed:.2f}s - Est. remaining: {remaining:.2f}s\")\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "    \n",
    "    user_preferences_df = pd.DataFrame(user_preferences)\n",
    "    logger.info(f\"Created preferences for {len(user_preferences_df)} users\")\n",
    "    \n",
    "    return user_preferences_df\n",
    "\n",
    "def prepare_dnn_training_data(train_ratings, user_preferences, movie_features, genre_columns, region_columns=None, threshold=3, max_samples=1000000):\n",
    "    \"\"\"\n",
    "    Prepare enhanced training data for DNN model with improved feature engineering\n",
    "    \"\"\"\n",
    "    logger.info(\"Preparing training data for DNN model with enhanced features\")\n",
    "    \n",
    "    # Include both genre and region columns for feature vectors\n",
    "    feature_columns = genre_columns.copy()\n",
    "    if region_columns:\n",
    "        feature_columns.extend(region_columns)\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # Calculate global stats\n",
    "    global_avg_rating = train_ratings['rating'].mean()\n",
    "    \n",
    "    # Calculate user stats\n",
    "    user_stats = {}\n",
    "    for user_id in user_preferences['userId'].unique():\n",
    "        user_ratings = train_ratings[train_ratings['userId'] == user_id]\n",
    "        if len(user_ratings) > 0:\n",
    "            user_stats[user_id] = {\n",
    "                'avg': user_ratings['rating'].mean(),\n",
    "                'std': user_ratings['rating'].std(),\n",
    "                'count': len(user_ratings)\n",
    "            }\n",
    "    \n",
    "    # Calculate movie stats\n",
    "    movie_stats = {}\n",
    "    for movie_id in movie_features['movieId'].unique():\n",
    "        movie_ratings = train_ratings[train_ratings['movieId'] == movie_id]\n",
    "        if len(movie_ratings) > 0:\n",
    "            movie_stats[movie_id] = {\n",
    "                'avg': movie_ratings['rating'].mean(),\n",
    "                'std': movie_ratings['rating'].std(),\n",
    "                'count': len(movie_ratings)\n",
    "            }\n",
    "    \n",
    "    # Limit sample size for memory efficiency with stratification\n",
    "    if len(train_ratings) > max_samples:\n",
    "        # Stratify by rating to maintain distribution\n",
    "        bin_edges = [0, 1.5, 2.5, 3, 4.5, 5.1]  # Bins for ratings\n",
    "        train_ratings['rating_bin'] = pd.cut(train_ratings['rating'], bins=bin_edges, labels=False)\n",
    "        \n",
    "        # Sample from each bin proportionally\n",
    "        sampled_ratings = pd.DataFrame()\n",
    "        for bin_id in range(len(bin_edges)-1):\n",
    "            bin_data = train_ratings[train_ratings['rating_bin'] == bin_id]\n",
    "            bin_sample_size = int(max_samples * (len(bin_data) / len(train_ratings)))\n",
    "            \n",
    "            if len(bin_data) > bin_sample_size:\n",
    "                bin_sampled = bin_data.sample(bin_sample_size, random_state=42)\n",
    "            else:\n",
    "                bin_sampled = bin_data\n",
    "                \n",
    "            sampled_ratings = pd.concat([sampled_ratings, bin_sampled])\n",
    "        \n",
    "        # Clean up\n",
    "        sampled_ratings = sampled_ratings.drop(columns=['rating_bin'])\n",
    "    else:\n",
    "        sampled_ratings = train_ratings\n",
    "    \n",
    "    # Process in batches for memory efficiency\n",
    "    batch_size = 10000\n",
    "    total_ratings = len(sampled_ratings)\n",
    "    processed_ratings = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_start in range(0, total_ratings, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, total_ratings)\n",
    "        ratings_batch = sampled_ratings.iloc[batch_start:batch_end]\n",
    "        \n",
    "        batch_features = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        for _, row in ratings_batch.iterrows():\n",
    "            user_id = row['userId']\n",
    "            movie_id = row['movieId']\n",
    "            rating = row['rating']\n",
    "            \n",
    "            # Convert rating to binary label based on threshold\n",
    "            binary_label = 1 if rating > threshold else 0\n",
    "            \n",
    "            if user_id not in user_preferences['userId'].values or \\\n",
    "               movie_id not in movie_features['movieId'].values:\n",
    "                continue\n",
    "            \n",
    "            user_prefs = user_preferences[user_preferences['userId'] == user_id].iloc[0]\n",
    "            \n",
    "            movie_row = movie_features[movie_features['movieId'] == movie_id]\n",
    "            if movie_row.empty:\n",
    "                continue\n",
    "                \n",
    "            movie_features_row = movie_row.iloc[0]\n",
    "            \n",
    "            # Create enhanced feature vector with rating context\n",
    "            feature_vector = []\n",
    "            \n",
    "            # Add user and movie bias features\n",
    "            user_info = user_stats.get(user_id, {'avg': global_avg_rating, 'std': 0.5, 'count': 0})\n",
    "            movie_info = movie_stats.get(movie_id, {'avg': global_avg_rating, 'std': 0.5, 'count': 0})\n",
    "            \n",
    "            # Add global context features\n",
    "            feature_vector.append(global_avg_rating / 5.0)  # Normalize to 0-1\n",
    "            \n",
    "            # Add user context features (normalized)\n",
    "            feature_vector.append(user_info['avg'] / 5.0)  # User average rating\n",
    "            feature_vector.append(min(1.0, user_info['std'] / 2.0))  # User rating variability\n",
    "            feature_vector.append(min(1.0, np.log1p(user_info['count']) / 10.0))  # User experience\n",
    "            \n",
    "            # Add movie context features (normalized)\n",
    "            feature_vector.append(movie_info['avg'] / 5.0)  # Movie average rating\n",
    "            feature_vector.append(min(1.0, movie_info['std'] / 2.0))  # Movie rating variability \n",
    "            feature_vector.append(min(1.0, np.log1p(movie_info['count']) / 10.0))  # Movie popularity\n",
    "            \n",
    "            # Add user-movie difference feature\n",
    "            feature_vector.append((user_info['avg'] - movie_info['avg'] + 2.5) / 5.0)  # Normalized difference\n",
    "            \n",
    "            # Add category features with more sophisticated interaction\n",
    "            for feature in feature_columns:\n",
    "                user_pref = user_prefs[feature]\n",
    "                movie_feat = movie_features_row[feature]\n",
    "                \n",
    "                # Add user preference\n",
    "                feature_vector.append(user_pref)\n",
    "                \n",
    "                # Add movie feature\n",
    "                feature_vector.append(movie_feat)\n",
    "                \n",
    "                # Add interaction features\n",
    "                feature_vector.append(user_pref * movie_feat)  # Product\n",
    "                feature_vector.append(user_pref + movie_feat - 0.5)  # Sum (normalized)\n",
    "                feature_vector.append(abs(user_pref - movie_feat))  # Absolute difference\n",
    "            \n",
    "            batch_features.append(feature_vector)\n",
    "            batch_labels.append(binary_label)\n",
    "        \n",
    "        features.extend(batch_features)\n",
    "        labels.extend(batch_labels)\n",
    "        \n",
    "        processed_ratings += len(ratings_batch)\n",
    "        \n",
    "        if batch_start % (10 * batch_size) == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            progress = processed_ratings / total_ratings * 100\n",
    "            remaining = elapsed * (total_ratings - processed_ratings) / processed_ratings if processed_ratings > 0 else 0\n",
    "            logger.info(f\"Processed {processed_ratings}/{total_ratings} ratings ({progress:.1f}%) - Elapsed: {elapsed:.2f}s - Est. remaining: {remaining:.2f}s\")\n",
    "        \n",
    "        gc.collect()\n",
    "    \n",
    "    X = np.array(features, dtype=np.float32)\n",
    "    y = np.array(labels, dtype=np.float32)\n",
    "    \n",
    "    # Split the data into training and validation sets with stratification\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Created training data: X_train shape {X_train.shape}, y_train shape {y_train.shape}\")\n",
    "    logger.info(f\"Created validation data: X_val shape {X_val.shape}, y_val shape {y_val.shape}\")\n",
    "    \n",
    "    # Check class distribution\n",
    "    train_pos = np.sum(y_train)\n",
    "    train_neg = len(y_train) - train_pos\n",
    "    val_pos = np.sum(y_val)\n",
    "    val_neg = len(y_val) - val_pos\n",
    "    \n",
    "    logger.info(f\"Training set class distribution: Positive {train_pos} ({train_pos/len(y_train)*100:.1f}%), Negative {train_neg} ({train_neg/len(y_train)*100:.1f}%)\")\n",
    "    logger.info(f\"Validation set class distribution: Positive {val_pos} ({val_pos/len(y_val)*100:.1f}%), Negative {val_neg} ({val_neg/len(y_val)*100:.1f}%)\")\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val, feature_columns\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    \"\"\"Custom F1 score metric for Keras\"\"\"\n",
    "    # Calculate precision and recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    \n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1\n",
    "\n",
    "def focal_loss(alpha=0.25, gamma=2.0):\n",
    "    \"\"\"\n",
    "    Focal loss for better handling of class imbalance\n",
    "    \n",
    "    Parameters:\n",
    "    alpha: Weighting factor for the rare class (positive)\n",
    "    gamma: Focusing parameter to down-weight easy examples\n",
    "    \"\"\"\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        \n",
    "        # Calculate loss with focal weighting\n",
    "        loss = -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1 + K.epsilon())) - \\\n",
    "               K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n",
    "               \n",
    "        # Normalize by batch size\n",
    "        return loss / K.cast(K.shape(y_true)[0], 'float32')\n",
    "    \n",
    "    return focal_loss_fixed\n",
    "\n",
    "class SelfAttentionLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Properly implemented self-attention layer that handles 2D inputs\"\"\"\n",
    "    def __init__(self, hidden_units, **kwargs):\n",
    "        super(SelfAttentionLayer, self).__init__(**kwargs)\n",
    "        self.hidden_units = hidden_units\n",
    "        self.supports_masking = True\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Input shape validation\n",
    "        if len(input_shape) < 2:\n",
    "            raise ValueError(f\"Input shape must be at least 2D, got {input_shape}\")\n",
    "            \n",
    "        # Determine if we need to reshape 2D input\n",
    "        self.needs_reshape = len(input_shape) == 2\n",
    "        input_dim = input_shape[-1]\n",
    "        \n",
    "        # Create trainable weights\n",
    "        self.query_dense = tf.keras.layers.Dense(self.hidden_units)\n",
    "        self.key_dense = tf.keras.layers.Dense(self.hidden_units)\n",
    "        self.value_dense = tf.keras.layers.Dense(self.hidden_units)\n",
    "        self.combine_dense = tf.keras.layers.Dense(input_dim)\n",
    "        \n",
    "        self.built = True\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Handle 2D inputs by reshaping to 3D (batch_size, 1, features)\n",
    "        original_shape = tf.shape(inputs)\n",
    "        needs_reshape = len(inputs.shape) == 2\n",
    "        \n",
    "        if needs_reshape:\n",
    "            # Add sequence dimension of length 1\n",
    "            inputs = tf.expand_dims(inputs, axis=1)\n",
    "        \n",
    "        # Apply transformations\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        \n",
    "        # Calculate attention scores - use standard matrix multiplication\n",
    "        # (batch_size, seq_len, hidden) × (batch_size, hidden, seq_len) \n",
    "        # → (batch_size, seq_len, seq_len)\n",
    "        key_transposed = tf.transpose(key, perm=[0, 2, 1])\n",
    "        scores = tf.matmul(query, key_transposed)\n",
    "        \n",
    "        # Scale scores\n",
    "        scores = scores / tf.math.sqrt(tf.cast(self.hidden_units, dtype=tf.float32))\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        weights = tf.nn.softmax(scores, axis=-1)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        context = tf.matmul(weights, value)\n",
    "        \n",
    "        # Final transformation\n",
    "        output = self.combine_dense(context)\n",
    "        \n",
    "        # Add residual connection\n",
    "        output = output + inputs\n",
    "        \n",
    "        # Reshape back to original shape if needed\n",
    "        if needs_reshape:\n",
    "            output = tf.squeeze(output, axis=1)\n",
    "            \n",
    "        return output\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # Output shape is same as input shape\n",
    "        return input_shape\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super(SelfAttentionLayer, self).get_config()\n",
    "        config.update({\n",
    "            'hidden_units': self.hidden_units\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "def attention_block(x, hidden_units):\n",
    "    \"\"\"Wrapper function to apply the attention layer\"\"\"\n",
    "    # Create the attention layer\n",
    "    attention_layer = SelfAttentionLayer(hidden_units)\n",
    "    \n",
    "    # Apply attention directly (no need to reshape first - layer handles that)\n",
    "    x = attention_layer(x)\n",
    "    \n",
    "    # Apply layer normalization\n",
    "    x = tf.keras.layers.LayerNormalization()(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def build_and_train_dnn_model(X_train, X_val, y_train, y_val, learning_rate=0.0005, batch_size=256, epochs=50):\n",
    "    \"\"\"\n",
    "    Build and train an enhanced DNN model with advanced architecture and training techniques\n",
    "    \"\"\"\n",
    "    logger.info(\"Building and training enhanced DNN model\")\n",
    "    \n",
    "    # Configure GPU memory growth if available\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logger.info(f\"Found {len(gpus)} GPU(s), enabled memory growth\")\n",
    "        except RuntimeError as e:\n",
    "            logger.warning(f\"GPU config error: {e}\")\n",
    "    \n",
    "    input_dim = X_train.shape[1]\n",
    "    \n",
    "    # Create model with more sophisticated architecture\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Normalize inputs\n",
    "    x = BatchNormalization()(inputs)\n",
    "    \n",
    "    # First block - extract latent representations\n",
    "    x = Dense(dnn_hidden_layers[0], activation='relu', kernel_regularizer=l2(dnn_l2_reg))(x)\n",
    "    x_shortcut1 = x  # Save for residual connection\n",
    "    x = Dropout(dnn_dropout_rate)(x)\n",
    "    \n",
    "    # Apply attention if enabled\n",
    "    if use_attention_mechanism:\n",
    "        x = attention_block(x, dnn_hidden_layers[0] // 2)\n",
    "    \n",
    "    # Middle layers with residual connections\n",
    "    for i, units in enumerate(dnn_hidden_layers[1:], 1):\n",
    "        # Normalization before each layer\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        # Dense layer with regularization\n",
    "        x = Dense(units, activation='relu', kernel_regularizer=l2(dnn_l2_reg))(x)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        x = Dropout(dnn_dropout_rate)(x)\n",
    "        \n",
    "        # Add residual connection when dimensions match or after projection\n",
    "        if i == 1 and x_shortcut1.shape[-1] >= units:\n",
    "            # Project if needed then add\n",
    "            if x_shortcut1.shape[-1] > units:\n",
    "                shortcut = Dense(units, activation='linear')(x_shortcut1)\n",
    "            else:\n",
    "                shortcut = x_shortcut1\n",
    "            x = Add()([x, shortcut])\n",
    "    \n",
    "    # Final normalization\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Output layer with sigmoid activation for binary classification\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Learning rate schedule with cosine annealing\n",
    "    if use_cosine_annealing:\n",
    "        lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "            initial_learning_rate=learning_rate,\n",
    "            first_decay_steps=int(len(X_train) / batch_size) * 5,  # 5 epochs\n",
    "            t_mul=2.0,  # Double period after each restart\n",
    "            m_mul=0.9,  # Slightly reduce max LR after each restart\n",
    "            alpha=1e-6  # Minimum LR\n",
    "        )\n",
    "        optimizer = Adam(learning_rate=lr_schedule, clipnorm=1.0)\n",
    "    else:\n",
    "        # Standard Adam optimizer with gradient clipping\n",
    "        optimizer = Adam(\n",
    "            learning_rate=learning_rate,\n",
    "            clipnorm=1.0\n",
    "        )\n",
    "    \n",
    "    # Compile with appropriate loss and metrics\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',  # Standard loss for binary classification\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.AUC(name='auc'),\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            f1_metric\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Calculate class weights to handle imbalance\n",
    "    neg_count = len(y_train) - np.sum(y_train)\n",
    "    pos_count = np.sum(y_train)\n",
    "    pos_weight = (1 / pos_count) * ((neg_count + pos_count) / 2.0) if pos_count > 0 else 1.0\n",
    "    neg_weight = (1 / neg_count) * ((neg_count + pos_count) / 2.0) if neg_count > 0 else 1.0\n",
    "    \n",
    "    class_weight = {0: neg_weight, 1: pos_weight}\n",
    "    \n",
    "    logger.info(f\"Class weights: {class_weight}\")\n",
    "    import keras\n",
    "    keras_version = keras.__version__\n",
    "    model_ext = '.keras' if keras_version.startswith('3.') else '.h5'\n",
    "    # Enhanced callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=early_stopping_patience,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        # ReduceLROnPlateau is removed!\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(output_path, 'best_model.keras'),\n",
    "            monitor='val_auc',\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train the model with class weights\n",
    "    logger.info(f\"Training model with {epochs} max epochs, batch size {batch_size}\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weight  # Apply class weights\n",
    "    )\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_metrics = model.evaluate(X_val, y_val, verbose=1)\n",
    "    \n",
    "    # Extract metrics - order matches the model.compile metrics list\n",
    "    val_loss = val_metrics[0]\n",
    "    val_acc = val_metrics[1]\n",
    "    val_auc = val_metrics[2]\n",
    "    val_precision = val_metrics[3]\n",
    "    val_recall = val_metrics[4]\n",
    "    val_f1 = val_metrics[5]\n",
    "    \n",
    "    logger.info(f\"Model validation metrics:\")\n",
    "    logger.info(f\"- Loss: {val_loss:.4f}\")\n",
    "    logger.info(f\"- Accuracy: {val_acc:.4f}\")\n",
    "    logger.info(f\"- AUC: {val_auc:.4f}\")\n",
    "    logger.info(f\"- Precision: {val_precision:.4f}\")\n",
    "    logger.info(f\"- Recall: {val_recall:.4f}\")\n",
    "    logger.info(f\"- F1 Score: {val_f1:.4f}\")\n",
    "    \n",
    "    # Plot training history with more metrics\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(history.history['auc'], label='Training AUC')\n",
    "    plt.plot(history.history['val_auc'], label='Validation AUC')\n",
    "    plt.title('AUC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(history.history['precision'], label='Training Precision')\n",
    "    plt.plot(history.history['val_precision'], label='Validation Precision')\n",
    "    plt.title('Precision')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(history.history['recall'], label='Training Recall')\n",
    "    plt.plot(history.history['val_recall'], label='Validation Recall')\n",
    "    plt.title('Recall')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.plot(history.history['f1_metric'], label='Training F1')\n",
    "    plt.plot(history.history['val_f1_metric'], label='Validation F1')\n",
    "    plt.title('F1 Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_path, 'dnn_training_history.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Create additional visualization - ROC curve\n",
    "    y_pred_prob = model.predict(X_val)\n",
    "    \n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(os.path.join(output_path, 'roc_curve.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Create precision-recall curve\n",
    "    from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)\n",
    "    avg_precision = average_precision_score(y_val, y_pred_prob)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AP = {avg_precision:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.savefig(os.path.join(output_path, 'precision_recall_curve.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def generate_user_movie_features(user_id, movie_id, user_preferences, movie_features, genre_columns, region_columns=None, user_stats=None, movie_stats=None, global_avg_rating=None):\n",
    "    \"\"\"\n",
    "    Generate enhanced feature vector for a user-movie pair\n",
    "    \"\"\"\n",
    "    if global_avg_rating is None:\n",
    "        global_avg_rating = 3.0\n",
    "    \n",
    "    if user_stats is None:\n",
    "        user_stats = {}\n",
    "    \n",
    "    if movie_stats is None:\n",
    "        movie_stats = {}\n",
    "        \n",
    "    feature_columns = genre_columns.copy()\n",
    "    if region_columns:\n",
    "        feature_columns.extend(region_columns)\n",
    "    \n",
    "    if user_id not in user_preferences['userId'].values or \\\n",
    "       movie_id not in movie_features['movieId'].values:\n",
    "        return None\n",
    "    \n",
    "    user_prefs = user_preferences[user_preferences['userId'] == user_id].iloc[0]\n",
    "    \n",
    "    movie_row = movie_features[movie_features['movieId'] == movie_id]\n",
    "    if movie_row.empty:\n",
    "        return None\n",
    "    \n",
    "    movie_features_row = movie_row.iloc[0]\n",
    "    \n",
    "    # Create enhanced feature vector with user/movie context\n",
    "    feature_vector = []\n",
    "    \n",
    "    # Get user and movie statistics\n",
    "    user_info = user_stats.get(user_id, {'avg': global_avg_rating, 'std': 0.5, 'count': 0})\n",
    "    movie_info = movie_stats.get(movie_id, {'avg': global_avg_rating, 'std': 0.5, 'count': 0})\n",
    "    \n",
    "    # Add global context features\n",
    "    feature_vector.append(global_avg_rating / 5.0)  # Normalize\n",
    "    \n",
    "    # Add user context features\n",
    "    feature_vector.append(user_info['avg'] / 5.0)  # User average rating\n",
    "    feature_vector.append(min(1.0, user_info['std'] / 2.0))  # User rating variability\n",
    "    feature_vector.append(min(1.0, np.log1p(user_info['count']) / 10.0))  # User experience\n",
    "    \n",
    "    # Add movie context features\n",
    "    feature_vector.append(movie_info['avg'] / 5.0)  # Movie average rating\n",
    "    feature_vector.append(min(1.0, movie_info['std'] / 2.0))  # Movie rating variability \n",
    "    feature_vector.append(min(1.0, np.log1p(movie_info['count']) / 10.0))  # Movie popularity\n",
    "    \n",
    "    # Add user-movie difference feature\n",
    "    feature_vector.append((user_info['avg'] - movie_info['avg'] + 2.5) / 5.0)  # Normalized difference\n",
    "    \n",
    "    # Add category features with more sophisticated interaction\n",
    "    for feature in feature_columns:\n",
    "        user_pref = user_prefs[feature]\n",
    "        movie_feat = movie_features_row[feature]\n",
    "        \n",
    "        # Add user preference\n",
    "        feature_vector.append(user_pref)\n",
    "        \n",
    "        # Add movie feature\n",
    "        feature_vector.append(movie_feat)\n",
    "        \n",
    "        # Add interaction features\n",
    "        feature_vector.append(user_pref * movie_feat)  # Product\n",
    "        feature_vector.append(user_pref + movie_feat - 0.5)  # Sum (normalized)\n",
    "        feature_vector.append(abs(user_pref - movie_feat))  # Absolute difference\n",
    "    \n",
    "    return np.array([feature_vector], dtype=np.float32)\n",
    "\n",
    "def generate_dnn_recommendations(user_id, dnn_model, user_preferences, movie_features, genre_columns, region_columns=None, train_ratings=None, user_stats=None, movie_stats=None, global_avg_rating=None, n=10):\n",
    "    \"\"\"\n",
    "    Generate movie recommendations for a user using the enhanced DNN model\n",
    "    \"\"\"\n",
    "    if global_avg_rating is None and train_ratings is not None:\n",
    "        global_avg_rating = train_ratings['rating'].mean()\n",
    "    elif global_avg_rating is None:\n",
    "        global_avg_rating = 3.0\n",
    "        \n",
    "    if user_id not in user_preferences['userId'].values:\n",
    "        logger.warning(f\"User {user_id} not found in preferences\")\n",
    "        return []\n",
    "    \n",
    "    # Get movies the user has already rated\n",
    "    rated_movies = set()\n",
    "    if train_ratings is not None:\n",
    "        rated_movies = set(train_ratings[train_ratings['userId'] == user_id]['movieId'].values)\n",
    "    \n",
    "    # Consider only unrated movies\n",
    "    unrated_movies = movie_features[~movie_features['movieId'].isin(rated_movies)]\n",
    "    \n",
    "    batch_size = 1000\n",
    "    all_predictions = []\n",
    "    \n",
    "    # Process in batches to avoid memory issues\n",
    "    for i in range(0, len(unrated_movies), batch_size):\n",
    "        batch = unrated_movies.iloc[i:i+batch_size]\n",
    "        \n",
    "        feature_vectors = []\n",
    "        movie_ids = []\n",
    "        \n",
    "        for _, movie_row in batch.iterrows():\n",
    "            movie_id = movie_row['movieId']\n",
    "            \n",
    "            # Generate features for this user-movie pair\n",
    "            feature_vector = generate_user_movie_features(\n",
    "                user_id, movie_id, \n",
    "                user_preferences, movie_features, \n",
    "                genre_columns, region_columns,\n",
    "                user_stats, movie_stats, global_avg_rating\n",
    "            )\n",
    "            \n",
    "            if feature_vector is not None:\n",
    "                feature_vectors.append(feature_vector[0])  # Flatten first dimension\n",
    "                movie_ids.append(movie_id)\n",
    "        \n",
    "        if not feature_vectors:\n",
    "            continue\n",
    "            \n",
    "        # Convert to numpy array for batch prediction\n",
    "        feature_array = np.array(feature_vectors)\n",
    "        \n",
    "        # Get probability scores (0-1)\n",
    "        like_probabilities = dnn_model.predict(feature_array, verbose=0).flatten()\n",
    "        \n",
    "        # Convert to rating scale for easier comparison\n",
    "        predicted_ratings = 0.5 + like_probabilities * 4.5\n",
    "        \n",
    "        for movie_id, pred in zip(movie_ids, predicted_ratings):\n",
    "            all_predictions.append((movie_id, float(pred)))\n",
    "    \n",
    "    # Sort by predicted rating\n",
    "    all_predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return all_predictions[:n]\n",
    "\n",
    "def generate_recommendations_for_all_users(dnn_model, user_preferences, movie_features, genre_columns, region_columns=None, train_ratings=None, n=10, batch_size=50, max_users=None):\n",
    "    \"\"\"\n",
    "    Generate recommendations for all users with improved memory management\n",
    "    \"\"\"\n",
    "    all_user_ids = user_preferences['userId'].unique()\n",
    "    \n",
    "    if max_users and max_users < len(all_user_ids):\n",
    "        user_ids = all_user_ids[:max_users]\n",
    "    else:\n",
    "        user_ids = all_user_ids\n",
    "    \n",
    "    logger.info(f\"Generating recommendations for {len(user_ids)} users\")\n",
    "    \n",
    "    all_recommendations = {}\n",
    "    total_users = len(user_ids)\n",
    "    \n",
    "    # Precompute global and user/movie statistics for faster recommendations\n",
    "    global_avg_rating = train_ratings['rating'].mean() if train_ratings is not None else 3.0\n",
    "    \n",
    "    # Precompute user statistics\n",
    "    user_stats = {}\n",
    "    if train_ratings is not None:\n",
    "        for user_id in user_ids:\n",
    "            user_ratings = train_ratings[train_ratings['userId'] == user_id]\n",
    "            if len(user_ratings) > 0:\n",
    "                user_stats[user_id] = {\n",
    "                    'avg': user_ratings['rating'].mean(),\n",
    "                    'std': user_ratings['rating'].std() if len(user_ratings) > 1 else 0.5,\n",
    "                    'count': len(user_ratings)\n",
    "                }\n",
    "    \n",
    "    # Precompute movie statistics (for most popular movies)\n",
    "    movie_stats = {}\n",
    "    if train_ratings is not None:\n",
    "        # Group by movieId and count\n",
    "        movie_counts = train_ratings['movieId'].value_counts()\n",
    "        \n",
    "        # Get popular movies (top 10%)\n",
    "        popular_threshold = np.percentile(movie_counts.values, 90) if len(movie_counts) > 10 else 0\n",
    "        popular_movies = movie_counts[movie_counts >= popular_threshold].index\n",
    "        \n",
    "        for movie_id in popular_movies:\n",
    "            movie_ratings = train_ratings[train_ratings['movieId'] == movie_id]\n",
    "            if len(movie_ratings) > 0:\n",
    "                movie_stats[movie_id] = {\n",
    "                    'avg': movie_ratings['rating'].mean(),\n",
    "                    'std': movie_ratings['rating'].std() if len(movie_ratings) > 1 else 0.5,\n",
    "                    'count': len(movie_ratings)\n",
    "                }\n",
    "    \n",
    "    # Create a dictionary of rated movies by user for faster lookups\n",
    "    user_rated_movies = {}\n",
    "    if train_ratings is not None:\n",
    "        for user_id in user_ids:\n",
    "            user_rated_movies[user_id] = set(train_ratings[train_ratings['userId'] == user_id]['movieId'].values)\n",
    "    \n",
    "    feature_columns = genre_columns.copy()\n",
    "    if region_columns:\n",
    "        feature_columns.extend(region_columns)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for i in range(0, total_users, batch_size):\n",
    "        batch_end = min(i + batch_size, total_users)\n",
    "        batch_users = user_ids[i:batch_end]\n",
    "        \n",
    "        batch_start_time = time.time()\n",
    "        \n",
    "        for user_idx, user_id in enumerate(batch_users):\n",
    "            user_prefs = user_preferences[user_preferences['userId'] == user_id]\n",
    "            if user_prefs.empty:\n",
    "                continue\n",
    "            \n",
    "            # Get movies already rated by this user\n",
    "            rated_movies = user_rated_movies.get(user_id, set())\n",
    "            \n",
    "            # Consider only unrated movies\n",
    "            unrated_movie_ids = set(movie_features['movieId']) - rated_movies\n",
    "            \n",
    "            # For large datasets, sample a subset of candidate movies\n",
    "            # to improve efficiency while maintaining diversity\n",
    "            max_movies_per_batch = 2000\n",
    "            \n",
    "            if len(unrated_movie_ids) > max_movies_per_batch:\n",
    "                # Use a smarter sampling method:\n",
    "                # 1. Include some popular movies (higher chance of being liked)\n",
    "                # 2. Include movies with high genre match to user preferences\n",
    "                # 3. Include some random movies for diversity\n",
    "                \n",
    "                # Convert preferences to dictionary for easier access\n",
    "                pref_dict = user_prefs.iloc[0].to_dict()\n",
    "                \n",
    "                # Find top genres by preference\n",
    "                genre_prefs = [(genre, pref_dict.get(genre, 0)) \n",
    "                              for genre in genre_columns if genre in pref_dict]\n",
    "                top_genres = sorted(genre_prefs, key=lambda x: x[1], reverse=True)[:5]\n",
    "                top_genre_names = [g[0] for g in top_genres if g[1] > 0]\n",
    "                \n",
    "                # Get movies from top genres (if any positive preferences)\n",
    "                top_genre_movies = set()\n",
    "                if top_genre_names:\n",
    "                    for genre in top_genre_names:\n",
    "                        genre_movies = set(movie_features[movie_features[genre] == 1]['movieId'])\n",
    "                        top_genre_movies.update(genre_movies)\n",
    "                    \n",
    "                    # Filter to unrated movies only\n",
    "                    top_genre_movies = top_genre_movies.intersection(unrated_movie_ids)\n",
    "                    \n",
    "                    # Limit to a reasonable number\n",
    "                    if len(top_genre_movies) > max_movies_per_batch // 2:\n",
    "                        top_genre_movies = set(list(top_genre_movies)[:max_movies_per_batch // 2])\n",
    "                \n",
    "                # Get popular movies based on movie_stats\n",
    "                popular_movies = set([m for m, stats in movie_stats.items() \n",
    "                                    if stats['count'] > 5 and stats['avg'] >= 3])\n",
    "                popular_unrated = popular_movies.intersection(unrated_movie_ids) - top_genre_movies\n",
    "                \n",
    "                # Limit number of popular movies\n",
    "                if len(popular_unrated) > max_movies_per_batch // 4:\n",
    "                    popular_unrated = set(list(popular_unrated)[:max_movies_per_batch // 4])\n",
    "                \n",
    "                # Random sampling for remaining slots\n",
    "                remaining_count = max_movies_per_batch - len(top_genre_movies) - len(popular_unrated)\n",
    "                remaining_movies = unrated_movie_ids - top_genre_movies - popular_unrated\n",
    "                \n",
    "                if len(remaining_movies) > remaining_count:\n",
    "                    remaining_sample = np.random.choice(list(remaining_movies), \n",
    "                                                       size=remaining_count, \n",
    "                                                       replace=False)\n",
    "                    remaining_movies = set(remaining_sample)\n",
    "                \n",
    "                # Combine all selected movies\n",
    "                unrated_movie_ids = top_genre_movies.union(popular_unrated).union(remaining_movies)\n",
    "            \n",
    "            candidate_movies = movie_features[movie_features['movieId'].isin(unrated_movie_ids)]\n",
    "            \n",
    "            if len(candidate_movies) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Process movies in batches to avoid memory issues\n",
    "            movie_batch_size = 200\n",
    "            predictions = []\n",
    "            \n",
    "            for j in range(0, len(candidate_movies), movie_batch_size):\n",
    "                movie_batch_end = min(j + movie_batch_size, len(candidate_movies))\n",
    "                movie_batch = candidate_movies.iloc[j:movie_batch_end]\n",
    "                \n",
    "                batch_features = []\n",
    "                batch_movie_ids = []\n",
    "                \n",
    "                for _, movie_row in movie_batch.iterrows():\n",
    "                    movie_id = movie_row['movieId']\n",
    "                    \n",
    "                    # Get or compute movie stats on-demand for non-cached movies\n",
    "                    if movie_id not in movie_stats and train_ratings is not None:\n",
    "                        movie_ratings = train_ratings[train_ratings['movieId'] == movie_id]\n",
    "                        if len(movie_ratings) > 0:\n",
    "                            movie_stats[movie_id] = {\n",
    "                                'avg': movie_ratings['rating'].mean(),\n",
    "                                'std': movie_ratings['rating'].std() if len(movie_ratings) > 1 else 0.5,\n",
    "                                'count': len(movie_ratings)\n",
    "                            }\n",
    "                    \n",
    "                    # Generate features\n",
    "                    feature_vector = []\n",
    "                    \n",
    "                    # Get user and movie stats\n",
    "                    user_info = user_stats.get(user_id, {'avg': global_avg_rating, 'std': 0.5, 'count': 0})\n",
    "                    movie_info = movie_stats.get(movie_id, {'avg': global_avg_rating, 'std': 0.5, 'count': 0})\n",
    "                    \n",
    "                    # Add global context features\n",
    "                    feature_vector.append(global_avg_rating / 5.0)  # Normalize\n",
    "                    \n",
    "                    # Add user context features\n",
    "                    feature_vector.append(user_info['avg'] / 5.0)  # User average rating\n",
    "                    feature_vector.append(min(1.0, user_info['std'] / 2.0))  # User rating variability\n",
    "                    feature_vector.append(min(1.0, np.log1p(user_info['count']) / 10.0))  # User experience\n",
    "                    \n",
    "                    # Add movie context features\n",
    "                    feature_vector.append(movie_info['avg'] / 5.0)  # Movie average rating\n",
    "                    feature_vector.append(min(1.0, movie_info['std'] / 2.0))  # Movie rating variability \n",
    "                    feature_vector.append(min(1.0, np.log1p(movie_info['count']) / 10.0))  # Movie popularity\n",
    "                    \n",
    "                    # Add user-movie difference feature\n",
    "                    feature_vector.append((user_info['avg'] - movie_info['avg'] + 2.5) / 5.0)  # Normalized difference\n",
    "                    \n",
    "                    # Add features with interactions\n",
    "                    for feature in feature_columns:\n",
    "                        user_pref = user_prefs.iloc[0][feature]\n",
    "                        movie_feat = movie_row[feature]\n",
    "                        \n",
    "                        # Add user preference\n",
    "                        feature_vector.append(user_pref)\n",
    "                        \n",
    "                        # Add movie feature\n",
    "                        feature_vector.append(movie_feat)\n",
    "                        \n",
    "                        # Add interaction features\n",
    "                        feature_vector.append(user_pref * movie_feat)  # Product\n",
    "                        feature_vector.append(user_pref + movie_feat - 0.5)  # Sum (normalized)\n",
    "                        feature_vector.append(abs(user_pref - movie_feat))  # Absolute difference\n",
    "                    \n",
    "                    batch_features.append(feature_vector)\n",
    "                    batch_movie_ids.append(movie_id)\n",
    "                \n",
    "                batch_features = np.array(batch_features, dtype=np.float32)\n",
    "                \n",
    "                if len(batch_features) == 0:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Get probability scores\n",
    "                    batch_predictions = dnn_model.predict(batch_features, verbose=0).flatten()\n",
    "                    \n",
    "                    # Convert to rating-like scale\n",
    "                    batch_ratings = 0.5 + (batch_predictions * 4.5)\n",
    "                    \n",
    "                    for movie_id, pred in zip(batch_movie_ids, batch_ratings):\n",
    "                        predictions.append((movie_id, float(pred)))\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error predicting for user {user_id}: {e}\")\n",
    "            \n",
    "            if predictions:\n",
    "                predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "                all_recommendations[user_id] = predictions[:n]\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        progress = batch_end / total_users * 100\n",
    "        remaining = elapsed / batch_end * (total_users - batch_end) if batch_end > 0 else 0\n",
    "        \n",
    "        logger.info(f\"Processed {batch_end}/{total_users} users ({progress:.1f}%) - Elapsed: {elapsed:.2f}s - Est. remaining: {remaining:.2f}s\")\n",
    "        \n",
    "        gc.collect()\n",
    "    \n",
    "    logger.info(f\"Generated recommendations for {len(all_recommendations)}/{total_users} users\")\n",
    "    \n",
    "    return all_recommendations\n",
    "\n",
    "def evaluate_recommendations(recommendations, test_ratings, dnn_model, user_preferences, movie_features, genre_columns, region_columns=None, threshold=3):\n",
    "    \"\"\"\n",
    "    Evaluate recommendation model using enhanced metrics\n",
    "    \"\"\"\n",
    "    logger.info(\"Evaluating recommendations with comprehensive metrics\")\n",
    "    \n",
    "    # Find common users between test set and recommendations\n",
    "    test_users = set(test_ratings['userId'].unique())\n",
    "    recommendation_users = set(recommendations.keys())\n",
    "    common_users = test_users.intersection(recommendation_users)\n",
    "    \n",
    "    logger.info(f\"Test users: {len(test_users)}, Users with recommendations: {len(recommendation_users)}\")\n",
    "    logger.info(f\"Common users for evaluation: {len(common_users)}\")\n",
    "    \n",
    "    if len(common_users) == 0:\n",
    "        logger.warning(\"No common users between test set and recommendations\")\n",
    "        \n",
    "        # Calculate baseline metrics for all test data\n",
    "        # Use global average rating as prediction\n",
    "        global_avg_rating = test_ratings['rating'].mean()\n",
    "        predictions = np.full(len(test_ratings), global_avg_rating)\n",
    "        actuals = test_ratings['rating'].values\n",
    "        \n",
    "        # Calculate RMSE and MAE\n",
    "        mse = np.mean((predictions - actuals) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = np.mean(np.abs(predictions - actuals))\n",
    "        \n",
    "        # Convert to binary for classification metrics\n",
    "        binary_preds = (predictions > threshold).astype(int)\n",
    "        binary_actuals = (actuals > threshold).astype(int)\n",
    "        \n",
    "        # Classification metrics\n",
    "        accuracy = np.mean(binary_preds == binary_actuals)\n",
    "        \n",
    "        # Calculate confusion matrix elements\n",
    "        true_positives = np.sum((binary_preds == 1) & (binary_actuals == 1))\n",
    "        false_positives = np.sum((binary_preds == 1) & (binary_actuals == 0))\n",
    "        true_negatives = np.sum((binary_preds == 0) & (binary_actuals == 0))\n",
    "        false_negatives = np.sum((binary_preds == 0) & (binary_actuals == 1))\n",
    "        \n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        logger.info(f\"Baseline evaluation results:\")\n",
    "        logger.info(f\"RMSE: {rmse:.4f}\")\n",
    "        logger.info(f\"MAE: {mae:.4f}\")\n",
    "        logger.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "        logger.info(f\"Precision: {precision:.4f}\")\n",
    "        logger.info(f\"Recall: {recall:.4f}\")\n",
    "        logger.info(f\"F1 Score: {f1_score:.4f}\")\n",
    "        \n",
    "        metrics = {\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score,\n",
    "            'num_predictions': len(test_ratings),\n",
    "            'method': 'baseline_average'\n",
    "        }\n",
    "        \n",
    "        return metrics, {}\n",
    "    \n",
    "    # Prepare data for evaluation\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    binary_predictions = []\n",
    "    binary_actuals = []\n",
    "    \n",
    "    # Tracking for rank-based metrics\n",
    "    rank_metrics = defaultdict(list)\n",
    "    \n",
    "    # Track metrics per user\n",
    "    user_metrics = {}\n",
    "    \n",
    "    # Precompute global stats\n",
    "    global_avg_rating = test_ratings['rating'].mean()\n",
    "    \n",
    "    # Calculate user stats once\n",
    "    user_stats = {}\n",
    "    for user_id in common_users:\n",
    "        user_test = test_ratings[test_ratings['userId'] == user_id]\n",
    "        if len(user_test) > 0:\n",
    "            user_stats[user_id] = {\n",
    "                'avg': user_test['rating'].mean(),\n",
    "                'std': user_test['rating'].std() if len(user_test) > 1 else 0.5,\n",
    "                'count': len(user_test)\n",
    "            }\n",
    "    \n",
    "    # Process each user\n",
    "    for user_id in common_users:\n",
    "        if user_id not in user_preferences['userId'].values:\n",
    "            continue\n",
    "        \n",
    "        user_test_ratings = test_ratings[test_ratings['userId'] == user_id]\n",
    "        \n",
    "        # Get this user's recommendations\n",
    "        user_recs = {}\n",
    "        if user_id in recommendations:\n",
    "            user_recs = dict(recommendations[user_id])\n",
    "        \n",
    "        user_preds = []\n",
    "        user_actuals = []\n",
    "        user_binary_preds = []\n",
    "        user_binary_actuals = []\n",
    "        \n",
    "        # For each test rating, compare with recommendation\n",
    "        for _, row in user_test_ratings.iterrows():\n",
    "            movie_id = row['movieId']\n",
    "            actual_rating = row['rating']\n",
    "            binary_actual = 1 if actual_rating > threshold else 0\n",
    "            \n",
    "            if movie_id in user_recs:\n",
    "                # Use pre-computed recommendation score\n",
    "                predicted_rating = user_recs[movie_id]\n",
    "                binary_prediction = 1 if predicted_rating > (threshold * 4.5 / 5.0 + 0.5) else 0\n",
    "                \n",
    "                user_preds.append(predicted_rating)\n",
    "                user_actuals.append(actual_rating)\n",
    "                user_binary_preds.append(binary_prediction)\n",
    "                user_binary_actuals.append(binary_actual)\n",
    "                \n",
    "                # Store for rank calculations\n",
    "                rank_metrics['movie_ids'].append(movie_id)\n",
    "                rank_metrics['users'].append(user_id)\n",
    "                rank_metrics['actuals'].append(actual_rating)\n",
    "                rank_metrics['predictions'].append(predicted_rating)\n",
    "                \n",
    "                # Find the rank of this movie in the user's recommendations\n",
    "                rec_items = [item[0] for item in recommendations[user_id]]\n",
    "                rank = rec_items.index(movie_id) + 1 if movie_id in rec_items else len(rec_items) + 1\n",
    "                rank_metrics['ranks'].append(rank)\n",
    "                \n",
    "            elif movie_id in movie_features['movieId'].values:\n",
    "                # Generate prediction for this movie\n",
    "                feature_vector = generate_user_movie_features(\n",
    "                    user_id, \n",
    "                    movie_id, \n",
    "                    user_preferences, \n",
    "                    movie_features, \n",
    "                    genre_columns,\n",
    "                    region_columns,\n",
    "                    user_stats=user_stats,\n",
    "                    global_avg_rating=global_avg_rating\n",
    "                )\n",
    "                \n",
    "                if feature_vector is not None:\n",
    "                    # Get probability from model\n",
    "                    like_probability = dnn_model.predict(feature_vector, verbose=0)[0][0]\n",
    "                    \n",
    "                    # Convert to rating-like scale\n",
    "                    predicted_rating = 0.5 + like_probability * 4.5\n",
    "                    \n",
    "                    # Binary prediction\n",
    "                    binary_prediction = 1 if like_probability > 0.5 else 0\n",
    "                    \n",
    "                    user_preds.append(predicted_rating)\n",
    "                    user_actuals.append(actual_rating)\n",
    "                    user_binary_preds.append(binary_prediction)\n",
    "                    user_binary_actuals.append(binary_actual)\n",
    "        \n",
    "        if user_preds:\n",
    "            # Add user predictions to global list\n",
    "            predictions.extend(user_preds)\n",
    "            actuals.extend(user_actuals)\n",
    "            binary_predictions.extend(user_binary_preds)\n",
    "            binary_actuals.extend(user_binary_actuals)\n",
    "            \n",
    "            # Calculate per-user metrics\n",
    "            user_binary_preds_np = np.array(user_binary_preds)\n",
    "            user_binary_actuals_np = np.array(user_binary_actuals)\n",
    "            \n",
    "            # Accuracy\n",
    "            user_accuracy = np.mean(user_binary_preds_np == user_binary_actuals_np)\n",
    "            \n",
    "            # Classification metrics\n",
    "            user_tp = np.sum((user_binary_preds_np == 1) & (user_binary_actuals_np == 1))\n",
    "            user_fp = np.sum((user_binary_preds_np == 1) & (user_binary_actuals_np == 0))\n",
    "            user_tn = np.sum((user_binary_preds_np == 0) & (user_binary_actuals_np == 0))\n",
    "            user_fn = np.sum((user_binary_preds_np == 0) & (user_binary_actuals_np == 1))\n",
    "            \n",
    "            user_precision = user_tp / (user_tp + user_fp) if (user_tp + user_fp) > 0 else 0\n",
    "            user_recall = user_tp / (user_tp + user_fn) if (user_tp + user_fn) > 0 else 0\n",
    "            user_f1 = 2 * user_precision * user_recall / (user_precision + user_recall) if (user_precision + user_recall) > 0 else 0\n",
    "            \n",
    "            # RMSE (on original rating scale)\n",
    "            user_preds_np = np.array(user_preds)\n",
    "            user_actuals_np = np.array(user_actuals)\n",
    "            user_rmse = np.sqrt(np.mean((user_preds_np - user_actuals_np) ** 2))\n",
    "            user_mae = np.mean(np.abs(user_preds_np - user_actuals_np))\n",
    "            \n",
    "            # Store user metrics\n",
    "            user_metrics[user_id] = {\n",
    "                'accuracy': user_accuracy,\n",
    "                'precision': user_precision,\n",
    "                'recall': user_recall,\n",
    "                'f1_score': user_f1,\n",
    "                'rmse': user_rmse,\n",
    "                'mae': user_mae,\n",
    "                'num_predictions': len(user_preds),\n",
    "                'tp': int(user_tp),\n",
    "                'fp': int(user_fp),\n",
    "                'tn': int(user_tn),\n",
    "                'fn': int(user_fn)\n",
    "            }\n",
    "    \n",
    "    if not predictions:\n",
    "        logger.warning(\"No predictions available for evaluation\")\n",
    "        return {\n",
    "            'rmse': 0.0,\n",
    "            'mae': 0.0,\n",
    "            'accuracy': 0.0,\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1_score': 0.0,\n",
    "            'num_predictions': 0\n",
    "        }, {}\n",
    "    \n",
    "    # Convert to numpy arrays for calculations\n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "    binary_predictions = np.array(binary_predictions)\n",
    "    binary_actuals = np.array(binary_actuals)\n",
    "    \n",
    "    # Calculate binary classification metrics\n",
    "    accuracy = np.mean(binary_predictions == binary_actuals)\n",
    "    \n",
    "    # Calculate confusion matrix elements\n",
    "    true_positives = np.sum((binary_predictions == 1) & (binary_actuals == 1))\n",
    "    false_positives = np.sum((binary_predictions == 1) & (binary_actuals == 0))\n",
    "    true_negatives = np.sum((binary_predictions == 0) & (binary_actuals == 0))\n",
    "    false_negatives = np.sum((binary_predictions == 0) & (binary_actuals == 1))\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Calculate RMSE and MAE\n",
    "    rmse = np.sqrt(np.mean((predictions - actuals) ** 2))\n",
    "    mae = np.mean(np.abs(predictions - actuals))\n",
    "    \n",
    "    # Calculate rank-based metrics if available\n",
    "    ndcg = 0.0\n",
    "    map_score = 0.0\n",
    "    mrr = 0.0\n",
    "    \n",
    "    if rank_metrics['ranks']:\n",
    "        # Normalized Discounted Cumulative Gain (NDCG)\n",
    "        # Calculate DCG: sum of (2^relevance - 1) / log2(rank + 1)\n",
    "        # For NDCG, we normalize by the ideal DCG (items sorted by relevance)\n",
    "        relevance = np.array(rank_metrics['actuals']) > threshold  # Convert ratings to binary relevance\n",
    "        ranks = np.array(rank_metrics['ranks'])\n",
    "        \n",
    "        # DCG calculation\n",
    "        dcg = np.sum((2**relevance - 1) / np.log2(ranks + 1))\n",
    "        \n",
    "        # IDCG calculation (sort by relevance)\n",
    "        ideal_ranks = np.argsort(relevance)[::-1] + 1  # Descending order\n",
    "        idcg = np.sum((2**relevance - 1) / np.log2(ideal_ranks + 1))\n",
    "        \n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "        \n",
    "        # Mean Average Precision (MAP)\n",
    "        # For each user, calculate average precision\n",
    "        users = np.array(rank_metrics['users'])\n",
    "        unique_users = np.unique(users)\n",
    "        \n",
    "        avg_precisions = []\n",
    "        reciprocal_ranks = []\n",
    "        \n",
    "        for user in unique_users:\n",
    "            user_indices = np.where(users == user)[0]\n",
    "            user_relevance = relevance[user_indices]\n",
    "            user_ranks = ranks[user_indices]\n",
    "            \n",
    "            # Sort by rank\n",
    "            sort_idx = np.argsort(user_ranks)\n",
    "            user_relevance = user_relevance[sort_idx]\n",
    "            user_ranks = user_ranks[sort_idx]\n",
    "            \n",
    "            # Calculate precision at each relevant position\n",
    "            relevant_positions = np.where(user_relevance == 1)[0]\n",
    "            \n",
    "            if len(relevant_positions) > 0:\n",
    "                # Mean Reciprocal Rank - 1/rank of first relevant item\n",
    "                reciprocal_ranks.append(1.0 / user_ranks[relevant_positions[0]])\n",
    "                \n",
    "                # Average Precision\n",
    "                precision_at_k = []\n",
    "                for k in relevant_positions:\n",
    "                    precision_at_k.append(np.sum(user_relevance[:k+1]) / (k+1))\n",
    "                \n",
    "                avg_precisions.append(np.mean(precision_at_k))\n",
    "        \n",
    "        map_score = np.mean(avg_precisions) if avg_precisions else 0\n",
    "        mrr = np.mean(reciprocal_ranks) if reciprocal_ranks else 0\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'true_positives': int(true_positives),\n",
    "        'false_positives': int(false_positives),\n",
    "        'true_negatives': int(true_negatives),\n",
    "        'false_negatives': int(false_negatives),\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'ndcg': ndcg,\n",
    "        'map': map_score,\n",
    "        'mrr': mrr,\n",
    "        'num_predictions': len(predictions),\n",
    "        'method': 'enhanced_evaluation'\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Evaluation completed:\")\n",
    "    logger.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "    logger.info(f\"Precision: {precision:.4f}\")\n",
    "    logger.info(f\"Recall: {recall:.4f}\")\n",
    "    logger.info(f\"F1 Score: {f1_score:.4f}\")\n",
    "    logger.info(f\"RMSE: {rmse:.4f}\")\n",
    "    logger.info(f\"MAE: {mae:.4f}\")\n",
    "    logger.info(f\"NDCG: {ndcg:.4f}\")\n",
    "    logger.info(f\"MAP: {map_score:.4f}\")\n",
    "    logger.info(f\"MRR: {mrr:.4f}\")\n",
    "    logger.info(f\"Predictions: {len(predictions)}\")\n",
    "    \n",
    "    # Create visualization for confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm = np.array([\n",
    "        [true_negatives, false_positives],\n",
    "        [false_negatives, true_positives]\n",
    "    ])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "               xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
    "               yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(os.path.join(output_path, 'dnn_confusion_matrix.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Create bar chart of metrics\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'NDCG', 'MAP', 'MRR']\n",
    "    metric_values = [accuracy, precision, recall, f1_score, ndcg, map_score, mrr]\n",
    "    \n",
    "    colors = plt.cm.viridis(np.linspace(0, 0.8, len(metric_names)))\n",
    "    bars = plt.bar(metric_names, metric_values, color=colors)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar, value in zip(bars, metric_values):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, \n",
    "                bar.get_height() + 0.02, \n",
    "                f'{value:.3f}', \n",
    "                ha='center', va='bottom', \n",
    "                fontweight='bold')\n",
    "    \n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.title('Evaluation Metrics')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.savefig(os.path.join(output_path, 'dnn_evaluation_metrics.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Create scatter plot of actual vs predicted ratings\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(actuals, predictions, alpha=0.5)\n",
    "    plt.plot([0.5, 5], [0.5, 5], 'r--')  # Perfect prediction line\n",
    "    plt.xlim(0.5, 5)\n",
    "    plt.ylim(0.5, 5)\n",
    "    plt.xlabel('Actual Ratings')\n",
    "    plt.ylabel('Predicted Ratings')\n",
    "    plt.title(f'Actual vs Predicted Ratings (RMSE={rmse:.3f})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(os.path.join(output_path, 'actual_vs_predicted.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return metrics, user_metrics\n",
    "\n",
    "def recommend_for_user(user_id, recommendations, movie_features=None, n=10):\n",
    "    \"\"\"\n",
    "    Display improved recommendations for a user\n",
    "    \"\"\"\n",
    "    if user_id not in recommendations:\n",
    "        logger.warning(f\"No recommendations found for user {user_id}\")\n",
    "        return\n",
    "    \n",
    "    user_recs = recommendations[user_id][:n]\n",
    "    \n",
    "    if not user_recs:\n",
    "        logger.warning(f\"No recommendations found for user {user_id}\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"\\nTop {len(user_recs)} recommendations for user {user_id}:\")\n",
    "    \n",
    "    recs_info = []\n",
    "    \n",
    "    for i, (movie_id, predicted_rating) in enumerate(user_recs, 1):\n",
    "        movie_info = f\"Movie ID: {movie_id}\"\n",
    "        genres = []\n",
    "        \n",
    "        if movie_features is not None:\n",
    "            movie_row = movie_features[movie_features['movieId'] == movie_id]\n",
    "            if not movie_row.empty:\n",
    "                if 'title' in movie_row.columns:\n",
    "                    movie_info = movie_row.iloc[0]['title']\n",
    "                \n",
    "                # Extract genres\n",
    "                genre_columns = [col for col in movie_row.columns if col not in \n",
    "                                ['movieId', 'title', 'tokens', 'token_count', 'top_keywords'] and\n",
    "                                col not in ['North America', 'Europe', 'East Asia', 'South Asia', \n",
    "                                           'Southeast Asia', 'Oceania', 'Middle East', 'Africa', \n",
    "                                           'Latin America', 'Other']]\n",
    "                \n",
    "                genres = [genre for genre in genre_columns if movie_row.iloc[0][genre] == 1]\n",
    "        \n",
    "        genre_str = \", \".join(genres[:3]) + (\", ...\" if len(genres) > 3 else \"\")\n",
    "        logger.info(f\"{i}. {movie_info} - Rating: {predicted_rating:.2f} - Genres: {genre_str}\")\n",
    "        \n",
    "        recs_info.append({\n",
    "            'rank': i,\n",
    "            'movie_id': movie_id,\n",
    "            'title': movie_info,\n",
    "            'predicted_rating': predicted_rating,\n",
    "            'genres': genres\n",
    "        })\n",
    "    \n",
    "    return recs_info\n",
    "\n",
    "# Main execution flow\n",
    "logger.info(\"Starting enhanced DNN-based recommendation pipeline\")\n",
    "log_memory_usage(\"Initial memory usage\")\n",
    "\n",
    "# Load data\n",
    "data = load_data()\n",
    "if data is None:\n",
    "    logger.error(\"Failed to load data\")\n",
    "    exit(1)\n",
    "\n",
    "log_memory_usage(\"After loading data\")\n",
    "\n",
    "# Extract genre and region features\n",
    "movie_features_with_regions, genre_columns, region_columns = extract_genre_and_region_features(data['movie_features'])\n",
    "\n",
    "if movie_features_with_regions is None:\n",
    "    logger.error(\"Failed to extract movie features\")\n",
    "    exit(1)\n",
    "\n",
    "log_memory_usage(\"After feature extraction\")\n",
    "\n",
    "# Calculate user preferences\n",
    "user_preferences = calculate_user_preferences(\n",
    "    data['train_ratings'], \n",
    "    movie_features_with_regions,\n",
    "    genre_columns + region_columns,\n",
    "    threshold_rating\n",
    ")\n",
    "\n",
    "# Save user preferences\n",
    "user_preferences.to_csv(os.path.join(output_path, 'user_genre_preferences.csv'), index=False)\n",
    "logger.info(f\"Saved user preferences for {len(user_preferences)} users\")\n",
    "\n",
    "# Save movie genre features\n",
    "movie_features_with_regions.to_csv(os.path.join(output_path, 'movie_genre_features.csv'), index=False)\n",
    "logger.info(f\"Saved movie genre features for {len(movie_features_with_regions)} movies\")\n",
    "\n",
    "log_memory_usage(\"After user preferences calculation\")\n",
    "\n",
    "# Prepare training data\n",
    "X_train, X_val, y_train, y_val, feature_columns = prepare_dnn_training_data(\n",
    "    data['train_ratings'],\n",
    "    user_preferences,\n",
    "    movie_features_with_regions,\n",
    "    genre_columns,\n",
    "    region_columns,\n",
    "    threshold=threshold_rating\n",
    ")\n",
    "\n",
    "log_memory_usage(\"After training data preparation\")\n",
    "\n",
    "# Build and train the model\n",
    "dnn_model, training_history = build_and_train_dnn_model(\n",
    "    X_train, \n",
    "    X_val, \n",
    "    y_train, \n",
    "    y_val,\n",
    "    learning_rate=dnn_learning_rate,\n",
    "    batch_size=dnn_batch_size,\n",
    "    epochs=dnn_epochs\n",
    ")\n",
    "\n",
    "log_memory_usage(\"After model training\")\n",
    "\n",
    "# Save the trained model\n",
    "dnn_model.save(os.path.join(output_path, 'dnn_model.h5'))\n",
    "logger.info(\"Saved trained DNN model\")\n",
    "\n",
    "# Generate recommendations for users with adaptive batch size\n",
    "dnn_recommendations = generate_recommendations_for_all_users(\n",
    "    dnn_model,\n",
    "    user_preferences,\n",
    "    movie_features_with_regions,\n",
    "    genre_columns,\n",
    "    region_columns,\n",
    "    data['train_ratings'],\n",
    "    top_n,\n",
    "    batch_size=50\n",
    ")\n",
    "\n",
    "log_memory_usage(\"After generating recommendations\")\n",
    "\n",
    "# Save recommendations\n",
    "with open(os.path.join(output_path, 'dnn_recommendations.pkl'), 'wb') as f:\n",
    "    pickle.dump(dnn_recommendations, f)\n",
    "logger.info(f\"Saved recommendations for {len(dnn_recommendations)} users\")\n",
    "\n",
    "# Also save in CSV format for easier inspection\n",
    "recommendations_list = []\n",
    "for user_id, recs in dnn_recommendations.items():\n",
    "    for rank, (movie_id, score) in enumerate(recs, 1):\n",
    "        movie_title = \"Unknown\"\n",
    "        genres = []\n",
    "        \n",
    "        if 'movie_features' in data:\n",
    "            movie_row = data['movie_features'][data['movie_features']['movieId'] == movie_id]\n",
    "            if not movie_row.empty and 'title' in movie_row.columns:\n",
    "                movie_title = movie_row.iloc[0]['title']\n",
    "                \n",
    "                # Extract genres\n",
    "                genre_cols = [col for col in movie_row.columns if col not in \n",
    "                            ['movieId', 'title', 'tokens', 'token_count', 'top_keywords'] and\n",
    "                            col not in ['North America', 'Europe', 'East Asia', 'South Asia', \n",
    "                                       'Southeast Asia', 'Oceania', 'Middle East', 'Africa', \n",
    "                                       'Latin America', 'Other']]\n",
    "                \n",
    "                genres = [genre for genre in genre_cols if movie_row.iloc[0][genre] == 1]\n",
    "        \n",
    "        recommendations_list.append({\n",
    "            'userId': user_id,\n",
    "            'movieId': movie_id,\n",
    "            'title': movie_title,\n",
    "            'rank': rank,\n",
    "            'predicted_rating': score,\n",
    "            'genres': '|'.join(genres)\n",
    "        })\n",
    "\n",
    "# Save recommendations to CSV in chunks\n",
    "if recommendations_list:\n",
    "    chunk_size = 10000\n",
    "    recommendations_df = pd.DataFrame(recommendations_list)\n",
    "    \n",
    "    # Save in chunks to avoid memory issues with large datasets\n",
    "    for i in range(0, len(recommendations_df), chunk_size):\n",
    "        chunk = recommendations_df.iloc[i:i+chunk_size]\n",
    "        \n",
    "        if i == 0:\n",
    "            chunk.to_csv(os.path.join(output_path, 'dnn_recommendations.csv'), index=False, mode='w')\n",
    "        else:\n",
    "            chunk.to_csv(os.path.join(output_path, 'dnn_recommendations.csv'), index=False, mode='a', header=False)\n",
    "            \n",
    "    logger.info(f\"Saved {len(recommendations_df)} recommendations to CSV\")\n",
    "\n",
    "log_memory_usage(\"After saving recommendations\")\n",
    "\n",
    "# Evaluate the recommendations\n",
    "logger.info(\"Evaluating DNN recommendations with enhanced metrics\")\n",
    "evaluation_metrics, user_metrics = evaluate_recommendations(\n",
    "    dnn_recommendations,\n",
    "    data['test_ratings'],\n",
    "    dnn_model,\n",
    "    user_preferences,\n",
    "    movie_features_with_regions,\n",
    "    genre_columns,\n",
    "    region_columns,\n",
    "    threshold=threshold_rating\n",
    ")\n",
    "\n",
    "log_memory_usage(\"After evaluation\")\n",
    "\n",
    "# Save evaluation results\n",
    "if evaluation_metrics:\n",
    "    evaluation_results = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_results.to_csv(os.path.join(output_path, 'dnn_evaluation.csv'), index=False)\n",
    "    logger.info(\"Saved evaluation metrics\")\n",
    "    \n",
    "    # Save per-user metrics\n",
    "    user_metrics_df = pd.DataFrame.from_dict(user_metrics, orient='index')\n",
    "    user_metrics_df.reset_index(inplace=True)\n",
    "    user_metrics_df.rename(columns={'index': 'userId'}, inplace=True)\n",
    "    user_metrics_df.to_csv(os.path.join(output_path, 'dnn_user_metrics.csv'), index=False)\n",
    "    logger.info(f\"Saved per-user metrics for {len(user_metrics)} users\")\n",
    "    \n",
    "    # Analyze user metrics by user rating count\n",
    "    if 'train_ratings' in data and len(user_metrics_df) > 0:\n",
    "        # Get rating counts for each user\n",
    "        user_rating_counts = data['train_ratings'].groupby('userId').size().reset_index(name='rating_count')\n",
    "        \n",
    "        # Merge with user metrics\n",
    "        user_analysis = pd.merge(user_metrics_df, user_rating_counts, on='userId', how='left')\n",
    "        \n",
    "        # Create rating count bins\n",
    "        user_analysis['rating_count_bin'] = pd.cut(\n",
    "            user_analysis['rating_count'], \n",
    "            bins=[0, 10, 25, 50, 100, float('inf')],\n",
    "            labels=['1-10', '11-25', '26-50', '51-100', '100+']\n",
    "        )\n",
    "        \n",
    "        # Group by rating count bin and calculate average metrics\n",
    "        metrics_by_count = user_analysis.groupby('rating_count_bin').agg({\n",
    "            'rmse': 'mean',\n",
    "            'mae': 'mean',\n",
    "            'accuracy': 'mean',\n",
    "            'precision': 'mean',\n",
    "            'recall': 'mean',\n",
    "            'f1_score': 'mean',\n",
    "            'userId': 'count'\n",
    "        }).reset_index()\n",
    "        \n",
    "        metrics_by_count.rename(columns={'userId': 'num_users'}, inplace=True)\n",
    "        \n",
    "        # Save to CSV\n",
    "        metrics_by_count.to_csv(os.path.join(output_path, 'metrics_by_rating_count.csv'), index=False)\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        metrics = ['rmse', 'accuracy', 'precision', 'recall', 'f1_score']\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(metrics)))\n",
    "        \n",
    "        for i, metric in enumerate(metrics):\n",
    "            plt.subplot(len(metrics), 1, i+1)\n",
    "            plt.bar(metrics_by_count['rating_count_bin'], \n",
    "                   metrics_by_count[metric], \n",
    "                   color=colors[i],\n",
    "                   alpha=0.7)\n",
    "            \n",
    "            # Add value labels\n",
    "            for j, v in enumerate(metrics_by_count[metric]):\n",
    "                plt.text(j, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "                \n",
    "            # Add user counts as text\n",
    "            if i == 0:\n",
    "                for j, count in enumerate(metrics_by_count['num_users']):\n",
    "                    plt.text(j, metrics_by_count[metric].max() * 0.8, \n",
    "                            f'n={count}', ha='center', \n",
    "                            bbox=dict(facecolor='white', alpha=0.5))\n",
    "            \n",
    "            plt.title(f'{metric.upper()} by User Rating Count')\n",
    "            plt.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            # For RMSE, lower is better\n",
    "            if metric == 'rmse':\n",
    "                plt.ylim(0, min(2.0, metrics_by_count[metric].max() * 1.2))\n",
    "            else:\n",
    "                plt.ylim(0, 1.0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_path, 'metrics_by_rating_count.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        logger.info(\"Created metrics analysis by user rating count\")\n",
    "\n",
    "# Show sample recommendations\n",
    "logger.info(\"\\nSample recommendation for exploration:\")\n",
    "if dnn_recommendations:\n",
    "    # Pick a random user with recommendations\n",
    "    sample_user_id = np.random.choice(list(dnn_recommendations.keys()))\n",
    "    \n",
    "    if sample_user_id in user_preferences['userId'].values:\n",
    "        user_prefs = user_preferences[user_preferences['userId'] == sample_user_id].iloc[0]\n",
    "        genre_pref_columns = [col for col in user_preferences.columns if col in genre_columns]\n",
    "        \n",
    "        logger.info(f\"\\nUser {sample_user_id} Preferences:\")\n",
    "        user_prefs_list = [(genre, user_prefs[genre]) for genre in genre_pref_columns if user_prefs[genre] != 0]\n",
    "        liked_genres = sorted(user_prefs_list, key=lambda x: x[1], reverse=True)[:3]\n",
    "        disliked_genres = sorted(user_prefs_list, key=lambda x: x[1])[:3]\n",
    "        \n",
    "        logger.info(f\"- Most liked genres: {', '.join([f'{g} ({v:.2f})' for g, v in liked_genres if v > 0])}\")\n",
    "        logger.info(f\"- Most disliked genres: {', '.join([f'{g} ({v:.2f})' for g, v in disliked_genres if v < 0])}\")\n",
    "        \n",
    "        # Show region preferences if available\n",
    "        if region_columns:\n",
    "            region_pref_columns = [col for col in user_preferences.columns if col in region_columns]\n",
    "            region_prefs_list = [(region, user_prefs[region]) for region in region_pref_columns if user_prefs[region] != 0]\n",
    "            liked_regions = sorted(region_prefs_list, key=lambda x: x[1], reverse=True)[:3]\n",
    "            \n",
    "            if liked_regions and liked_regions[0][1] > 0:\n",
    "                logger.info(f\"- Preferred regions: {', '.join([f'{r} ({v:.2f})' for r, v in liked_regions if v > 0])}\")\n",
    "    \n",
    "    # Show the recommendations\n",
    "    recommend_for_user(sample_user_id, dnn_recommendations, data['movie_features'])\n",
    "\n",
    "    # Look up this user in the metrics to see how we did\n",
    "    if user_metrics and sample_user_id in user_metrics:\n",
    "        user_metric = user_metrics[sample_user_id]\n",
    "        logger.info(f\"\\nEvaluation metrics for user {sample_user_id}:\")\n",
    "        logger.info(f\"- RMSE: {user_metric['rmse']:.4f}\")\n",
    "        logger.info(f\"- Accuracy: {user_metric['accuracy']:.4f}\")\n",
    "        logger.info(f\"- F1 Score: {user_metric['f1_score']:.4f}\")\n",
    "        logger.info(f\"- Number of predictions: {user_metric['num_predictions']}\")\n",
    "\n",
    "log_memory_usage(\"Final memory usage\")\n",
    "logger.info(\"Enhanced DNN-based recommendation pipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
