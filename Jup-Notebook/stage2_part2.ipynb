{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COLLABORATIVE FILTERING WITH DEEP NEURAL NETWORK\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 1: DATA LOADING\n",
      "================================================================================\n",
      "Loading processed data from stage1.py...\n",
      "Loaded features for 24378 movies\n",
      "Loaded 14472091 normalized ratings\n",
      "Split ratings into 11530404 training and 2941687 testing samples\n",
      "\n",
      "--------------------------------------------------\n",
      "DATA ANALYSIS: LOADED DATASETS\n",
      "--------------------------------------------------\n",
      "\n",
      "Movie Features Summary:\n",
      "- Total movies: 24378\n",
      "- Number of genres: 33\n",
      "- Genre columns: ['(no genres listed)', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'IMAX', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western', 'Central Asia', 'East Asia', 'Eastern Europe', 'Latin America/Caribbean', 'Middle East', 'North Africa', 'North America', 'Oceania', 'Other', 'South Asia', 'Southeast Asia', 'Sub-Saharan Africa', 'Western Europe']\n",
      "\n",
      "Sample movie features:\n",
      "   movieId                    title  (no genres listed)  Action  Adventure\n",
      "0        1         Toy Story (1995)                   0       0          1\n",
      "1        2           Jumanji (1995)                   0       0          1\n",
      "2        3  Grumpier Old Men (1995)                   0       0          0\n",
      "\n",
      "Ratings Summary:\n",
      "- Total ratings: 14472091\n",
      "- Unique users: 100000\n",
      "- Unique movies: 24829\n",
      "- Rating range: 0.5 - 5.0\n",
      "- Average rating: 3.53\n",
      "\n",
      "Rating distribution plot saved to ../rec/collaborative-recommendations\\rating_distribution.png\n",
      "\n",
      "Train/Test Split Summary:\n",
      "- Training ratings: 11530404 (79.7%)\n",
      "- Testing ratings: 2941687 (20.3%)\n",
      "- Training users: 80000\n",
      "- Testing users: 20000\n",
      "\n",
      "Ratings per user:\n",
      "- Training set - Avg: 144.13, Min: 20, Max: 7515\n",
      "- Testing set - Avg: 147.08, Min: 20, Max: 4236\n",
      "\n",
      "================================================================================\n",
      "STEP 2: MOVIE GENRE FEATURE EXTRACTION\n",
      "================================================================================\n",
      "Extracting genre features for movies...\n",
      "Extracted 33 genre features for 24378 movies\n",
      "\n",
      "--------------------------------------------------\n",
      "DATA ANALYSIS: GENRE FEATURES\n",
      "--------------------------------------------------\n",
      "\n",
      "Genre Distribution:\n",
      "- (no genres listed): 125 movies (0.5%)\n",
      "- Action: 3241 movies (13.3%)\n",
      "- Adventure: 2129 movies (8.7%)\n",
      "- Animation: 914 movies (3.7%)\n",
      "- Children: 1057 movies (4.3%)\n",
      "- Comedy: 7665 movies (31.4%)\n",
      "- Crime: 2674 movies (11.0%)\n",
      "- Documentary: 1991 movies (8.2%)\n",
      "- Drama: 12034 movies (49.4%)\n",
      "- Fantasy: 1301 movies (5.3%)\n",
      "- Film-Noir: 302 movies (1.2%)\n",
      "- Horror: 2434 movies (10.0%)\n",
      "- IMAX: 193 movies (0.8%)\n",
      "- Musical: 928 movies (3.8%)\n",
      "- Mystery: 1367 movies (5.6%)\n",
      "- Romance: 3773 movies (15.5%)\n",
      "- Sci-Fi: 1608 movies (6.6%)\n",
      "- Thriller: 3897 movies (16.0%)\n",
      "- War: 1096 movies (4.5%)\n",
      "- Western: 577 movies (2.4%)\n",
      "- Central Asia: 20 movies (0.1%)\n",
      "- East Asia: 1643 movies (6.7%)\n",
      "- Eastern Europe: 851 movies (3.5%)\n",
      "- Latin America/Caribbean: 361 movies (1.5%)\n",
      "- Middle East: 273 movies (1.1%)\n",
      "- North Africa: 55 movies (0.2%)\n",
      "- North America: 15600 movies (64.0%)\n",
      "- Oceania: 447 movies (1.8%)\n",
      "- Other: 1 movies (0.0%)\n",
      "- South Asia: 276 movies (1.1%)\n",
      "- Southeast Asia: 123 movies (0.5%)\n",
      "- Sub-Saharan Africa: 133 movies (0.5%)\n",
      "- Western Europe: 8007 movies (32.8%)\n",
      "\n",
      "Genre distribution plot saved to ../rec/collaborative-recommendations\\genre_distribution.png\n",
      "\n",
      "Genre Co-occurrence Analysis:\n",
      "Most common genre combinations:\n",
      "- (no genres listed) most commonly appears with: Eastern Europe (0.01), Southeast Asia (0.01), Western Europe (0.01), Middle East (0.01), South Asia (0.01)\n",
      "- Action most commonly appears with: IMAX (0.52), Adventure (0.42), Sci-Fi (0.37), East Asia (0.36), Southeast Asia (0.34)\n",
      "- Adventure most commonly appears with: IMAX (0.45), Children (0.40), Animation (0.37), Fantasy (0.36), Action (0.28)\n",
      "- Animation most commonly appears with: Children (0.42), Fantasy (0.19), IMAX (0.18), Adventure (0.16), East Asia (0.13)\n",
      "- Children most commonly appears with: Animation (0.48), Fantasy (0.23), Adventure (0.20), IMAX (0.16), Musical (0.14)\n",
      "\n",
      "Genre co-occurrence matrix saved to ../rec/collaborative-recommendations\\genre_co_occurrence.png\n",
      "\n",
      "Sample of movie genre features:\n",
      "   movieId  (no genres listed)  Action  Adventure  Animation  Children  \\\n",
      "0        1                   0       0          1          1         1   \n",
      "1        2                   0       0          1          0         1   \n",
      "2        3                   0       0          0          0         0   \n",
      "\n",
      "   Comedy  Crime  Documentary  Drama  ...  Latin America/Caribbean  \\\n",
      "0       1      0            0      0  ...                        0   \n",
      "1       0      0            0      0  ...                        0   \n",
      "2       1      0            0      0  ...                        0   \n",
      "\n",
      "   Middle East  North Africa  North America  Oceania  Other  South Asia  \\\n",
      "0            0             0              1        0      0           0   \n",
      "1            0             0              1        0      0           0   \n",
      "2            0             0              1        0      0           0   \n",
      "\n",
      "   Southeast Asia  Sub-Saharan Africa  Western Europe  \n",
      "0               0                   0               0  \n",
      "1               0                   0               0  \n",
      "2               0                   0               0  \n",
      "\n",
      "[3 rows x 34 columns]\n",
      "\n",
      "Saved movie genre features to ../rec/collaborative-recommendations\\movie_genre_features.csv\n",
      "\n",
      "================================================================================\n",
      "STEP 3: USER GENRE PREFERENCE CALCULATION\n",
      "================================================================================\n",
      "Calculating user preferences for movie genres...\n",
      "Processed 100/80000 users (0.1%)\n",
      "Processed 200/80000 users (0.2%)\n",
      "Processed 300/80000 users (0.4%)\n",
      "Processed 400/80000 users (0.5%)\n",
      "Processed 500/80000 users (0.6%)\n",
      "Processed 600/80000 users (0.8%)\n",
      "Processed 700/80000 users (0.9%)\n",
      "Processed 800/80000 users (1.0%)\n",
      "Processed 900/80000 users (1.1%)\n",
      "Processed 1000/80000 users (1.2%)\n",
      "Processed 1100/80000 users (1.4%)\n",
      "Processed 1200/80000 users (1.5%)\n",
      "Processed 1300/80000 users (1.6%)\n",
      "Processed 1400/80000 users (1.8%)\n",
      "Processed 1500/80000 users (1.9%)\n",
      "Processed 1600/80000 users (2.0%)\n",
      "Processed 1700/80000 users (2.1%)\n",
      "Processed 1800/80000 users (2.2%)\n",
      "Processed 1900/80000 users (2.4%)\n",
      "Processed 2000/80000 users (2.5%)\n",
      "Processed 2100/80000 users (2.6%)\n",
      "Processed 2200/80000 users (2.8%)\n",
      "Processed 2300/80000 users (2.9%)\n",
      "Processed 2400/80000 users (3.0%)\n",
      "Processed 2500/80000 users (3.1%)\n",
      "Processed 2600/80000 users (3.2%)\n",
      "Processed 2700/80000 users (3.4%)\n",
      "Processed 2800/80000 users (3.5%)\n",
      "Processed 2900/80000 users (3.6%)\n",
      "Processed 3000/80000 users (3.8%)\n",
      "Processed 3100/80000 users (3.9%)\n",
      "Processed 3200/80000 users (4.0%)\n",
      "Processed 3300/80000 users (4.1%)\n",
      "Processed 3400/80000 users (4.2%)\n",
      "Processed 3500/80000 users (4.4%)\n",
      "Processed 3600/80000 users (4.5%)\n",
      "Processed 3700/80000 users (4.6%)\n",
      "Processed 3800/80000 users (4.8%)\n",
      "Processed 3900/80000 users (4.9%)\n",
      "Processed 4000/80000 users (5.0%)\n",
      "Processed 4100/80000 users (5.1%)\n",
      "Processed 4200/80000 users (5.2%)\n",
      "Processed 4300/80000 users (5.4%)\n",
      "Processed 4400/80000 users (5.5%)\n",
      "Processed 4500/80000 users (5.6%)\n",
      "Processed 4600/80000 users (5.8%)\n",
      "Processed 4700/80000 users (5.9%)\n",
      "Processed 4800/80000 users (6.0%)\n",
      "Processed 4900/80000 users (6.1%)\n",
      "Processed 5000/80000 users (6.2%)\n",
      "Processed 5100/80000 users (6.4%)\n",
      "Processed 5200/80000 users (6.5%)\n",
      "Processed 5300/80000 users (6.6%)\n",
      "Processed 5400/80000 users (6.8%)\n",
      "Processed 5500/80000 users (6.9%)\n",
      "Processed 5600/80000 users (7.0%)\n",
      "Processed 5700/80000 users (7.1%)\n",
      "Processed 5800/80000 users (7.2%)\n",
      "Processed 5900/80000 users (7.4%)\n",
      "Processed 6000/80000 users (7.5%)\n",
      "Processed 6100/80000 users (7.6%)\n",
      "Processed 6200/80000 users (7.8%)\n",
      "Processed 6300/80000 users (7.9%)\n",
      "Processed 6400/80000 users (8.0%)\n",
      "Processed 6500/80000 users (8.1%)\n",
      "Processed 6600/80000 users (8.2%)\n",
      "Processed 6700/80000 users (8.4%)\n",
      "Processed 6800/80000 users (8.5%)\n",
      "Processed 6900/80000 users (8.6%)\n",
      "Processed 7000/80000 users (8.8%)\n",
      "Processed 7100/80000 users (8.9%)\n",
      "Processed 7200/80000 users (9.0%)\n",
      "Processed 7300/80000 users (9.1%)\n",
      "Processed 7400/80000 users (9.2%)\n",
      "Processed 7500/80000 users (9.4%)\n",
      "Processed 7600/80000 users (9.5%)\n",
      "Processed 7700/80000 users (9.6%)\n",
      "Processed 7800/80000 users (9.8%)\n",
      "Processed 7900/80000 users (9.9%)\n",
      "Processed 8000/80000 users (10.0%)\n",
      "Processed 8100/80000 users (10.1%)\n",
      "Processed 8200/80000 users (10.2%)\n",
      "Processed 8300/80000 users (10.4%)\n",
      "Processed 8400/80000 users (10.5%)\n",
      "Processed 8500/80000 users (10.6%)\n",
      "Processed 8600/80000 users (10.8%)\n",
      "Processed 8700/80000 users (10.9%)\n",
      "Processed 8800/80000 users (11.0%)\n",
      "Processed 8900/80000 users (11.1%)\n",
      "Processed 9000/80000 users (11.2%)\n",
      "Processed 9100/80000 users (11.4%)\n",
      "Processed 9200/80000 users (11.5%)\n",
      "Processed 9300/80000 users (11.6%)\n",
      "Processed 9400/80000 users (11.8%)\n",
      "Processed 9500/80000 users (11.9%)\n",
      "Processed 9600/80000 users (12.0%)\n",
      "Processed 9700/80000 users (12.1%)\n",
      "Processed 9800/80000 users (12.2%)\n",
      "Processed 9900/80000 users (12.4%)\n",
      "Processed 10000/80000 users (12.5%)\n",
      "Processed 10100/80000 users (12.6%)\n",
      "Processed 10200/80000 users (12.8%)\n",
      "Processed 10300/80000 users (12.9%)\n",
      "Processed 10400/80000 users (13.0%)\n",
      "Processed 10500/80000 users (13.1%)\n",
      "Processed 10600/80000 users (13.2%)\n",
      "Processed 10700/80000 users (13.4%)\n",
      "Processed 10800/80000 users (13.5%)\n",
      "Processed 10900/80000 users (13.6%)\n",
      "Processed 11000/80000 users (13.8%)\n",
      "Processed 11100/80000 users (13.9%)\n",
      "Processed 11200/80000 users (14.0%)\n",
      "Processed 11300/80000 users (14.1%)\n",
      "Processed 11400/80000 users (14.2%)\n",
      "Processed 11500/80000 users (14.4%)\n",
      "Processed 11600/80000 users (14.5%)\n",
      "Processed 11700/80000 users (14.6%)\n",
      "Processed 11800/80000 users (14.8%)\n",
      "Processed 11900/80000 users (14.9%)\n",
      "Processed 12000/80000 users (15.0%)\n",
      "Processed 12100/80000 users (15.1%)\n",
      "Processed 12200/80000 users (15.2%)\n",
      "Processed 12300/80000 users (15.4%)\n",
      "Processed 12400/80000 users (15.5%)\n",
      "Processed 12500/80000 users (15.6%)\n",
      "Processed 12600/80000 users (15.8%)\n",
      "Processed 12700/80000 users (15.9%)\n",
      "Processed 12800/80000 users (16.0%)\n",
      "Processed 12900/80000 users (16.1%)\n",
      "Processed 13000/80000 users (16.2%)\n",
      "Processed 13100/80000 users (16.4%)\n",
      "Processed 13200/80000 users (16.5%)\n",
      "Processed 13300/80000 users (16.6%)\n",
      "Processed 13400/80000 users (16.8%)\n",
      "Processed 13500/80000 users (16.9%)\n",
      "Processed 13600/80000 users (17.0%)\n",
      "Processed 13700/80000 users (17.1%)\n",
      "Processed 13800/80000 users (17.2%)\n",
      "Processed 13900/80000 users (17.4%)\n",
      "Processed 14000/80000 users (17.5%)\n",
      "Processed 14100/80000 users (17.6%)\n",
      "Processed 14200/80000 users (17.8%)\n",
      "Processed 14300/80000 users (17.9%)\n",
      "Processed 14400/80000 users (18.0%)\n",
      "Processed 14500/80000 users (18.1%)\n",
      "Processed 14600/80000 users (18.2%)\n",
      "Processed 14700/80000 users (18.4%)\n",
      "Processed 14800/80000 users (18.5%)\n",
      "Processed 14900/80000 users (18.6%)\n",
      "Processed 15000/80000 users (18.8%)\n",
      "Processed 15100/80000 users (18.9%)\n",
      "Processed 15200/80000 users (19.0%)\n",
      "Processed 15300/80000 users (19.1%)\n",
      "Processed 15400/80000 users (19.2%)\n",
      "Processed 15500/80000 users (19.4%)\n",
      "Processed 15600/80000 users (19.5%)\n",
      "Processed 15700/80000 users (19.6%)\n",
      "Processed 15800/80000 users (19.8%)\n",
      "Processed 15900/80000 users (19.9%)\n",
      "Processed 16000/80000 users (20.0%)\n",
      "Processed 16100/80000 users (20.1%)\n",
      "Processed 16200/80000 users (20.2%)\n",
      "Processed 16300/80000 users (20.4%)\n",
      "Processed 16400/80000 users (20.5%)\n",
      "Processed 16500/80000 users (20.6%)\n",
      "Processed 16600/80000 users (20.8%)\n",
      "Processed 16700/80000 users (20.9%)\n",
      "Processed 16800/80000 users (21.0%)\n",
      "Processed 16900/80000 users (21.1%)\n",
      "Processed 17000/80000 users (21.2%)\n",
      "Processed 17100/80000 users (21.4%)\n",
      "Processed 17200/80000 users (21.5%)\n",
      "Processed 17300/80000 users (21.6%)\n",
      "Processed 17400/80000 users (21.8%)\n",
      "Processed 17500/80000 users (21.9%)\n",
      "Processed 17600/80000 users (22.0%)\n",
      "Processed 17700/80000 users (22.1%)\n",
      "Processed 17800/80000 users (22.2%)\n",
      "Processed 17900/80000 users (22.4%)\n",
      "Processed 18000/80000 users (22.5%)\n",
      "Processed 18100/80000 users (22.6%)\n",
      "Processed 18200/80000 users (22.8%)\n",
      "Processed 18300/80000 users (22.9%)\n",
      "Processed 18400/80000 users (23.0%)\n",
      "Processed 18500/80000 users (23.1%)\n",
      "Processed 18600/80000 users (23.2%)\n",
      "Processed 18700/80000 users (23.4%)\n",
      "Processed 18800/80000 users (23.5%)\n",
      "Processed 18900/80000 users (23.6%)\n",
      "Processed 19000/80000 users (23.8%)\n",
      "Processed 19100/80000 users (23.9%)\n",
      "Processed 19200/80000 users (24.0%)\n",
      "Processed 19300/80000 users (24.1%)\n",
      "Processed 19400/80000 users (24.2%)\n",
      "Processed 19500/80000 users (24.4%)\n",
      "Processed 19600/80000 users (24.5%)\n",
      "Processed 19700/80000 users (24.6%)\n",
      "Processed 19800/80000 users (24.8%)\n",
      "Processed 19900/80000 users (24.9%)\n",
      "Processed 20000/80000 users (25.0%)\n",
      "Processed 20100/80000 users (25.1%)\n",
      "Processed 20200/80000 users (25.2%)\n",
      "Processed 20300/80000 users (25.4%)\n",
      "Processed 20400/80000 users (25.5%)\n",
      "Processed 20500/80000 users (25.6%)\n",
      "Processed 20600/80000 users (25.8%)\n",
      "Processed 20700/80000 users (25.9%)\n",
      "Processed 20800/80000 users (26.0%)\n",
      "Processed 20900/80000 users (26.1%)\n",
      "Processed 21000/80000 users (26.2%)\n",
      "Processed 21100/80000 users (26.4%)\n",
      "Processed 21200/80000 users (26.5%)\n",
      "Processed 21300/80000 users (26.6%)\n",
      "Processed 21400/80000 users (26.8%)\n",
      "Processed 21500/80000 users (26.9%)\n",
      "Processed 21600/80000 users (27.0%)\n",
      "Processed 21700/80000 users (27.1%)\n",
      "Processed 21800/80000 users (27.3%)\n",
      "Processed 21900/80000 users (27.4%)\n",
      "Processed 22000/80000 users (27.5%)\n",
      "Processed 22100/80000 users (27.6%)\n",
      "Processed 22200/80000 users (27.8%)\n",
      "Processed 22300/80000 users (27.9%)\n",
      "Processed 22400/80000 users (28.0%)\n",
      "Processed 22500/80000 users (28.1%)\n",
      "Processed 22600/80000 users (28.2%)\n",
      "Processed 22700/80000 users (28.4%)\n",
      "Processed 22800/80000 users (28.5%)\n",
      "Processed 22900/80000 users (28.6%)\n",
      "Processed 23000/80000 users (28.7%)\n",
      "Processed 23100/80000 users (28.9%)\n",
      "Processed 23200/80000 users (29.0%)\n",
      "Processed 23300/80000 users (29.1%)\n",
      "Processed 23400/80000 users (29.2%)\n",
      "Processed 23500/80000 users (29.4%)\n",
      "Processed 23600/80000 users (29.5%)\n",
      "Processed 23700/80000 users (29.6%)\n",
      "Processed 23800/80000 users (29.8%)\n",
      "Processed 23900/80000 users (29.9%)\n",
      "Processed 24000/80000 users (30.0%)\n",
      "Processed 24100/80000 users (30.1%)\n",
      "Processed 24200/80000 users (30.2%)\n",
      "Processed 24300/80000 users (30.4%)\n",
      "Processed 24400/80000 users (30.5%)\n",
      "Processed 24500/80000 users (30.6%)\n",
      "Processed 24600/80000 users (30.8%)\n",
      "Processed 24700/80000 users (30.9%)\n",
      "Processed 24800/80000 users (31.0%)\n",
      "Processed 24900/80000 users (31.1%)\n",
      "Processed 25000/80000 users (31.2%)\n",
      "Processed 25100/80000 users (31.4%)\n",
      "Processed 25200/80000 users (31.5%)\n",
      "Processed 25300/80000 users (31.6%)\n",
      "Processed 25400/80000 users (31.8%)\n",
      "Processed 25500/80000 users (31.9%)\n",
      "Processed 25600/80000 users (32.0%)\n",
      "Processed 25700/80000 users (32.1%)\n",
      "Processed 25800/80000 users (32.2%)\n",
      "Processed 25900/80000 users (32.4%)\n",
      "Processed 26000/80000 users (32.5%)\n",
      "Processed 26100/80000 users (32.6%)\n",
      "Processed 26200/80000 users (32.8%)\n",
      "Processed 26300/80000 users (32.9%)\n",
      "Processed 26400/80000 users (33.0%)\n",
      "Processed 26500/80000 users (33.1%)\n",
      "Processed 26600/80000 users (33.2%)\n",
      "Processed 26700/80000 users (33.4%)\n",
      "Processed 26800/80000 users (33.5%)\n",
      "Processed 26900/80000 users (33.6%)\n",
      "Processed 27000/80000 users (33.8%)\n",
      "Processed 27100/80000 users (33.9%)\n",
      "Processed 27200/80000 users (34.0%)\n",
      "Processed 27300/80000 users (34.1%)\n",
      "Processed 27400/80000 users (34.2%)\n",
      "Processed 27500/80000 users (34.4%)\n",
      "Processed 27600/80000 users (34.5%)\n",
      "Processed 27700/80000 users (34.6%)\n",
      "Processed 27800/80000 users (34.8%)\n",
      "Processed 27900/80000 users (34.9%)\n",
      "Processed 28000/80000 users (35.0%)\n",
      "Processed 28100/80000 users (35.1%)\n",
      "Processed 28200/80000 users (35.2%)\n",
      "Processed 28300/80000 users (35.4%)\n",
      "Processed 28400/80000 users (35.5%)\n",
      "Processed 28500/80000 users (35.6%)\n",
      "Processed 28600/80000 users (35.8%)\n",
      "Processed 28700/80000 users (35.9%)\n",
      "Processed 28800/80000 users (36.0%)\n",
      "Processed 28900/80000 users (36.1%)\n",
      "Processed 29000/80000 users (36.2%)\n",
      "Processed 29100/80000 users (36.4%)\n",
      "Processed 29200/80000 users (36.5%)\n",
      "Processed 29300/80000 users (36.6%)\n",
      "Processed 29400/80000 users (36.8%)\n",
      "Processed 29500/80000 users (36.9%)\n",
      "Processed 29600/80000 users (37.0%)\n",
      "Processed 29700/80000 users (37.1%)\n",
      "Processed 29800/80000 users (37.2%)\n",
      "Processed 29900/80000 users (37.4%)\n",
      "Processed 30000/80000 users (37.5%)\n",
      "Processed 30100/80000 users (37.6%)\n",
      "Processed 30200/80000 users (37.8%)\n",
      "Processed 30300/80000 users (37.9%)\n",
      "Processed 30400/80000 users (38.0%)\n",
      "Processed 30500/80000 users (38.1%)\n",
      "Processed 30600/80000 users (38.2%)\n",
      "Processed 30700/80000 users (38.4%)\n",
      "Processed 30800/80000 users (38.5%)\n",
      "Processed 30900/80000 users (38.6%)\n",
      "Processed 31000/80000 users (38.8%)\n",
      "Processed 31100/80000 users (38.9%)\n",
      "Processed 31200/80000 users (39.0%)\n",
      "Processed 31300/80000 users (39.1%)\n",
      "Processed 31400/80000 users (39.2%)\n",
      "Processed 31500/80000 users (39.4%)\n",
      "Processed 31600/80000 users (39.5%)\n",
      "Processed 31700/80000 users (39.6%)\n",
      "Processed 31800/80000 users (39.8%)\n",
      "Processed 31900/80000 users (39.9%)\n",
      "Processed 32000/80000 users (40.0%)\n",
      "Processed 32100/80000 users (40.1%)\n",
      "Processed 32200/80000 users (40.2%)\n",
      "Processed 32300/80000 users (40.4%)\n",
      "Processed 32400/80000 users (40.5%)\n",
      "Processed 32500/80000 users (40.6%)\n",
      "Processed 32600/80000 users (40.8%)\n",
      "Processed 32700/80000 users (40.9%)\n",
      "Processed 32800/80000 users (41.0%)\n",
      "Processed 32900/80000 users (41.1%)\n",
      "Processed 33000/80000 users (41.2%)\n",
      "Processed 33100/80000 users (41.4%)\n",
      "Processed 33200/80000 users (41.5%)\n",
      "Processed 33300/80000 users (41.6%)\n",
      "Processed 33400/80000 users (41.8%)\n",
      "Processed 33500/80000 users (41.9%)\n",
      "Processed 33600/80000 users (42.0%)\n",
      "Processed 33700/80000 users (42.1%)\n",
      "Processed 33800/80000 users (42.2%)\n",
      "Processed 33900/80000 users (42.4%)\n",
      "Processed 34000/80000 users (42.5%)\n",
      "Processed 34100/80000 users (42.6%)\n",
      "Processed 34200/80000 users (42.8%)\n",
      "Processed 34300/80000 users (42.9%)\n",
      "Processed 34400/80000 users (43.0%)\n",
      "Processed 34500/80000 users (43.1%)\n",
      "Processed 34600/80000 users (43.2%)\n",
      "Processed 34700/80000 users (43.4%)\n",
      "Processed 34800/80000 users (43.5%)\n",
      "Processed 34900/80000 users (43.6%)\n",
      "Processed 35000/80000 users (43.8%)\n",
      "Processed 35100/80000 users (43.9%)\n",
      "Processed 35200/80000 users (44.0%)\n",
      "Processed 35300/80000 users (44.1%)\n",
      "Processed 35400/80000 users (44.2%)\n",
      "Processed 35500/80000 users (44.4%)\n",
      "Processed 35600/80000 users (44.5%)\n",
      "Processed 35700/80000 users (44.6%)\n",
      "Processed 35800/80000 users (44.8%)\n",
      "Processed 35900/80000 users (44.9%)\n",
      "Processed 36000/80000 users (45.0%)\n",
      "Processed 36100/80000 users (45.1%)\n",
      "Processed 36200/80000 users (45.2%)\n",
      "Processed 36300/80000 users (45.4%)\n",
      "Processed 36400/80000 users (45.5%)\n",
      "Processed 36500/80000 users (45.6%)\n",
      "Processed 36600/80000 users (45.8%)\n",
      "Processed 36700/80000 users (45.9%)\n",
      "Processed 36800/80000 users (46.0%)\n",
      "Processed 36900/80000 users (46.1%)\n",
      "Processed 37000/80000 users (46.2%)\n",
      "Processed 37100/80000 users (46.4%)\n",
      "Processed 37200/80000 users (46.5%)\n",
      "Processed 37300/80000 users (46.6%)\n",
      "Processed 37400/80000 users (46.8%)\n",
      "Processed 37500/80000 users (46.9%)\n",
      "Processed 37600/80000 users (47.0%)\n",
      "Processed 37700/80000 users (47.1%)\n",
      "Processed 37800/80000 users (47.2%)\n",
      "Processed 37900/80000 users (47.4%)\n",
      "Processed 38000/80000 users (47.5%)\n",
      "Processed 38100/80000 users (47.6%)\n",
      "Processed 38200/80000 users (47.8%)\n",
      "Processed 38300/80000 users (47.9%)\n",
      "Processed 38400/80000 users (48.0%)\n",
      "Processed 38500/80000 users (48.1%)\n",
      "Processed 38600/80000 users (48.2%)\n",
      "Processed 38700/80000 users (48.4%)\n",
      "Processed 38800/80000 users (48.5%)\n",
      "Processed 38900/80000 users (48.6%)\n",
      "Processed 39000/80000 users (48.8%)\n",
      "Processed 39100/80000 users (48.9%)\n",
      "Processed 39200/80000 users (49.0%)\n",
      "Processed 39300/80000 users (49.1%)\n",
      "Processed 39400/80000 users (49.2%)\n",
      "Processed 39500/80000 users (49.4%)\n",
      "Processed 39600/80000 users (49.5%)\n",
      "Processed 39700/80000 users (49.6%)\n",
      "Processed 39800/80000 users (49.8%)\n",
      "Processed 39900/80000 users (49.9%)\n",
      "Processed 40000/80000 users (50.0%)\n",
      "Processed 40100/80000 users (50.1%)\n",
      "Processed 40200/80000 users (50.2%)\n",
      "Processed 40300/80000 users (50.4%)\n",
      "Processed 40400/80000 users (50.5%)\n",
      "Processed 40500/80000 users (50.6%)\n",
      "Processed 40600/80000 users (50.7%)\n",
      "Processed 40700/80000 users (50.9%)\n",
      "Processed 40800/80000 users (51.0%)\n",
      "Processed 40900/80000 users (51.1%)\n",
      "Processed 41000/80000 users (51.2%)\n",
      "Processed 41100/80000 users (51.4%)\n",
      "Processed 41200/80000 users (51.5%)\n",
      "Processed 41300/80000 users (51.6%)\n",
      "Processed 41400/80000 users (51.7%)\n",
      "Processed 41500/80000 users (51.9%)\n",
      "Processed 41600/80000 users (52.0%)\n",
      "Processed 41700/80000 users (52.1%)\n",
      "Processed 41800/80000 users (52.2%)\n",
      "Processed 41900/80000 users (52.4%)\n",
      "Processed 42000/80000 users (52.5%)\n",
      "Processed 42100/80000 users (52.6%)\n",
      "Processed 42200/80000 users (52.8%)\n",
      "Processed 42300/80000 users (52.9%)\n",
      "Processed 42400/80000 users (53.0%)\n",
      "Processed 42500/80000 users (53.1%)\n",
      "Processed 42600/80000 users (53.2%)\n",
      "Processed 42700/80000 users (53.4%)\n",
      "Processed 42800/80000 users (53.5%)\n",
      "Processed 42900/80000 users (53.6%)\n",
      "Processed 43000/80000 users (53.8%)\n",
      "Processed 43100/80000 users (53.9%)\n",
      "Processed 43200/80000 users (54.0%)\n",
      "Processed 43300/80000 users (54.1%)\n",
      "Processed 43400/80000 users (54.2%)\n",
      "Processed 43500/80000 users (54.4%)\n",
      "Processed 43600/80000 users (54.5%)\n",
      "Processed 43700/80000 users (54.6%)\n",
      "Processed 43800/80000 users (54.8%)\n",
      "Processed 43900/80000 users (54.9%)\n",
      "Processed 44000/80000 users (55.0%)\n",
      "Processed 44100/80000 users (55.1%)\n",
      "Processed 44200/80000 users (55.2%)\n",
      "Processed 44300/80000 users (55.4%)\n",
      "Processed 44400/80000 users (55.5%)\n",
      "Processed 44500/80000 users (55.6%)\n",
      "Processed 44600/80000 users (55.8%)\n",
      "Processed 44700/80000 users (55.9%)\n",
      "Processed 44800/80000 users (56.0%)\n",
      "Processed 44900/80000 users (56.1%)\n",
      "Processed 45000/80000 users (56.2%)\n",
      "Processed 45100/80000 users (56.4%)\n",
      "Processed 45200/80000 users (56.5%)\n",
      "Processed 45300/80000 users (56.6%)\n",
      "Processed 45400/80000 users (56.8%)\n",
      "Processed 45500/80000 users (56.9%)\n",
      "Processed 45600/80000 users (57.0%)\n",
      "Processed 45700/80000 users (57.1%)\n",
      "Processed 45800/80000 users (57.2%)\n",
      "Processed 45900/80000 users (57.4%)\n",
      "Processed 46000/80000 users (57.5%)\n",
      "Processed 46100/80000 users (57.6%)\n",
      "Processed 46200/80000 users (57.8%)\n",
      "Processed 46300/80000 users (57.9%)\n",
      "Processed 46400/80000 users (58.0%)\n",
      "Processed 46500/80000 users (58.1%)\n",
      "Processed 46600/80000 users (58.2%)\n",
      "Processed 46700/80000 users (58.4%)\n",
      "Processed 46800/80000 users (58.5%)\n",
      "Processed 46900/80000 users (58.6%)\n",
      "Processed 47000/80000 users (58.8%)\n",
      "Processed 47100/80000 users (58.9%)\n",
      "Processed 47200/80000 users (59.0%)\n",
      "Processed 47300/80000 users (59.1%)\n",
      "Processed 47400/80000 users (59.2%)\n",
      "Processed 47500/80000 users (59.4%)\n",
      "Processed 47600/80000 users (59.5%)\n",
      "Processed 47700/80000 users (59.6%)\n",
      "Processed 47800/80000 users (59.8%)\n",
      "Processed 47900/80000 users (59.9%)\n",
      "Processed 48000/80000 users (60.0%)\n",
      "Processed 48100/80000 users (60.1%)\n",
      "Processed 48200/80000 users (60.2%)\n",
      "Processed 48300/80000 users (60.4%)\n",
      "Processed 48400/80000 users (60.5%)\n",
      "Processed 48500/80000 users (60.6%)\n",
      "Processed 48600/80000 users (60.8%)\n",
      "Processed 48700/80000 users (60.9%)\n",
      "Processed 48800/80000 users (61.0%)\n",
      "Processed 48900/80000 users (61.1%)\n",
      "Processed 49000/80000 users (61.3%)\n",
      "Processed 49100/80000 users (61.4%)\n",
      "Processed 49200/80000 users (61.5%)\n",
      "Processed 49300/80000 users (61.6%)\n",
      "Processed 49400/80000 users (61.8%)\n",
      "Processed 49500/80000 users (61.9%)\n",
      "Processed 49600/80000 users (62.0%)\n",
      "Processed 49700/80000 users (62.1%)\n",
      "Processed 49800/80000 users (62.3%)\n",
      "Processed 49900/80000 users (62.4%)\n",
      "Processed 50000/80000 users (62.5%)\n",
      "Processed 50100/80000 users (62.6%)\n",
      "Processed 50200/80000 users (62.7%)\n",
      "Processed 50300/80000 users (62.9%)\n",
      "Processed 50400/80000 users (63.0%)\n",
      "Processed 50500/80000 users (63.1%)\n",
      "Processed 50600/80000 users (63.2%)\n",
      "Processed 50700/80000 users (63.4%)\n",
      "Processed 50800/80000 users (63.5%)\n",
      "Processed 50900/80000 users (63.6%)\n",
      "Processed 51000/80000 users (63.7%)\n",
      "Processed 51100/80000 users (63.9%)\n",
      "Processed 51200/80000 users (64.0%)\n",
      "Processed 51300/80000 users (64.1%)\n",
      "Processed 51400/80000 users (64.2%)\n",
      "Processed 51500/80000 users (64.4%)\n",
      "Processed 51600/80000 users (64.5%)\n",
      "Processed 51700/80000 users (64.6%)\n",
      "Processed 51800/80000 users (64.8%)\n",
      "Processed 51900/80000 users (64.9%)\n",
      "Processed 52000/80000 users (65.0%)\n",
      "Processed 52100/80000 users (65.1%)\n",
      "Processed 52200/80000 users (65.2%)\n",
      "Processed 52300/80000 users (65.4%)\n",
      "Processed 52400/80000 users (65.5%)\n",
      "Processed 52500/80000 users (65.6%)\n",
      "Processed 52600/80000 users (65.8%)\n",
      "Processed 52700/80000 users (65.9%)\n",
      "Processed 52800/80000 users (66.0%)\n",
      "Processed 52900/80000 users (66.1%)\n",
      "Processed 53000/80000 users (66.2%)\n",
      "Processed 53100/80000 users (66.4%)\n",
      "Processed 53200/80000 users (66.5%)\n",
      "Processed 53300/80000 users (66.6%)\n",
      "Processed 53400/80000 users (66.8%)\n",
      "Processed 53500/80000 users (66.9%)\n",
      "Processed 53600/80000 users (67.0%)\n",
      "Processed 53700/80000 users (67.1%)\n",
      "Processed 53800/80000 users (67.2%)\n",
      "Processed 53900/80000 users (67.4%)\n",
      "Processed 54000/80000 users (67.5%)\n",
      "Processed 54100/80000 users (67.6%)\n",
      "Processed 54200/80000 users (67.8%)\n",
      "Processed 54300/80000 users (67.9%)\n",
      "Processed 54400/80000 users (68.0%)\n",
      "Processed 54500/80000 users (68.1%)\n",
      "Processed 54600/80000 users (68.2%)\n",
      "Processed 54700/80000 users (68.4%)\n",
      "Processed 54800/80000 users (68.5%)\n",
      "Processed 54900/80000 users (68.6%)\n",
      "Processed 55000/80000 users (68.8%)\n",
      "Processed 55100/80000 users (68.9%)\n",
      "Processed 55200/80000 users (69.0%)\n",
      "Processed 55300/80000 users (69.1%)\n",
      "Processed 55400/80000 users (69.2%)\n",
      "Processed 55500/80000 users (69.4%)\n",
      "Processed 55600/80000 users (69.5%)\n",
      "Processed 55700/80000 users (69.6%)\n",
      "Processed 55800/80000 users (69.8%)\n",
      "Processed 55900/80000 users (69.9%)\n",
      "Processed 56000/80000 users (70.0%)\n",
      "Processed 56100/80000 users (70.1%)\n",
      "Processed 56200/80000 users (70.2%)\n",
      "Processed 56300/80000 users (70.4%)\n",
      "Processed 56400/80000 users (70.5%)\n",
      "Processed 56500/80000 users (70.6%)\n",
      "Processed 56600/80000 users (70.8%)\n",
      "Processed 56700/80000 users (70.9%)\n",
      "Processed 56800/80000 users (71.0%)\n",
      "Processed 56900/80000 users (71.1%)\n",
      "Processed 57000/80000 users (71.2%)\n",
      "Processed 57100/80000 users (71.4%)\n",
      "Processed 57200/80000 users (71.5%)\n",
      "Processed 57300/80000 users (71.6%)\n",
      "Processed 57400/80000 users (71.8%)\n",
      "Processed 57500/80000 users (71.9%)\n",
      "Processed 57600/80000 users (72.0%)\n",
      "Processed 57700/80000 users (72.1%)\n",
      "Processed 57800/80000 users (72.2%)\n",
      "Processed 57900/80000 users (72.4%)\n",
      "Processed 58000/80000 users (72.5%)\n",
      "Processed 58100/80000 users (72.6%)\n",
      "Processed 58200/80000 users (72.8%)\n",
      "Processed 58300/80000 users (72.9%)\n",
      "Processed 58400/80000 users (73.0%)\n",
      "Processed 58500/80000 users (73.1%)\n",
      "Processed 58600/80000 users (73.2%)\n",
      "Processed 58700/80000 users (73.4%)\n",
      "Processed 58800/80000 users (73.5%)\n",
      "Processed 58900/80000 users (73.6%)\n",
      "Processed 59000/80000 users (73.8%)\n",
      "Processed 59100/80000 users (73.9%)\n",
      "Processed 59200/80000 users (74.0%)\n",
      "Processed 59300/80000 users (74.1%)\n",
      "Processed 59400/80000 users (74.2%)\n",
      "Processed 59500/80000 users (74.4%)\n",
      "Processed 59600/80000 users (74.5%)\n",
      "Processed 59700/80000 users (74.6%)\n",
      "Processed 59800/80000 users (74.8%)\n",
      "Processed 59900/80000 users (74.9%)\n",
      "Processed 60000/80000 users (75.0%)\n",
      "Processed 60100/80000 users (75.1%)\n",
      "Processed 60200/80000 users (75.2%)\n",
      "Processed 60300/80000 users (75.4%)\n",
      "Processed 60400/80000 users (75.5%)\n",
      "Processed 60500/80000 users (75.6%)\n",
      "Processed 60600/80000 users (75.8%)\n",
      "Processed 60700/80000 users (75.9%)\n",
      "Processed 60800/80000 users (76.0%)\n",
      "Processed 60900/80000 users (76.1%)\n",
      "Processed 61000/80000 users (76.2%)\n",
      "Processed 61100/80000 users (76.4%)\n",
      "Processed 61200/80000 users (76.5%)\n",
      "Processed 61300/80000 users (76.6%)\n",
      "Processed 61400/80000 users (76.8%)\n",
      "Processed 61500/80000 users (76.9%)\n",
      "Processed 61600/80000 users (77.0%)\n",
      "Processed 61700/80000 users (77.1%)\n",
      "Processed 61800/80000 users (77.2%)\n",
      "Processed 61900/80000 users (77.4%)\n",
      "Processed 62000/80000 users (77.5%)\n",
      "Processed 62100/80000 users (77.6%)\n",
      "Processed 62200/80000 users (77.8%)\n",
      "Processed 62300/80000 users (77.9%)\n",
      "Processed 62400/80000 users (78.0%)\n",
      "Processed 62500/80000 users (78.1%)\n",
      "Processed 62600/80000 users (78.2%)\n",
      "Processed 62700/80000 users (78.4%)\n",
      "Processed 62800/80000 users (78.5%)\n",
      "Processed 62900/80000 users (78.6%)\n",
      "Processed 63000/80000 users (78.8%)\n",
      "Processed 63100/80000 users (78.9%)\n",
      "Processed 63200/80000 users (79.0%)\n",
      "Processed 63300/80000 users (79.1%)\n",
      "Processed 63400/80000 users (79.2%)\n",
      "Processed 63500/80000 users (79.4%)\n",
      "Processed 63600/80000 users (79.5%)\n",
      "Processed 63700/80000 users (79.6%)\n",
      "Processed 63800/80000 users (79.8%)\n",
      "Processed 63900/80000 users (79.9%)\n",
      "Processed 64000/80000 users (80.0%)\n",
      "Processed 64100/80000 users (80.1%)\n",
      "Processed 64200/80000 users (80.2%)\n",
      "Processed 64300/80000 users (80.4%)\n",
      "Processed 64400/80000 users (80.5%)\n",
      "Processed 64500/80000 users (80.6%)\n",
      "Processed 64600/80000 users (80.8%)\n",
      "Processed 64700/80000 users (80.9%)\n",
      "Processed 64800/80000 users (81.0%)\n",
      "Processed 64900/80000 users (81.1%)\n",
      "Processed 65000/80000 users (81.2%)\n",
      "Processed 65100/80000 users (81.4%)\n",
      "Processed 65200/80000 users (81.5%)\n",
      "Processed 65300/80000 users (81.6%)\n",
      "Processed 65400/80000 users (81.8%)\n",
      "Processed 65500/80000 users (81.9%)\n",
      "Processed 65600/80000 users (82.0%)\n",
      "Processed 65700/80000 users (82.1%)\n",
      "Processed 65800/80000 users (82.2%)\n",
      "Processed 65900/80000 users (82.4%)\n",
      "Processed 66000/80000 users (82.5%)\n",
      "Processed 66100/80000 users (82.6%)\n",
      "Processed 66200/80000 users (82.8%)\n",
      "Processed 66300/80000 users (82.9%)\n",
      "Processed 66400/80000 users (83.0%)\n",
      "Processed 66500/80000 users (83.1%)\n",
      "Processed 66600/80000 users (83.2%)\n",
      "Processed 66700/80000 users (83.4%)\n",
      "Processed 66800/80000 users (83.5%)\n",
      "Processed 66900/80000 users (83.6%)\n",
      "Processed 67000/80000 users (83.8%)\n",
      "Processed 67100/80000 users (83.9%)\n",
      "Processed 67200/80000 users (84.0%)\n",
      "Processed 67300/80000 users (84.1%)\n",
      "Processed 67400/80000 users (84.2%)\n",
      "Processed 67500/80000 users (84.4%)\n",
      "Processed 67600/80000 users (84.5%)\n",
      "Processed 67700/80000 users (84.6%)\n",
      "Processed 67800/80000 users (84.8%)\n",
      "Processed 67900/80000 users (84.9%)\n",
      "Processed 68000/80000 users (85.0%)\n",
      "Processed 68100/80000 users (85.1%)\n",
      "Processed 68200/80000 users (85.2%)\n",
      "Processed 68300/80000 users (85.4%)\n",
      "Processed 68400/80000 users (85.5%)\n",
      "Processed 68500/80000 users (85.6%)\n",
      "Processed 68600/80000 users (85.8%)\n",
      "Processed 68700/80000 users (85.9%)\n",
      "Processed 68800/80000 users (86.0%)\n",
      "Processed 68900/80000 users (86.1%)\n",
      "Processed 69000/80000 users (86.2%)\n",
      "Processed 69100/80000 users (86.4%)\n",
      "Processed 69200/80000 users (86.5%)\n",
      "Processed 69300/80000 users (86.6%)\n",
      "Processed 69400/80000 users (86.8%)\n",
      "Processed 69500/80000 users (86.9%)\n",
      "Processed 69600/80000 users (87.0%)\n",
      "Processed 69700/80000 users (87.1%)\n",
      "Processed 69800/80000 users (87.2%)\n",
      "Processed 69900/80000 users (87.4%)\n",
      "Processed 70000/80000 users (87.5%)\n",
      "Processed 70100/80000 users (87.6%)\n",
      "Processed 70200/80000 users (87.8%)\n",
      "Processed 70300/80000 users (87.9%)\n",
      "Processed 70400/80000 users (88.0%)\n",
      "Processed 70500/80000 users (88.1%)\n",
      "Processed 70600/80000 users (88.2%)\n",
      "Processed 70700/80000 users (88.4%)\n",
      "Processed 70800/80000 users (88.5%)\n",
      "Processed 70900/80000 users (88.6%)\n",
      "Processed 71000/80000 users (88.8%)\n",
      "Processed 71100/80000 users (88.9%)\n",
      "Processed 71200/80000 users (89.0%)\n",
      "Processed 71300/80000 users (89.1%)\n",
      "Processed 71400/80000 users (89.2%)\n",
      "Processed 71500/80000 users (89.4%)\n",
      "Processed 71600/80000 users (89.5%)\n",
      "Processed 71700/80000 users (89.6%)\n",
      "Processed 71800/80000 users (89.8%)\n",
      "Processed 71900/80000 users (89.9%)\n",
      "Processed 72000/80000 users (90.0%)\n",
      "Processed 72100/80000 users (90.1%)\n",
      "Processed 72200/80000 users (90.2%)\n",
      "Processed 72300/80000 users (90.4%)\n",
      "Processed 72400/80000 users (90.5%)\n",
      "Processed 72500/80000 users (90.6%)\n",
      "Processed 72600/80000 users (90.8%)\n",
      "Processed 72700/80000 users (90.9%)\n",
      "Processed 72800/80000 users (91.0%)\n",
      "Processed 72900/80000 users (91.1%)\n",
      "Processed 73000/80000 users (91.2%)\n",
      "Processed 73100/80000 users (91.4%)\n",
      "Processed 73200/80000 users (91.5%)\n",
      "Processed 73300/80000 users (91.6%)\n",
      "Processed 73400/80000 users (91.8%)\n",
      "Processed 73500/80000 users (91.9%)\n",
      "Processed 73600/80000 users (92.0%)\n",
      "Processed 73700/80000 users (92.1%)\n",
      "Processed 73800/80000 users (92.2%)\n",
      "Processed 73900/80000 users (92.4%)\n",
      "Processed 74000/80000 users (92.5%)\n",
      "Processed 74100/80000 users (92.6%)\n",
      "Processed 74200/80000 users (92.8%)\n",
      "Processed 74300/80000 users (92.9%)\n",
      "Processed 74400/80000 users (93.0%)\n",
      "Processed 74500/80000 users (93.1%)\n",
      "Processed 74600/80000 users (93.2%)\n",
      "Processed 74700/80000 users (93.4%)\n",
      "Processed 74800/80000 users (93.5%)\n",
      "Processed 74900/80000 users (93.6%)\n",
      "Processed 75000/80000 users (93.8%)\n",
      "Processed 75100/80000 users (93.9%)\n",
      "Processed 75200/80000 users (94.0%)\n",
      "Processed 75300/80000 users (94.1%)\n",
      "Processed 75400/80000 users (94.2%)\n",
      "Processed 75500/80000 users (94.4%)\n",
      "Processed 75600/80000 users (94.5%)\n",
      "Processed 75700/80000 users (94.6%)\n",
      "Processed 75800/80000 users (94.8%)\n",
      "Processed 75900/80000 users (94.9%)\n",
      "Processed 76000/80000 users (95.0%)\n",
      "Processed 76100/80000 users (95.1%)\n",
      "Processed 76200/80000 users (95.2%)\n",
      "Processed 76300/80000 users (95.4%)\n",
      "Processed 76400/80000 users (95.5%)\n",
      "Processed 76500/80000 users (95.6%)\n",
      "Processed 76600/80000 users (95.8%)\n",
      "Processed 76700/80000 users (95.9%)\n",
      "Processed 76800/80000 users (96.0%)\n",
      "Processed 76900/80000 users (96.1%)\n",
      "Processed 77000/80000 users (96.2%)\n",
      "Processed 77100/80000 users (96.4%)\n",
      "Processed 77200/80000 users (96.5%)\n",
      "Processed 77300/80000 users (96.6%)\n",
      "Processed 77400/80000 users (96.8%)\n",
      "Processed 77500/80000 users (96.9%)\n",
      "Processed 77600/80000 users (97.0%)\n",
      "Processed 77700/80000 users (97.1%)\n",
      "Processed 77800/80000 users (97.2%)\n",
      "Processed 77900/80000 users (97.4%)\n",
      "Processed 78000/80000 users (97.5%)\n",
      "Processed 78100/80000 users (97.6%)\n",
      "Processed 78200/80000 users (97.8%)\n",
      "Processed 78300/80000 users (97.9%)\n",
      "Processed 78400/80000 users (98.0%)\n",
      "Processed 78500/80000 users (98.1%)\n",
      "Processed 78600/80000 users (98.2%)\n",
      "Processed 78700/80000 users (98.4%)\n",
      "Processed 78800/80000 users (98.5%)\n",
      "Processed 78900/80000 users (98.6%)\n",
      "Processed 79000/80000 users (98.8%)\n",
      "Processed 79100/80000 users (98.9%)\n",
      "Processed 79200/80000 users (99.0%)\n",
      "Processed 79300/80000 users (99.1%)\n",
      "Processed 79400/80000 users (99.2%)\n",
      "Processed 79500/80000 users (99.4%)\n",
      "Processed 79600/80000 users (99.5%)\n",
      "Processed 79700/80000 users (99.6%)\n",
      "Processed 79800/80000 users (99.8%)\n",
      "Processed 79900/80000 users (99.9%)\n",
      "Processed 80000/80000 users (100.0%)\n",
      "Calculated genre preferences for 80000 users\n",
      "\n",
      "--------------------------------------------------\n",
      "DATA ANALYSIS: USER GENRE PREFERENCES\n",
      "--------------------------------------------------\n",
      "\n",
      "User Genre Preferences Summary:\n",
      "\n",
      "Statistics for top genres:\n",
      "- Drama:\n",
      "  * Mean preference: 0.324 (std: 0.527)\n",
      "  * Range: -1.000 to 1.000\n",
      "  * Users with positive preference: 54762 (68.5%)\n",
      "  * Users with negative preference: 22576 (28.2%)\n",
      "- Crime:\n",
      "  * Mean preference: 0.135 (std: 0.309)\n",
      "  * Range: -1.000 to 1.000\n",
      "  * Users with positive preference: 49859 (62.3%)\n",
      "  * Users with negative preference: 23681 (29.6%)\n",
      "- Western Europe:\n",
      "  * Mean preference: 0.139 (std: 0.362)\n",
      "  * Range: -1.000 to 1.000\n",
      "  * Users with positive preference: 48930 (61.2%)\n",
      "  * Users with negative preference: 25478 (31.8%)\n",
      "- War:\n",
      "  * Mean preference: 0.073 (std: 0.165)\n",
      "  * Range: -1.000 to 1.000\n",
      "  * Users with positive preference: 47277 (59.1%)\n",
      "  * Users with negative preference: 17033 (21.3%)\n",
      "- North America:\n",
      "  * Mean preference: 0.146 (std: 0.865)\n",
      "  * Range: -1.000 to 1.000\n",
      "  * Users with positive preference: 46251 (57.8%)\n",
      "  * Users with negative preference: 32218 (40.3%)\n",
      "\n",
      "User preference distributions saved to ../rec/collaborative-recommendations\\user_preference_distributions.png\n",
      "\n",
      "Genre preference correlation matrix saved to ../rec/collaborative-recommendations\\genre_preference_correlation.png\n",
      "\n",
      "Example users with diverse preferences:\n",
      "\n",
      "User 93761.0 (diversity score: 0.566):\n",
      "- Most liked genres: Animation (1.00), Comedy (1.00), Documentary (1.00)\n",
      "- Most disliked genres: Horror (-1.00), War (-1.00), Middle East (-1.00)\n",
      "\n",
      "User 63686.0 (diversity score: 0.565):\n",
      "- Most liked genres: Drama (1.00), War (1.00), Thriller (0.67)\n",
      "- Most disliked genres: Action (-1.00), Adventure (-1.00), Animation (-1.00)\n",
      "\n",
      "User 8374.0 (diversity score: 0.559):\n",
      "- Most liked genres: Animation (1.00), Children (1.00), Musical (1.00)\n",
      "- Most disliked genres: Comedy (-1.00), Crime (-1.00), Drama (-1.00)\n",
      "\n",
      "Sample of user genre preferences:\n",
      "\n",
      "User 45430.0 preferences:\n",
      "- North America: 1.000\n",
      "- Adventure: 0.726\n",
      "- Comedy: 0.694\n",
      "- Action: 0.403\n",
      "- Romance: 0.339\n",
      "\n",
      "User 55852.0 preferences:\n",
      "- North America: 1.000\n",
      "- Action: 0.600\n",
      "- Thriller: 0.550\n",
      "- Adventure: 0.450\n",
      "- Drama: 0.400\n",
      "\n",
      "User 20245.0 preferences:\n",
      "- North America: -1.000\n",
      "- Comedy: -0.731\n",
      "- Drama: -0.385\n",
      "- Romance: -0.385\n",
      "- Adventure: -0.308\n",
      "\n",
      "Saved user genre preferences to ../rec/collaborative-recommendations\\user_genre_preferences.csv\n",
      "\n",
      "================================================================================\n",
      "STEP 4: DNN TRAINING DATA PREPARATION\n",
      "================================================================================\n",
      "Preparing training data for DNN model...\n",
      "Using 1000000 ratings to train the DNN model\n",
      "Processed 10000/1000000 ratings (1.0%)\n",
      "Processed 20000/1000000 ratings (2.0%)\n",
      "Processed 30000/1000000 ratings (3.0%)\n",
      "Processed 40000/1000000 ratings (4.0%)\n",
      "Processed 50000/1000000 ratings (5.0%)\n",
      "Processed 60000/1000000 ratings (6.0%)\n",
      "Processed 70000/1000000 ratings (7.0%)\n",
      "Processed 80000/1000000 ratings (8.0%)\n",
      "Processed 90000/1000000 ratings (9.0%)\n",
      "Processed 100000/1000000 ratings (10.0%)\n",
      "Processed 110000/1000000 ratings (11.0%)\n",
      "Processed 120000/1000000 ratings (12.0%)\n",
      "Processed 130000/1000000 ratings (13.0%)\n",
      "Processed 140000/1000000 ratings (14.0%)\n",
      "Processed 150000/1000000 ratings (15.0%)\n",
      "Processed 160000/1000000 ratings (16.0%)\n",
      "Processed 170000/1000000 ratings (17.0%)\n",
      "Processed 180000/1000000 ratings (18.0%)\n",
      "Processed 190000/1000000 ratings (19.0%)\n",
      "Processed 200000/1000000 ratings (20.0%)\n",
      "Processed 210000/1000000 ratings (21.0%)\n",
      "Processed 220000/1000000 ratings (22.0%)\n",
      "Processed 230000/1000000 ratings (23.0%)\n",
      "Processed 240000/1000000 ratings (24.0%)\n",
      "Processed 250000/1000000 ratings (25.0%)\n",
      "Processed 260000/1000000 ratings (26.0%)\n",
      "Processed 270000/1000000 ratings (27.0%)\n",
      "Processed 280000/1000000 ratings (28.0%)\n",
      "Processed 290000/1000000 ratings (29.0%)\n",
      "Processed 300000/1000000 ratings (30.0%)\n",
      "Processed 310000/1000000 ratings (31.0%)\n",
      "Processed 320000/1000000 ratings (32.0%)\n",
      "Processed 330000/1000000 ratings (33.0%)\n",
      "Processed 340000/1000000 ratings (34.0%)\n",
      "Processed 350000/1000000 ratings (35.0%)\n",
      "Processed 360000/1000000 ratings (36.0%)\n",
      "Processed 370000/1000000 ratings (37.0%)\n",
      "Processed 380000/1000000 ratings (38.0%)\n",
      "Processed 390000/1000000 ratings (39.0%)\n",
      "Processed 400000/1000000 ratings (40.0%)\n",
      "Processed 410000/1000000 ratings (41.0%)\n",
      "Processed 420000/1000000 ratings (42.0%)\n",
      "Processed 430000/1000000 ratings (43.0%)\n",
      "Processed 440000/1000000 ratings (44.0%)\n",
      "Processed 450000/1000000 ratings (45.0%)\n",
      "Processed 460000/1000000 ratings (46.0%)\n",
      "Processed 470000/1000000 ratings (47.0%)\n",
      "Processed 480000/1000000 ratings (48.0%)\n",
      "Processed 490000/1000000 ratings (49.0%)\n",
      "Processed 500000/1000000 ratings (50.0%)\n",
      "Processed 510000/1000000 ratings (51.0%)\n",
      "Processed 520000/1000000 ratings (52.0%)\n",
      "Processed 530000/1000000 ratings (53.0%)\n",
      "Processed 540000/1000000 ratings (54.0%)\n",
      "Processed 550000/1000000 ratings (55.0%)\n",
      "Processed 560000/1000000 ratings (56.0%)\n",
      "Processed 570000/1000000 ratings (57.0%)\n",
      "Processed 580000/1000000 ratings (58.0%)\n",
      "Processed 590000/1000000 ratings (59.0%)\n",
      "Processed 600000/1000000 ratings (60.0%)\n",
      "Processed 610000/1000000 ratings (61.0%)\n",
      "Processed 620000/1000000 ratings (62.0%)\n",
      "Processed 630000/1000000 ratings (63.0%)\n",
      "Processed 640000/1000000 ratings (64.0%)\n",
      "Processed 650000/1000000 ratings (65.0%)\n",
      "Processed 660000/1000000 ratings (66.0%)\n",
      "Processed 670000/1000000 ratings (67.0%)\n",
      "Processed 680000/1000000 ratings (68.0%)\n",
      "Processed 690000/1000000 ratings (69.0%)\n",
      "Processed 700000/1000000 ratings (70.0%)\n",
      "Processed 710000/1000000 ratings (71.0%)\n",
      "Processed 720000/1000000 ratings (72.0%)\n",
      "Processed 730000/1000000 ratings (73.0%)\n",
      "Processed 740000/1000000 ratings (74.0%)\n",
      "Processed 750000/1000000 ratings (75.0%)\n",
      "Processed 760000/1000000 ratings (76.0%)\n",
      "Processed 770000/1000000 ratings (77.0%)\n",
      "Processed 780000/1000000 ratings (78.0%)\n",
      "Processed 790000/1000000 ratings (79.0%)\n",
      "Processed 800000/1000000 ratings (80.0%)\n",
      "Processed 810000/1000000 ratings (81.0%)\n",
      "Processed 820000/1000000 ratings (82.0%)\n",
      "Processed 830000/1000000 ratings (83.0%)\n",
      "Processed 840000/1000000 ratings (84.0%)\n",
      "Processed 850000/1000000 ratings (85.0%)\n",
      "Processed 860000/1000000 ratings (86.0%)\n",
      "Processed 870000/1000000 ratings (87.0%)\n",
      "Processed 880000/1000000 ratings (88.0%)\n",
      "Processed 890000/1000000 ratings (89.0%)\n",
      "Processed 900000/1000000 ratings (90.0%)\n",
      "Processed 910000/1000000 ratings (91.0%)\n",
      "Processed 920000/1000000 ratings (92.0%)\n",
      "Processed 930000/1000000 ratings (93.0%)\n",
      "Processed 940000/1000000 ratings (94.0%)\n",
      "Processed 950000/1000000 ratings (95.0%)\n",
      "Processed 960000/1000000 ratings (96.0%)\n",
      "Processed 970000/1000000 ratings (97.0%)\n",
      "Processed 980000/1000000 ratings (98.0%)\n",
      "Processed 990000/1000000 ratings (99.0%)\n",
      "Processed 1000000/1000000 ratings (100.0%)\n",
      "Created feature matrix with shape (997527, 66) and labels with shape (997527,)\n",
      "Prepared training data with 798021 samples, validation data with 199506 samples\n",
      "\n",
      "--------------------------------------------------\n",
      "DATA ANALYSIS: DNN TRAINING DATA\n",
      "--------------------------------------------------\n",
      "\n",
      "Feature Vector Dimension: 66\n",
      "Number of genres: 33\n",
      "Features per genre: 2 (user preference + movie indicator)\n",
      "Total features: 66\n",
      "\n",
      "Training Labels Distribution:\n",
      "Training labels distribution saved to ../rec/collaborative-recommendations\\training_labels_distribution.png\n",
      "\n",
      "Feature Statistics:\n",
      "\n",
      "Feature statistics by genre (top 5 genres):\n",
      "             Genre  User_Pref_Mean  User_Pref_Std  Movie_Ind_Mean  \\\n",
      "8            Drama        0.258634       0.550517        0.441622   \n",
      "32  Western Europe        0.107578       0.352459        0.241426   \n",
      "6            Crime        0.102566       0.270400        0.164577   \n",
      "18             War        0.060658       0.133819        0.051922   \n",
      "14         Mystery        0.046088       0.157611        0.078004   \n",
      "\n",
      "    Movie_Ind_Std  \n",
      "8        0.495548  \n",
      "32       0.427326  \n",
      "6        0.369109  \n",
      "18       0.221741  \n",
      "14       0.269312  \n",
      "Feature distributions saved to ../rec/collaborative-recommendations\\feature_distributions.png\n",
      "\n",
      "Sample of DNN training data (showing first 3 genres for 1 sample):\n",
      "(no genres listed)_user_pref    0.000000\n",
      "Action_user_pref                0.361111\n",
      "Adventure_user_pref             0.333333\n",
      "(no genres listed)_movie_ind    0.000000\n",
      "Action_movie_ind                0.000000\n",
      "Adventure_movie_ind             0.000000\n",
      "rating                          1.000000\n",
      "\n",
      "================================================================================\n",
      "STEP 5: DNN MODEL BUILDING AND TRAINING\n",
      "================================================================================\n",
      "Building and training DNN model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NCPC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m12470/12470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1ms/step - loss: 2.1995 - mae: 1.0879 - val_loss: 0.8816 - val_mae: 0.7237\n",
      "Epoch 2/20\n",
      "\u001b[1m12470/12470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.9079 - mae: 0.7370 - val_loss: 0.8642 - val_mae: 0.7108\n",
      "Epoch 3/20\n",
      "\u001b[1m12470/12470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8801 - mae: 0.7220 - val_loss: 0.8596 - val_mae: 0.7083\n",
      "Epoch 4/20\n",
      "\u001b[1m12470/12470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8749 - mae: 0.7198 - val_loss: 0.8517 - val_mae: 0.7050\n",
      "Epoch 5/20\n",
      "\u001b[1m12470/12470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8699 - mae: 0.7174 - val_loss: 0.8491 - val_mae: 0.7069\n",
      "Epoch 6/20\n",
      "\u001b[1m12470/12470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8678 - mae: 0.7166 - val_loss: 0.8474 - val_mae: 0.7066\n",
      "Epoch 7/20\n",
      "\u001b[1m12470/12470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8685 - mae: 0.7168 - val_loss: 0.8494 - val_mae: 0.7041\n",
      "Epoch 8/20\n",
      "\u001b[1m12470/12470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8630 - mae: 0.7143 - val_loss: 0.8412 - val_mae: 0.7010\n",
      "Epoch 9/20\n",
      "\u001b[1m12470/12470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1ms/step - loss: 0.8622 - mae: 0.7142 - val_loss: 0.8430 - val_mae: 0.7058\n",
      "Epoch 10/20\n",
      "\u001b[1m12470/12470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8628 - mae: 0.7147 - val_loss: 0.8399 - val_mae: 0.7028\n",
      "Epoch 11/20\n",
      "\u001b[1m12470/12470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8607 - mae: 0.7139 - val_loss: 0.8401 - val_mae: 0.7039\n",
      "Epoch 12/20\n",
      "\u001b[1m12470/12470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8577 - mae: 0.7124 - val_loss: 0.8373 - val_mae: 0.7026\n",
      "Epoch 13/20\n",
      "\u001b[1m12470/12470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8615 - mae: 0.7139 - val_loss: 0.8371 - val_mae: 0.7008\n",
      "Epoch 14/20\n",
      "\u001b[1m12470/12470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - loss: 0.8587 - mae: 0.7128 - val_loss: 0.8373 - val_mae: 0.6997\n",
      "Epoch 15/20\n",
      "\u001b[1m12470/12470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1ms/step - loss: 0.8589 - mae: 0.7133 - val_loss: 0.8378 - val_mae: 0.7015\n",
      "Epoch 16/20\n",
      "\u001b[1m12470/12470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1ms/step - loss: 0.8578 - mae: 0.7132 - val_loss: 0.8370 - val_mae: 0.6987\n",
      "Epoch 17/20\n",
      "\u001b[1m12470/12470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1ms/step - loss: 0.8585 - mae: 0.7121 - val_loss: 0.8391 - val_mae: 0.6996\n",
      "Epoch 18/20\n",
      "\u001b[1m12470/12470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1ms/step - loss: 0.8578 - mae: 0.7120 - val_loss: 0.8353 - val_mae: 0.7009\n",
      "Epoch 19/20\n",
      "\u001b[1m12470/12470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1ms/step - loss: 0.8578 - mae: 0.7124 - val_loss: 0.8345 - val_mae: 0.6984\n",
      "Epoch 20/20\n",
      "\u001b[1m12470/12470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1ms/step - loss: 0.8587 - mae: 0.7125 - val_loss: 0.8344 - val_mae: 0.6974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 04:47:52,527 : WARNING : You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training completed. Validation MSE: 0.8344, validation MAE: 0.6974\n",
      "Saved DNN model to ../rec/collaborative-recommendations\\dnn_model.h5\n",
      "\n",
      "--------------------------------------------------\n",
      "MODEL ANALYSIS: DNN TRAINING RESULTS\n",
      "--------------------------------------------------\n",
      "Training history plot saved to ../rec/collaborative-recommendations\\dnn_training_history.png\n",
      "\n",
      "DNN Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,288</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,288\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,637</span> (84.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,637\u001b[0m (84.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,137</span> (27.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,137\u001b[0m (27.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> (896.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m224\u001b[0m (896.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,276</span> (55.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m14,276\u001b[0m (55.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Training Metrics:\n",
      "- MSE Loss: 0.8569\n",
      "- MAE: 0.7120\n",
      "\n",
      "Final Validation Metrics:\n",
      "- MSE Loss: 0.8344\n",
      "- MAE: 0.6974\n",
      "\n",
      "Final RMSE:\n",
      "- Training RMSE: 0.9257\n",
      "- Validation RMSE: 0.9135\n",
      "\n",
      "Prediction Quality Analysis:\n",
      "\u001b[1m6235/6235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 582us/step\n",
      "Prediction error distribution saved to ../rec/collaborative-recommendations\\prediction_error_distribution.png\n",
      "Predicted vs actual plot saved to ../rec/collaborative-recommendations\\predicted_vs_actual.png\n",
      "\n",
      "Error by rating level:\n",
      "     count  mean_error  abs_error      rmse\n",
      "0.5   2399    2.629057   2.629057  2.663440\n",
      "1.0   6857    2.157614   2.157614  2.201260\n",
      "1.5   2804    1.605833   1.605833  1.655772\n",
      "2.0  14447    1.227455   1.227860  1.302369\n",
      "2.5   8927    0.687689   0.691596  0.794004\n",
      "3.0  43146    0.366506   0.447590  0.567102\n",
      "3.5  21914   -0.081471   0.369241  0.435887\n",
      "4.0  54837   -0.277226   0.383501  0.503701\n",
      "4.5  15443   -0.666235   0.666302  0.778649\n",
      "5.0  28732   -1.075210   1.075210  1.136135\n",
      "Error by rating plot saved to ../rec/collaborative-recommendations\\error_by_rating.png\n",
      "\n",
      "================================================================================\n",
      "STEP 6: MOVIE RECOMMENDATION GENERATION\n",
      "================================================================================\n",
      "Generating top-20 DNN recommendations for all users with optimized batching...\n",
      "Limiting to 200 users out of 80000 total users\n",
      "Creating user rating lookup dictionary...\n",
      "Processing batch of 50 users (1-50 of 200)\n",
      "  Processed 10/50 users in batch, avg time: 1.13s per user\n",
      "  Processed 20/50 users in batch, avg time: 1.14s per user\n",
      "  Processed 30/50 users in batch, avg time: 1.14s per user\n",
      "  Processed 40/50 users in batch, avg time: 1.14s per user\n",
      "  Processed 50/50 users in batch, avg time: 1.13s per user\n",
      "Completed batch 1/4\n",
      "Progress: 25.0% - Elapsed: 56.60s - Est. remaining: 169.79s\n",
      "Processing batch of 50 users (51-100 of 200)\n",
      "  Processed 10/50 users in batch, avg time: 1.14s per user\n",
      "  Processed 20/50 users in batch, avg time: 1.13s per user\n",
      "  Processed 30/50 users in batch, avg time: 1.13s per user\n",
      "  Processed 40/50 users in batch, avg time: 1.13s per user\n",
      "  Processed 50/50 users in batch, avg time: 1.14s per user\n",
      "Completed batch 2/4\n",
      "Progress: 50.0% - Elapsed: 113.99s - Est. remaining: 227.98s\n",
      "Processing batch of 50 users (101-150 of 200)\n",
      "  Processed 10/50 users in batch, avg time: 1.14s per user\n",
      "  Processed 20/50 users in batch, avg time: 1.14s per user\n",
      "  Processed 30/50 users in batch, avg time: 1.15s per user\n",
      "  Processed 40/50 users in batch, avg time: 1.15s per user\n",
      "  Processed 50/50 users in batch, avg time: 1.15s per user\n",
      "Completed batch 3/4\n",
      "Progress: 75.0% - Elapsed: 171.95s - Est. remaining: 171.95s\n",
      "Processing batch of 50 users (151-200 of 200)\n",
      "  Processed 10/50 users in batch, avg time: 1.15s per user\n",
      "  Processed 20/50 users in batch, avg time: 1.14s per user\n",
      "  Processed 30/50 users in batch, avg time: 1.14s per user\n",
      "  Processed 40/50 users in batch, avg time: 1.13s per user\n",
      "  Processed 50/50 users in batch, avg time: 1.13s per user\n",
      "Completed batch 4/4\n",
      "Progress: 100.0% - Elapsed: 229.29s - Est. remaining: 0.00s\n",
      "Generated recommendations for 200 users\n",
      "\n",
      "--------------------------------------------------\n",
      "RECOMMENDATION ANALYSIS: DNN RECOMMENDATIONS\n",
      "--------------------------------------------------\n",
      "Saved recommendations to ../rec/collaborative-recommendations\\dnn_recommendations.csv\n",
      "\n",
      "Recommendation Statistics:\n",
      "- Users with recommendations: 200\n",
      "- Total recommendation entries: 4000\n",
      "- Average recommendations per user: 20.00\n",
      "\n",
      "Predicted Rating Distribution:\n",
      "- Min: 3.30\n",
      "- Max: 4.62\n",
      "- Mean: 4.14\n",
      "- Median: 4.23\n",
      "- Std Dev: 0.23\n",
      "Recommendation rating distribution saved to ../rec/collaborative-recommendations\\recommendation_rating_distribution.png\n",
      "\n",
      "Top Recommended Movies:\n",
      "1. 'Underground (1995)' - Recommended to 143 users\n",
      "2. 'Before the Rain (Pred dozhdot) (1994)' - Recommended to 95 users\n",
      "3. 'African Queen, The (1951)' - Recommended to 80 users\n",
      "4. 'Maltese Falcon, The (1941)' - Recommended to 69 users\n",
      "5. 'Blade Runner (1982)' - Recommended to 60 users\n",
      "6. 'Walking Dead, The (1995)' - Recommended to 54 users\n",
      "7. 'Little Odessa (1994)' - Recommended to 52 users\n",
      "8. 'New Jersey Drive (1995)' - Recommended to 51 users\n",
      "9. 'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)' - Recommended to 49 users\n",
      "10. 'Unforgettable (1996)' - Recommended to 47 users\n",
      "Top recommendations genre distribution saved to ../rec/collaborative-recommendations\\top_recommendations_genre_distribution.png\n",
      "\n",
      "Sample Recommendations for 3 Users:\n",
      "\n",
      "User 1:\n",
      "1. Underground (1995) - Predicted Rating: 4.24\n",
      "2. Beat the Devil (1953) - Predicted Rating: 4.20\n",
      "3. Wings of Courage (1995) - Predicted Rating: 4.20\n",
      "4. African Queen, The (1951) - Predicted Rating: 4.14\n",
      "5. Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964) - Predicted Rating: 4.13\n",
      "\n",
      "User 2:\n",
      "1. Tombstone (1993) - Predicted Rating: 4.33\n",
      "2. Quest, The (1996) - Predicted Rating: 4.32\n",
      "3. Phantom, The (1996) - Predicted Rating: 4.32\n",
      "4. Fled (1996) - Predicted Rating: 4.32\n",
      "5. Twelve Monkeys (a.k.a. 12 Monkeys) (1995) - Predicted Rating: 4.30\n",
      "\n",
      "User 4:\n",
      "1. Before the Rain (Pred dozhdot) (1994) - Predicted Rating: 4.28\n",
      "2. Richard III (1995) - Predicted Rating: 4.28\n",
      "3. Misérables, Les (1995) - Predicted Rating: 4.28\n",
      "4. Land and Freedom (Tierra y libertad) (1995) - Predicted Rating: 4.28\n",
      "5. Stalingrad (1993) - Predicted Rating: 4.28\n",
      "\n",
      "================================================================================\n",
      "STEP 7: EVALUATION\n",
      "================================================================================\n",
      "\n",
      "Evaluating DNN recommendations...\n",
      "Evaluating recommendations using RMSE and MAE...\n",
      "Train users: 80000, Test users: 20000, Common users: 0\n",
      "Using user-based split - no common users between train and test.\n",
      "Evaluating using average rating for all predictions instead.\n",
      "Baseline evaluation results (using avg rating 3.53):\n",
      "RMSE: 1.0449\n",
      "MAE: 0.8354\n",
      "Number of predictions: 2941687\n",
      "Saved evaluation metrics to ../rec/collaborative-recommendations\\dnn_evaluation.csv\n",
      "\n",
      "--------------------------------------------------\n",
      "EVALUATION ANALYSIS: MODEL PERFORMANCE\n",
      "--------------------------------------------------\n",
      "\n",
      "Evaluation Metrics:\n",
      "- rmse: 1.0449\n",
      "- mae: 0.8354\n",
      "- num_predictions: 2941687\n",
      "- method: baseline_average_rating\n",
      "\n",
      "Sample recommendation for exploration:\n",
      "\n",
      "User 101 Genre Preferences:\n",
      "- Most liked genres: Drama (1.00), Thriller (1.00), Western Europe (1.00)\n",
      "- Most disliked genres: Comedy (-0.88), North America (-0.88), Action (-0.75)\n",
      "\n",
      "Top 10 recommendations for user 101:\n",
      "1. Colonel Chabert, Le (1994) - Predicted Rating: 4.62\n",
      "2. All Things Fair (Lust och fägring stor) (1995) - Predicted Rating: 4.62\n",
      "3. Hedd Wyn (1992) - Predicted Rating: 4.62\n",
      "4. Beat the Devil (1953) - Predicted Rating: 4.55\n",
      "5. Captives (1994) - Predicted Rating: 4.50\n",
      "6. African Queen, The (1951) - Predicted Rating: 4.47\n",
      "7. Escort, The (Scorta, La) (1993) - Predicted Rating: 4.44\n",
      "8. Before the Rain (Pred dozhdot) (1994) - Predicted Rating: 4.41\n",
      "9. Underground (1995) - Predicted Rating: 4.38\n",
      "10. Purple Noon (Plein soleil) (1960) - Predicted Rating: 4.37\n",
      "\n",
      "================================================================================\n",
      "SUMMARY: COLLABORATIVE FILTERING WITH DNN\n",
      "================================================================================\n",
      "\n",
      "Model Characteristics:\n",
      "- Hidden layer sizes: [64, 32, 16]\n",
      "- Dropout rate: 0.2\n",
      "- Learning rate: 0.001\n",
      "- Batch size: 64\n",
      "\n",
      "Dataset Statistics:\n",
      "- Training samples: 798021\n",
      "- Validation samples: 199506\n",
      "- Feature dimensions: 66\n",
      "- Number of users with genre preferences: 80000\n",
      "- Number of movies with genre features: 24378\n",
      "\n",
      "Performance Metrics:\n",
      "- RMSE: 1.0449\n",
      "- MAE: 0.8354\n",
      "- Predictions evaluated: 2941687\n",
      "\n",
      "Comparison with Other Methods:\n",
      "+-------------------------------+--------+--------+-------------+\n",
      "| Model                         | RMSE   | MAE    | Predictions |\n",
      "+-------------------------------+--------+--------+-------------+\n",
      "| Collaborative Filtering (DNN) | 1.0449 | 0.8354 | 2941687     |\n",
      "+-------------------------------+--------+--------+-------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import time\n",
    "import gc  # For garbage collection\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COLLABORATIVE FILTERING WITH DEEP NEURAL NETWORK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Set paths\n",
    "input_path = \"../processed/\"  # Current directory where stage1.py saved the files\n",
    "output_path = \"../rec/collaborative-recommendations\"\n",
    "top_n = 20\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Model parameters\n",
    "dnn_hidden_layers = [64, 32, 16]  # Optimized architecture \n",
    "dnn_dropout_rate = 0.2\n",
    "dnn_learning_rate = 0.001\n",
    "dnn_batch_size = 64   # Increased batch size for faster training\n",
    "dnn_epochs = 20       # Reduced epochs with early stopping\n",
    "threshold_rating = 3.5  # Rating threshold to classify as \"like\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: DATA LOADING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load processed data from stage1.py\n",
    "    \n",
    "    Input: None (reads from files)\n",
    "    Output: Dictionary containing DataFrames for movie features and ratings\n",
    "    \"\"\"\n",
    "    print(\"Loading processed data from stage1.py...\")\n",
    "    \n",
    "    # Data containers\n",
    "    data = {}\n",
    "    \n",
    "    # Load movie features\n",
    "    movie_features_path = os.path.join(input_path, 'processed_movie_features.csv')\n",
    "    if os.path.exists(movie_features_path):\n",
    "        data['movie_features'] = pd.read_csv(movie_features_path)\n",
    "        print(f\"Loaded features for {len(data['movie_features'])} movies\")\n",
    "    else:\n",
    "        print(f\"Movie features not found at {movie_features_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Load normalized ratings\n",
    "    ratings_path = os.path.join(input_path, 'normalized_ratings.csv')\n",
    "    if os.path.exists(ratings_path):\n",
    "        data['ratings'] = pd.read_csv(ratings_path)\n",
    "        print(f\"Loaded {len(data['ratings'])} normalized ratings\")\n",
    "    else:\n",
    "        print(f\"Normalized ratings not found at {ratings_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Create training and testing sets with 80-20 split\n",
    "    if 'ratings' in data:\n",
    "        # Get all unique user IDs\n",
    "        all_user_ids = data['ratings']['userId'].unique()\n",
    "\n",
    "        # Split users into train (80%) and test (20%) sets\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        np.random.shuffle(all_user_ids)\n",
    "\n",
    "        split_idx = int(len(all_user_ids) * 0.8)\n",
    "        train_users = all_user_ids[:split_idx]\n",
    "        test_users = all_user_ids[split_idx:]\n",
    "\n",
    "        # Split ratings based on user assignments\n",
    "        data['train_ratings'] = data['ratings'][data['ratings']['userId'].isin(train_users)]\n",
    "        data['test_ratings'] = data['ratings'][data['ratings']['userId'].isin(test_users)]\n",
    "        \n",
    "        print(f\"Split ratings into {len(data['train_ratings'])} training and {len(data['test_ratings'])} testing samples\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load the data\n",
    "data = load_data()\n",
    "if data is None:\n",
    "    print(\"Failed to load required data\")\n",
    "    exit(1)\n",
    "\n",
    "# Analyze the loaded data\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"DATA ANALYSIS: LOADED DATASETS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Show movie features summary\n",
    "if 'movie_features' in data:\n",
    "    print(f\"\\nMovie Features Summary:\")\n",
    "    print(f\"- Total movies: {len(data['movie_features'])}\")\n",
    "    \n",
    "    # Get genre columns\n",
    "    genre_columns = [col for col in data['movie_features'].columns if col not in \n",
    "                     ['movieId', 'title', 'tokens', 'token_count', 'top_keywords']]\n",
    "    \n",
    "    print(f\"- Number of genres: {len(genre_columns)}\")\n",
    "    print(f\"- Genre columns: {genre_columns}\")\n",
    "    \n",
    "    # Show sample movie features\n",
    "    print(\"\\nSample movie features:\")\n",
    "    print(data['movie_features'][['movieId', 'title'] + genre_columns[:3]].head(3))\n",
    "\n",
    "# Show ratings summary\n",
    "if 'ratings' in data:\n",
    "    print(f\"\\nRatings Summary:\")\n",
    "    print(f\"- Total ratings: {len(data['ratings'])}\")\n",
    "    print(f\"- Unique users: {data['ratings']['userId'].nunique()}\")\n",
    "    print(f\"- Unique movies: {data['ratings']['movieId'].nunique()}\")\n",
    "    print(f\"- Rating range: {data['ratings']['rating'].min()} - {data['ratings']['rating'].max()}\")\n",
    "    print(f\"- Average rating: {data['ratings']['rating'].mean():.2f}\")\n",
    "    \n",
    "    # Show rating distribution\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(data=data['ratings'], x='rating', bins=9, kde=True)\n",
    "    plt.title('Distribution of Ratings')\n",
    "    plt.xlabel('Rating')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig(os.path.join(output_path, 'rating_distribution.png'))\n",
    "    print(f\"\\nRating distribution plot saved to {os.path.join(output_path, 'rating_distribution.png')}\")\n",
    "    plt.close()\n",
    "\n",
    "# Show train/test split summary\n",
    "if 'train_ratings' in data and 'test_ratings' in data:\n",
    "    print(f\"\\nTrain/Test Split Summary:\")\n",
    "    print(f\"- Training ratings: {len(data['train_ratings'])} ({len(data['train_ratings'])/len(data['ratings'])*100:.1f}%)\")\n",
    "    print(f\"- Testing ratings: {len(data['test_ratings'])} ({len(data['test_ratings'])/len(data['ratings'])*100:.1f}%)\")\n",
    "    print(f\"- Training users: {data['train_ratings']['userId'].nunique()}\")\n",
    "    print(f\"- Testing users: {data['test_ratings']['userId'].nunique()}\")\n",
    "    \n",
    "    # Analyze user rating distribution in train/test sets\n",
    "    train_ratings_per_user = data['train_ratings'].groupby('userId').size()\n",
    "    test_ratings_per_user = data['test_ratings'].groupby('userId').size()\n",
    "    \n",
    "    print(f\"\\nRatings per user:\")\n",
    "    print(f\"- Training set - Avg: {train_ratings_per_user.mean():.2f}, Min: {train_ratings_per_user.min()}, Max: {train_ratings_per_user.max()}\")\n",
    "    print(f\"- Testing set - Avg: {test_ratings_per_user.mean():.2f}, Min: {test_ratings_per_user.min()}, Max: {test_ratings_per_user.max()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: MOVIE GENRE FEATURE EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def extract_genre_features(movie_features):\n",
    "    \"\"\"\n",
    "    Extract genre features for each movie\n",
    "    \n",
    "    Input: \n",
    "      - movie_features: DataFrame with movie features including genre columns\n",
    "    \n",
    "    Output:\n",
    "      - movie_genre_features: DataFrame with movieId and genre columns only\n",
    "    \"\"\"\n",
    "    print(\"Extracting genre features for movies...\")\n",
    "    \n",
    "    # Get all genre columns (assuming they're already one-hot encoded)\n",
    "    genre_columns = [col for col in movie_features.columns if col not in \n",
    "                     ['movieId', 'title', 'tokens', 'token_count', 'top_keywords']]\n",
    "    \n",
    "    if not genre_columns:\n",
    "        print(\"No genre columns found in movie features\")\n",
    "        return None\n",
    "    \n",
    "    # Create genre feature matrix\n",
    "    movie_genre_features = movie_features[['movieId'] + genre_columns].copy()\n",
    "    \n",
    "    print(f\"Extracted {len(genre_columns)} genre features for {len(movie_features)} movies\")\n",
    "    \n",
    "    return movie_genre_features\n",
    "\n",
    "# Extract genre features\n",
    "movie_genre_features = extract_genre_features(data['movie_features'])\n",
    "if movie_genre_features is None:\n",
    "    print(\"Failed to extract genre features\")\n",
    "    exit(1)\n",
    "\n",
    "# Analyze the extracted genre features\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"DATA ANALYSIS: GENRE FEATURES\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Show genre distribution\n",
    "print(\"\\nGenre Distribution:\")\n",
    "genre_columns = [col for col in movie_genre_features.columns if col != 'movieId']\n",
    "genre_counts = {}\n",
    "\n",
    "for genre in genre_columns:\n",
    "    count = movie_genre_features[genre].sum()\n",
    "    genre_counts[genre] = count\n",
    "    print(f\"- {genre}: {count} movies ({count/len(movie_genre_features)*100:.1f}%)\")\n",
    "\n",
    "# Plot genre distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sorted_genres = sorted(genre_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "genres, counts = zip(*sorted_genres)\n",
    "plt.bar(genres, counts)\n",
    "plt.title('Distribution of Movies by Genre')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Number of Movies')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_path, 'genre_distribution.png'))\n",
    "print(f\"\\nGenre distribution plot saved to {os.path.join(output_path, 'genre_distribution.png')}\")\n",
    "plt.close()\n",
    "\n",
    "# Analyze genre co-occurrence\n",
    "print(\"\\nGenre Co-occurrence Analysis:\")\n",
    "genre_co_occurrence = pd.DataFrame(0, index=genre_columns, columns=genre_columns)\n",
    "\n",
    "for _, row in movie_genre_features.iterrows():\n",
    "    movie_genres = [genre for genre in genre_columns if row[genre] == 1]\n",
    "    for g1 in movie_genres:\n",
    "        for g2 in movie_genres:\n",
    "            genre_co_occurrence.loc[g1, g2] += 1\n",
    "\n",
    "# Normalize by diagonal for correlation-like measure\n",
    "for g in genre_columns:\n",
    "    genre_co_occurrence[g] = genre_co_occurrence[g] / genre_co_occurrence.loc[g, g]\n",
    "\n",
    "# Display most common genre combinations\n",
    "print(\"Most common genre combinations:\")\n",
    "for i, g1 in enumerate(genre_columns[:5]):  # Limit to 5 genres for brevity\n",
    "    most_common = genre_co_occurrence.loc[g1].sort_values(ascending=False)[1:6]  # Skip self (always 1.0)\n",
    "    print(f\"- {g1} most commonly appears with: {', '.join([f'{g2} ({v:.2f})' for g2, v in most_common.items()])}\")\n",
    "\n",
    "# Save genre co-occurrence matrix plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(genre_co_occurrence, annot=False, cmap='viridis')\n",
    "plt.title('Genre Co-occurrence Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_path, 'genre_co_occurrence.png'))\n",
    "print(f\"\\nGenre co-occurrence matrix saved to {os.path.join(output_path, 'genre_co_occurrence.png')}\")\n",
    "plt.close()\n",
    "\n",
    "# Show sample of genre features\n",
    "print(\"\\nSample of movie genre features:\")\n",
    "print(movie_genre_features.head(3))\n",
    "\n",
    "# Save the genre features for later use\n",
    "movie_genre_features.to_csv(os.path.join(output_path, 'movie_genre_features.csv'), index=False)\n",
    "print(f\"\\nSaved movie genre features to {os.path.join(output_path, 'movie_genre_features.csv')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: USER GENRE PREFERENCE CALCULATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def calculate_user_genre_preferences(train_ratings, movie_genre_features):\n",
    "    \"\"\"\n",
    "    Calculate user preferences for movie genres based on ratings\n",
    "    \n",
    "    Input:\n",
    "      - train_ratings: DataFrame with user-movie ratings\n",
    "      - movie_genre_features: DataFrame with movie genre features\n",
    "    \n",
    "    Output:\n",
    "      - user_genre_preferences_df: DataFrame with userId and genre preference scores\n",
    "    \"\"\"\n",
    "    print(\"Calculating user preferences for movie genres...\")\n",
    "    \n",
    "    # Get genre columns\n",
    "    genre_columns = [col for col in movie_genre_features.columns if col != 'movieId']\n",
    "    \n",
    "    # Initialize user genre preferences dataframe\n",
    "    user_genre_preferences = []\n",
    "    \n",
    "    # Process each user\n",
    "    total_users = len(train_ratings['userId'].unique())\n",
    "    processed_users = 0\n",
    "    \n",
    "    for user_id in train_ratings['userId'].unique():\n",
    "        # Get user ratings\n",
    "        user_ratings = train_ratings[train_ratings['userId'] == user_id]\n",
    "        \n",
    "        if len(user_ratings) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Separate liked and disliked movies\n",
    "        liked_movies = user_ratings[user_ratings['rating'] > threshold_rating]['movieId'].values\n",
    "        disliked_movies = user_ratings[user_ratings['rating'] <= threshold_rating]['movieId'].values\n",
    "        \n",
    "        # Calculate genre preferences using equation (7) from the paper:\n",
    "        # R̂g = (Nlikes - Ndislikes) / Max(Nlikes - Ndislikes)\n",
    "        genre_preferences = {}\n",
    "        \n",
    "        for genre in genre_columns:\n",
    "            # Get genre values for liked movies\n",
    "            genre_liked = movie_genre_features[movie_genre_features['movieId'].isin(liked_movies)][genre].sum()\n",
    "            \n",
    "            # Get genre values for disliked movies\n",
    "            genre_disliked = movie_genre_features[movie_genre_features['movieId'].isin(disliked_movies)][genre].sum()\n",
    "            \n",
    "            # Calculate preference\n",
    "            genre_preferences[genre] = genre_liked - genre_disliked\n",
    "        \n",
    "        # Calculate maximum absolute genre preference\n",
    "        max_abs_preference = max(abs(val) for val in genre_preferences.values()) if genre_preferences else 1\n",
    "        \n",
    "        # Normalize preferences to [-1, 1]\n",
    "        for genre in genre_preferences:\n",
    "            genre_preferences[genre] = genre_preferences[genre] / max_abs_preference if max_abs_preference > 0 else 0\n",
    "        \n",
    "        # Add user ID\n",
    "        genre_preferences['userId'] = user_id\n",
    "        \n",
    "        user_genre_preferences.append(genre_preferences)\n",
    "        \n",
    "        # Update progress\n",
    "        processed_users += 1\n",
    "        if processed_users % 100 == 0 or processed_users == total_users:\n",
    "            print(f\"Processed {processed_users}/{total_users} users ({processed_users/total_users*100:.1f}%)\")\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    user_genre_preferences_df = pd.DataFrame(user_genre_preferences)\n",
    "    \n",
    "    print(f\"Calculated genre preferences for {len(user_genre_preferences_df)} users\")\n",
    "    \n",
    "    return user_genre_preferences_df\n",
    "\n",
    "# Calculate user genre preferences\n",
    "user_genre_preferences = calculate_user_genre_preferences(data['train_ratings'], movie_genre_features)\n",
    "\n",
    "# Analyze the user genre preferences\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"DATA ANALYSIS: USER GENRE PREFERENCES\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Show basic statistics of user genre preferences\n",
    "if not user_genre_preferences.empty:\n",
    "    print(\"\\nUser Genre Preferences Summary:\")\n",
    "    \n",
    "    genre_columns = [col for col in user_genre_preferences.columns if col != 'userId']\n",
    "    \n",
    "    # Calculate statistics for each genre\n",
    "    genre_stats = {}\n",
    "    for genre in genre_columns:\n",
    "        stats = {\n",
    "            'mean': user_genre_preferences[genre].mean(),\n",
    "            'min': user_genre_preferences[genre].min(),\n",
    "            'max': user_genre_preferences[genre].max(),\n",
    "            'std': user_genre_preferences[genre].std(),\n",
    "            'positive': (user_genre_preferences[genre] > 0).sum(),\n",
    "            'negative': (user_genre_preferences[genre] < 0).sum(),\n",
    "            'neutral': (user_genre_preferences[genre] == 0).sum()\n",
    "        }\n",
    "        genre_stats[genre] = stats\n",
    "    \n",
    "    # Display statistics for top genres\n",
    "    print(\"\\nStatistics for top genres:\")\n",
    "    top_genres = sorted(genre_stats.items(), key=lambda x: x[1]['positive'], reverse=True)[:5]\n",
    "    \n",
    "    for genre, stats in top_genres:\n",
    "        print(f\"- {genre}:\")\n",
    "        print(f\"  * Mean preference: {stats['mean']:.3f} (std: {stats['std']:.3f})\")\n",
    "        print(f\"  * Range: {stats['min']:.3f} to {stats['max']:.3f}\")\n",
    "        print(f\"  * Users with positive preference: {stats['positive']} ({stats['positive']/len(user_genre_preferences)*100:.1f}%)\")\n",
    "        print(f\"  * Users with negative preference: {stats['negative']} ({stats['negative']/len(user_genre_preferences)*100:.1f}%)\")\n",
    "    \n",
    "    # Plot distribution of preferences for top genres\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, (genre, _) in enumerate(top_genres):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        sns.histplot(user_genre_preferences[genre], kde=True)\n",
    "        plt.title(f'Distribution of {genre} Preferences')\n",
    "        plt.xlabel('Preference Score')\n",
    "        plt.ylabel('Number of Users')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_path, 'user_preference_distributions.png'))\n",
    "    print(f\"\\nUser preference distributions saved to {os.path.join(output_path, 'user_preference_distributions.png')}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a correlation heatmap of genre preferences\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    corr_matrix = user_genre_preferences[genre_columns].corr()\n",
    "    sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')\n",
    "    plt.title('Correlation Between Genre Preferences')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_path, 'genre_preference_correlation.png'))\n",
    "    print(f\"\\nGenre preference correlation matrix saved to {os.path.join(output_path, 'genre_preference_correlation.png')}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Show example users with diverse preferences\n",
    "    print(\"\\nExample users with diverse preferences:\")\n",
    "    # Calculate preference diversity as standard deviation across genres\n",
    "    user_genre_preferences['preference_diversity'] = user_genre_preferences[genre_columns].std(axis=1)\n",
    "    \n",
    "    # Get top 3 users with highest diversity\n",
    "    diverse_users = user_genre_preferences.nlargest(3, 'preference_diversity')\n",
    "    for _, user in diverse_users.iterrows():\n",
    "        user_id = user['userId']\n",
    "        print(f\"\\nUser {user_id} (diversity score: {user['preference_diversity']:.3f}):\")\n",
    "        \n",
    "        # Show top 3 liked and disliked genres\n",
    "        user_prefs = [(genre, user[genre]) for genre in genre_columns]\n",
    "        liked_genres = sorted(user_prefs, key=lambda x: x[1], reverse=True)[:3]\n",
    "        disliked_genres = sorted(user_prefs, key=lambda x: x[1])[:3]\n",
    "        \n",
    "        print(f\"- Most liked genres: {', '.join([f'{g} ({v:.2f})' for g, v in liked_genres])}\")\n",
    "        print(f\"- Most disliked genres: {', '.join([f'{g} ({v:.2f})' for g, v in disliked_genres])}\")\n",
    "    \n",
    "    # Remove the temporary column\n",
    "    user_genre_preferences.drop('preference_diversity', axis=1, inplace=True)\n",
    "    \n",
    "    # Show sample of user genre preferences\n",
    "    print(\"\\nSample of user genre preferences:\")\n",
    "    sample_users = user_genre_preferences.sample(3)\n",
    "    for _, user in sample_users.iterrows():\n",
    "        user_id = user['userId']\n",
    "        print(f\"\\nUser {user_id} preferences:\")\n",
    "        # Show top 5 genres with non-zero preferences\n",
    "        user_prefs = [(genre, user[genre]) for genre in genre_columns if user[genre] != 0]\n",
    "        sorted_prefs = sorted(user_prefs, key=lambda x: abs(x[1]), reverse=True)[:5]\n",
    "        for genre, value in sorted_prefs:\n",
    "            print(f\"- {genre}: {value:.3f}\")\n",
    "\n",
    "    # Save the user genre preferences for later use\n",
    "    user_genre_preferences.to_csv(os.path.join(output_path, 'user_genre_preferences.csv'), index=False)\n",
    "    print(f\"\\nSaved user genre preferences to {os.path.join(output_path, 'user_genre_preferences.csv')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: DNN TRAINING DATA PREPARATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def prepare_dnn_training_data(train_ratings, user_genre_preferences, movie_genre_features):\n",
    "    \"\"\"\n",
    "    Prepare training data for the DNN model\n",
    "    \n",
    "    Input:\n",
    "      - train_ratings: DataFrame with user-movie ratings\n",
    "      - user_genre_preferences: DataFrame with user genre preferences\n",
    "      - movie_genre_features: DataFrame with movie genre features\n",
    "    \n",
    "    Output:\n",
    "      - X_train, X_val: Feature matrices for training and validation\n",
    "      - y_train, y_val: Target values for training and validation\n",
    "      - genre_columns: List of genre column names\n",
    "    \"\"\"\n",
    "    print(\"Preparing training data for DNN model...\")\n",
    "    \n",
    "    # Get genre columns\n",
    "    genre_columns = [col for col in movie_genre_features.columns if col != 'movieId']\n",
    "    \n",
    "    # Initialize lists for features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # Process only a sample of ratings for efficiency\n",
    "    sample_size = min(1000000, len(train_ratings))  # Cap at 1M ratings\n",
    "    sampled_ratings = train_ratings.sample(sample_size, random_state=42) if len(train_ratings) > sample_size else train_ratings\n",
    "    \n",
    "    print(f\"Using {len(sampled_ratings)} ratings to train the DNN model\")\n",
    "    \n",
    "    # Process each rating in batches to avoid memory issues\n",
    "    batch_size = 10000\n",
    "    total_ratings = len(sampled_ratings)\n",
    "    processed_ratings = 0\n",
    "    \n",
    "    for batch_start in range(0, total_ratings, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, total_ratings)\n",
    "        ratings_batch = sampled_ratings.iloc[batch_start:batch_end]\n",
    "        \n",
    "        batch_features = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        for _, row in ratings_batch.iterrows():\n",
    "            user_id = row['userId']\n",
    "            movie_id = row['movieId']\n",
    "            rating = row['rating']\n",
    "            \n",
    "            # Skip if user or movie not found\n",
    "            if user_id not in user_genre_preferences['userId'].values or \\\n",
    "               movie_id not in movie_genre_features['movieId'].values:\n",
    "                continue\n",
    "            \n",
    "            # Get user genre preferences\n",
    "            user_prefs = user_genre_preferences[user_genre_preferences['userId'] == user_id].iloc[0]\n",
    "            \n",
    "            # Get movie genres\n",
    "            movie_genres = movie_genre_features[movie_genre_features['movieId'] == movie_id].iloc[0]\n",
    "            \n",
    "            # Create feature vector by combining user preferences and movie genres\n",
    "            feature_vector = []\n",
    "            \n",
    "            for genre in genre_columns:\n",
    "                # Add user preference for this genre\n",
    "                feature_vector.append(user_prefs[genre])\n",
    "                # Add movie genre indicator\n",
    "                feature_vector.append(movie_genres[genre])\n",
    "            \n",
    "            # Use the actual rating as the target\n",
    "            batch_features.append(feature_vector)\n",
    "            batch_labels.append(rating)\n",
    "        \n",
    "        # Extend the main lists\n",
    "        features.extend(batch_features)\n",
    "        labels.extend(batch_labels)\n",
    "        \n",
    "        # Update progress\n",
    "        processed_ratings += len(ratings_batch)\n",
    "        print(f\"Processed {processed_ratings}/{total_ratings} ratings ({processed_ratings/total_ratings*100:.1f}%)\")\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(features, dtype=np.float32)\n",
    "    y = np.array(labels, dtype=np.float32)\n",
    "    \n",
    "    print(f\"Created feature matrix with shape {X.shape} and labels with shape {y.shape}\")\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"Prepared training data with {len(X_train)} samples, validation data with {len(X_val)} samples\")\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val, genre_columns\n",
    "\n",
    "# Prepare DNN training data\n",
    "X_train, X_val, y_train, y_val, genre_columns = prepare_dnn_training_data(\n",
    "    data['train_ratings'], \n",
    "    user_genre_preferences, \n",
    "    movie_genre_features\n",
    ")\n",
    "\n",
    "# Analyze the training data\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"DATA ANALYSIS: DNN TRAINING DATA\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Feature dimension analysis\n",
    "feature_dim = X_train.shape[1]\n",
    "print(f\"\\nFeature Vector Dimension: {feature_dim}\")\n",
    "print(f\"Number of genres: {len(genre_columns)}\")\n",
    "print(f\"Features per genre: 2 (user preference + movie indicator)\")\n",
    "print(f\"Total features: {len(genre_columns) * 2}\")\n",
    "\n",
    "# Analyze distribution of training labels\n",
    "print(\"\\nTraining Labels Distribution:\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(y_train, bins=9, kde=True)\n",
    "plt.title('Distribution of Training Labels (Ratings)')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig(os.path.join(output_path, 'training_labels_distribution.png'))\n",
    "print(f\"Training labels distribution saved to {os.path.join(output_path, 'training_labels_distribution.png')}\")\n",
    "plt.close()\n",
    "\n",
    "# Analyze feature statistics\n",
    "print(\"\\nFeature Statistics:\")\n",
    "feature_means = np.mean(X_train, axis=0)\n",
    "feature_stds = np.std(X_train, axis=0)\n",
    "\n",
    "# Organize features by genre for better interpretation\n",
    "genre_feature_stats = []\n",
    "for i, genre in enumerate(genre_columns):\n",
    "    # User preference feature is at index 2*i\n",
    "    # Movie indicator feature is at index 2*i + 1\n",
    "    user_pref_idx = 2*i\n",
    "    movie_ind_idx = 2*i + 1\n",
    "    \n",
    "    genre_feature_stats.append({\n",
    "        'Genre': genre,\n",
    "        'User_Pref_Mean': feature_means[user_pref_idx],\n",
    "        'User_Pref_Std': feature_stds[user_pref_idx],\n",
    "        'Movie_Ind_Mean': feature_means[movie_ind_idx],\n",
    "        'Movie_Ind_Std': feature_stds[movie_ind_idx]\n",
    "    })\n",
    "\n",
    "# Convert to dataframe for easier analysis\n",
    "feature_stats_df = pd.DataFrame(genre_feature_stats)\n",
    "print(\"\\nFeature statistics by genre (top 5 genres):\")\n",
    "print(feature_stats_df.sort_values('User_Pref_Mean', ascending=False).head())\n",
    "\n",
    "# Plot feature distributions for a few genres\n",
    "plt.figure(figsize=(15, 10))\n",
    "top_genres = feature_stats_df.sort_values('User_Pref_Mean', ascending=False).head(4)['Genre'].values\n",
    "\n",
    "for i, genre in enumerate(top_genres):\n",
    "    genre_idx = genre_columns.index(genre)\n",
    "    user_pref_idx = 2 * genre_idx\n",
    "    movie_ind_idx = 2 * genre_idx + 1\n",
    "    \n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.histplot(X_train[:, user_pref_idx], label='User Preference', alpha=0.7)\n",
    "    plt.title(f'{genre} - User Preference Distribution')\n",
    "    plt.xlabel('Preference Value')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_path, 'feature_distributions.png'))\n",
    "print(f\"Feature distributions saved to {os.path.join(output_path, 'feature_distributions.png')}\")\n",
    "plt.close()\n",
    "\n",
    "# Save a sample of the training data for reference\n",
    "sample_indices = np.random.choice(len(X_train), min(5, len(X_train)), replace=False)\n",
    "sample_data = []\n",
    "\n",
    "for idx in sample_indices:\n",
    "    features = X_train[idx]\n",
    "    rating = y_train[idx]\n",
    "    \n",
    "    sample_features = {}\n",
    "    for i, genre in enumerate(genre_columns):\n",
    "        user_pref_idx = 2*i\n",
    "        movie_ind_idx = 2*i + 1\n",
    "        \n",
    "        sample_features[f\"{genre}_user_pref\"] = features[user_pref_idx]\n",
    "        sample_features[f\"{genre}_movie_ind\"] = features[movie_ind_idx]\n",
    "    \n",
    "    sample_features['rating'] = rating\n",
    "    sample_data.append(sample_features)\n",
    "\n",
    "sample_df = pd.DataFrame(sample_data)\n",
    "print(\"\\nSample of DNN training data (showing first 3 genres for 1 sample):\")\n",
    "print(sample_df.iloc[0][[f\"{genre}_user_pref\" for genre in genre_columns[:3]] + \n",
    "                       [f\"{genre}_movie_ind\" for genre in genre_columns[:3]] + \n",
    "                       ['rating']].to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: DNN MODEL BUILDING AND TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def build_and_train_dnn_model(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Build and train the DNN model for collaborative filtering\n",
    "    \n",
    "    Input:\n",
    "      - X_train, X_val: Feature matrices for training and validation\n",
    "      - y_train, y_val: Target values for training and validation\n",
    "    \n",
    "    Output:\n",
    "      - model: Trained DNN model\n",
    "      - history: Training history\n",
    "    \"\"\"\n",
    "    print(\"Building and training DNN model...\")\n",
    "    \n",
    "    # Set memory limit to avoid OOM errors\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f\"Found {len(gpus)} GPU(s), enabled memory growth\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error setting GPU memory growth: {e}\")\n",
    "    \n",
    "    # Define input dimension\n",
    "    input_dim = X_train.shape[1]\n",
    "    \n",
    "    # Build model based on optimized architecture\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(Dense(dnn_hidden_layers[0], input_dim=input_dim, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dnn_dropout_rate))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for units in dnn_hidden_layers[1:]:\n",
    "        model.add(Dense(units, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dnn_dropout_rate))\n",
    "    \n",
    "    # Output layer - single value for rating prediction\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compile model with Adam optimizer and Mean Squared Error loss for regression\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=dnn_learning_rate),\n",
    "        loss='mse',  # Use MSE for regression\n",
    "        metrics=['mae']  # Track mean absolute error during training\n",
    "    )\n",
    "    \n",
    "    # Define early stopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=dnn_epochs,\n",
    "        batch_size=dnn_batch_size,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
    "    \n",
    "    print(f\"Model training completed. Validation MSE: {val_loss:.4f}, validation MAE: {val_mae:.4f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Build and train DNN model\n",
    "dnn_model, training_history = build_and_train_dnn_model(X_train, X_val, y_train, y_val)\n",
    "\n",
    "# Save DNN model\n",
    "dnn_model.save(os.path.join(output_path, 'dnn_model.h5'))\n",
    "print(f\"Saved DNN model to {os.path.join(output_path, 'dnn_model.h5')}\")\n",
    "\n",
    "# Analyze the training results\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"MODEL ANALYSIS: DNN TRAINING RESULTS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot MSE loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_history.history['loss'], label='Training MSE')\n",
    "plt.plot(training_history.history['val_loss'], label='Validation MSE')\n",
    "plt.title('Model MSE Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend()\n",
    "\n",
    "# Plot MAE\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_history.history['mae'], label='Training MAE')\n",
    "plt.plot(training_history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Model MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_path, 'dnn_training_history.png'))\n",
    "print(f\"Training history plot saved to {os.path.join(output_path, 'dnn_training_history.png')}\")\n",
    "plt.close()\n",
    "\n",
    "# Model architecture summary\n",
    "print(\"\\nDNN Model Architecture:\")\n",
    "dnn_model.summary()\n",
    "\n",
    "# Analyze convergence\n",
    "final_train_loss = training_history.history['loss'][-1]\n",
    "final_val_loss = training_history.history['val_loss'][-1]\n",
    "final_train_mae = training_history.history['mae'][-1]\n",
    "final_val_mae = training_history.history['val_mae'][-1]\n",
    "\n",
    "print(f\"\\nFinal Training Metrics:\")\n",
    "print(f\"- MSE Loss: {final_train_loss:.4f}\")\n",
    "print(f\"- MAE: {final_train_mae:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal Validation Metrics:\")\n",
    "print(f\"- MSE Loss: {final_val_loss:.4f}\")\n",
    "print(f\"- MAE: {final_val_mae:.4f}\")\n",
    "\n",
    "# Calculate RMSE from MSE\n",
    "final_train_rmse = np.sqrt(final_train_loss)\n",
    "final_val_rmse = np.sqrt(final_val_loss)\n",
    "\n",
    "print(f\"\\nFinal RMSE:\")\n",
    "print(f\"- Training RMSE: {final_train_rmse:.4f}\")\n",
    "print(f\"- Validation RMSE: {final_val_rmse:.4f}\")\n",
    "\n",
    "# Analyze prediction quality\n",
    "print(\"\\nPrediction Quality Analysis:\")\n",
    "val_predictions = dnn_model.predict(X_val)\n",
    "val_errors = val_predictions.flatten() - y_val\n",
    "\n",
    "# Create error histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(val_errors, bins=30, alpha=0.7)\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.title('Validation Prediction Error Distribution')\n",
    "plt.xlabel('Prediction Error (Predicted - Actual)')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(output_path, 'prediction_error_distribution.png'))\n",
    "print(f\"Prediction error distribution saved to {os.path.join(output_path, 'prediction_error_distribution.png')}\")\n",
    "plt.close()\n",
    "\n",
    "# Create scatter plot of predicted vs actual\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_val, val_predictions, alpha=0.3)\n",
    "plt.plot([0.5, 5.0], [0.5, 5.0], 'r--')\n",
    "plt.xlabel('Actual Ratings')\n",
    "plt.ylabel('Predicted Ratings')\n",
    "plt.title('Predicted vs Actual Ratings')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(output_path, 'predicted_vs_actual.png'))\n",
    "print(f\"Predicted vs actual plot saved to {os.path.join(output_path, 'predicted_vs_actual.png')}\")\n",
    "plt.close()\n",
    "\n",
    "# Calculate error statistics by rating level\n",
    "error_by_rating = {}\n",
    "for rating in sorted(np.unique(np.round(y_val * 2) / 2)):  # Round to nearest 0.5\n",
    "    mask = (np.round(y_val * 2) / 2 == rating)\n",
    "    if np.sum(mask) > 0:\n",
    "        rating_errors = val_errors[mask]\n",
    "        error_by_rating[rating] = {\n",
    "            'count': len(rating_errors),\n",
    "            'mean_error': np.mean(rating_errors),\n",
    "            'abs_error': np.mean(np.abs(rating_errors)),\n",
    "            'rmse': np.sqrt(np.mean(rating_errors**2))\n",
    "        }\n",
    "\n",
    "print(\"\\nError by rating level:\")\n",
    "error_df = pd.DataFrame.from_dict(error_by_rating, orient='index')\n",
    "print(error_df)\n",
    "\n",
    "# Plot error by rating level\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(error_df.index, error_df['mean_error'])\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Mean Error by Rating Level')\n",
    "plt.xlabel('Actual Rating')\n",
    "plt.ylabel('Mean Error')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(error_df.index, error_df['rmse'])\n",
    "plt.title('RMSE by Rating Level')\n",
    "plt.xlabel('Actual Rating')\n",
    "plt.ylabel('RMSE')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_path, 'error_by_rating.png'))\n",
    "print(f\"Error by rating plot saved to {os.path.join(output_path, 'error_by_rating.png')}\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: MOVIE RECOMMENDATION GENERATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def generate_user_movie_features(user_id, movie_id, user_genre_preferences, movie_genre_features):\n",
    "    \"\"\"\n",
    "    Generate feature vector for a specific user-movie pair\n",
    "    \n",
    "    Input:\n",
    "      - user_id: User ID\n",
    "      - movie_id: Movie ID\n",
    "      - user_genre_preferences: DataFrame with user genre preferences\n",
    "      - movie_genre_features: DataFrame with movie genre features\n",
    "    \n",
    "    Output:\n",
    "      - feature_vector: Feature vector for the user-movie pair\n",
    "    \"\"\"\n",
    "    # Get genre columns\n",
    "    genre_columns = [col for col in movie_genre_features.columns if col != 'movieId']\n",
    "    \n",
    "    # Skip if user or movie not found\n",
    "    if user_id not in user_genre_preferences['userId'].values or \\\n",
    "       movie_id not in movie_genre_features['movieId'].values:\n",
    "        return None\n",
    "    \n",
    "    # Get user genre preferences\n",
    "    user_prefs = user_genre_preferences[user_genre_preferences['userId'] == user_id].iloc[0]\n",
    "    \n",
    "    # Get movie genres\n",
    "    movie_row = movie_genre_features[movie_genre_features['movieId'] == movie_id]\n",
    "    if movie_row.empty:\n",
    "        return None\n",
    "    movie_genres = movie_row.iloc[0]\n",
    "    \n",
    "    # Create feature vector\n",
    "    feature_vector = []\n",
    "    \n",
    "    for genre in genre_columns:\n",
    "        # Add user preference for this genre\n",
    "        feature_vector.append(user_prefs[genre])\n",
    "        # Add movie genre indicator\n",
    "        feature_vector.append(movie_genres[genre])\n",
    "    \n",
    "    return np.array([feature_vector], dtype=np.float32)\n",
    "\n",
    "def generate_dnn_recommendations(user_id, dnn_model, user_genre_preferences, movie_genre_features, train_ratings, n=10):\n",
    "    \"\"\"Optimized version with batched predictions\"\"\"\n",
    "    print(f\"Generating recommendations for user {user_id}...\")\n",
    "    \n",
    "    # Skip if user not found in genre preferences\n",
    "    if user_id not in user_genre_preferences['userId'].values:\n",
    "        print(f\"User {user_id} not found in genre preferences\")\n",
    "        return []\n",
    "    \n",
    "    # Get genre columns\n",
    "    genre_columns = [col for col in movie_genre_features.columns if col != 'movieId']\n",
    "    \n",
    "    # Get user genre preferences\n",
    "    user_prefs = user_genre_preferences[user_genre_preferences['userId'] == user_id].iloc[0]\n",
    "    \n",
    "    # Get movies already rated by the user\n",
    "    rated_movies = set(train_ratings[train_ratings['userId'] == user_id]['movieId'].values)\n",
    "    \n",
    "    # Get unrated movies\n",
    "    unrated_movies = movie_genre_features[~movie_genre_features['movieId'].isin(rated_movies)]\n",
    "    \n",
    "    # Process in batches to avoid memory issues\n",
    "    batch_size = 1000\n",
    "    all_predictions = []\n",
    "    \n",
    "    for i in range(0, len(unrated_movies), batch_size):\n",
    "        batch = unrated_movies.iloc[i:i+batch_size]\n",
    "        \n",
    "        # Create feature vectors in a vectorized way\n",
    "        feature_vectors = []\n",
    "        movie_ids = []\n",
    "        \n",
    "        for _, movie_row in batch.iterrows():\n",
    "            movie_id = movie_row['movieId']\n",
    "            feature_vector = []\n",
    "            \n",
    "            for genre in genre_columns:\n",
    "                feature_vector.append(user_prefs[genre])\n",
    "                feature_vector.append(movie_row[genre])\n",
    "            \n",
    "            feature_vectors.append(feature_vector)\n",
    "            movie_ids.append(movie_id)\n",
    "        \n",
    "        # Convert to numpy array for batch prediction\n",
    "        feature_array = np.array(feature_vectors)\n",
    "        \n",
    "        # Predict in batch\n",
    "        predictions = dnn_model.predict(feature_array, verbose=0).flatten()\n",
    "        \n",
    "        # Ensure ratings are within bounds\n",
    "        predictions = np.clip(predictions, 0.5, 5.0)\n",
    "        \n",
    "        # Add to results\n",
    "        for movie_id, pred in zip(movie_ids, predictions):\n",
    "            all_predictions.append((movie_id, pred))\n",
    "            \n",
    "    # Sort by predicted rating in descending order\n",
    "    all_predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"Generated {len(all_predictions)} predictions for user {user_id}\")\n",
    "    \n",
    "    # Return top N recommendations\n",
    "    return all_predictions[:n]\n",
    "\n",
    "def generate_recommendations_for_all_users(dnn_model, user_genre_preferences, movie_genre_features, train_ratings, n=10, batch_size=50, max_users=None):\n",
    "    \"\"\"\n",
    "    Generate recommendations for all users using the DNN model with improved batching\n",
    "    \n",
    "    Input:\n",
    "      - dnn_model: Trained DNN model\n",
    "      - user_genre_preferences: DataFrame with user genre preferences\n",
    "      - movie_genre_features: DataFrame with movie genre features\n",
    "      - train_ratings: DataFrame with training ratings\n",
    "      - n: Number of recommendations to generate per user\n",
    "      - batch_size: Number of users to process in each batch\n",
    "      - max_users: Maximum number of users to process (optional)\n",
    "    \n",
    "    Output:\n",
    "      - all_recommendations: Dictionary mapping user IDs to recommendation lists\n",
    "    \"\"\"\n",
    "    print(f\"Generating top-{n} DNN recommendations for all users with optimized batching...\")\n",
    "    \n",
    "    # Get all user IDs\n",
    "    all_user_ids = user_genre_preferences['userId'].unique()\n",
    "    \n",
    "    # Limit to max_users if specified\n",
    "    if max_users and max_users < len(all_user_ids):\n",
    "        user_ids = all_user_ids[:max_users]\n",
    "        print(f\"Limiting to {max_users} users out of {len(all_user_ids)} total users\")\n",
    "    else:\n",
    "        user_ids = all_user_ids\n",
    "    \n",
    "    all_recommendations = {}\n",
    "    total_users = len(user_ids)\n",
    "    \n",
    "    # Create a lookup dictionary for user ratings to avoid repeated filtering\n",
    "    print(\"Creating user rating lookup dictionary...\")\n",
    "    user_rated_movies = {}\n",
    "    for _, row in train_ratings.iterrows():\n",
    "        user_id = row['userId']\n",
    "        movie_id = row['movieId']\n",
    "        if user_id not in user_rated_movies:\n",
    "            user_rated_movies[user_id] = set()\n",
    "        user_rated_movies[user_id].add(movie_id)\n",
    "    \n",
    "    # Get genre columns\n",
    "    genre_columns = [col for col in movie_genre_features.columns if col != 'movieId']\n",
    "    \n",
    "    # Process users in batches\n",
    "    start_time = time.time()\n",
    "    for i in range(0, total_users, batch_size):\n",
    "        batch_end = min(i + batch_size, total_users)\n",
    "        batch_users = user_ids[i:batch_end]\n",
    "        \n",
    "        print(f\"Processing batch of {len(batch_users)} users ({i+1}-{batch_end} of {total_users})\")\n",
    "        batch_start_time = time.time()\n",
    "        \n",
    "        # Process each user in the batch\n",
    "        for user_idx, user_id in enumerate(batch_users):\n",
    "            # Skip if user not found in genre preferences\n",
    "            user_prefs = user_genre_preferences[user_genre_preferences['userId'] == user_id]\n",
    "            if user_prefs.empty:\n",
    "                continue\n",
    "            \n",
    "            # Get movies already rated by the user\n",
    "            rated_movies = user_rated_movies.get(user_id, set())\n",
    "            \n",
    "            # Get candidate movies (not yet rated by the user)\n",
    "            # To improve efficiency, we'll use a modified approach:\n",
    "            # 1. Get all unrated movies\n",
    "            unrated_movie_ids = set(movie_genre_features['movieId']) - rated_movies\n",
    "            \n",
    "            # If too many, limit to a manageable number to improve performance\n",
    "            max_movies_per_batch = 1000\n",
    "            if len(unrated_movie_ids) > max_movies_per_batch:\n",
    "                # Convert to list so we can slice it\n",
    "                unrated_movie_ids = list(unrated_movie_ids)[:max_movies_per_batch]\n",
    "            \n",
    "            # Get movie features for unrated movies\n",
    "            candidate_movies = movie_genre_features[movie_genre_features['movieId'].isin(unrated_movie_ids)]\n",
    "            \n",
    "            # If no candidates, skip this user\n",
    "            if len(candidate_movies) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Process candidates in smaller batches to avoid memory issues\n",
    "            movie_batch_size = 200  # Adjust based on memory constraints\n",
    "            predictions = []\n",
    "            \n",
    "            for j in range(0, len(candidate_movies), movie_batch_size):\n",
    "                movie_batch_end = min(j + movie_batch_size, len(candidate_movies))\n",
    "                movie_batch = candidate_movies.iloc[j:movie_batch_end]\n",
    "                \n",
    "                # Create feature vectors for all movies in this batch\n",
    "                batch_features = []\n",
    "                batch_movie_ids = []\n",
    "                \n",
    "                for _, movie_row in movie_batch.iterrows():\n",
    "                    movie_id = movie_row['movieId']\n",
    "                    feature_vector = []\n",
    "                    \n",
    "                    for genre in genre_columns:\n",
    "                        # User preference for this genre\n",
    "                        feature_vector.append(user_prefs.iloc[0][genre])\n",
    "                        # Movie genre indicator\n",
    "                        feature_vector.append(movie_row[genre])\n",
    "                    \n",
    "                    batch_features.append(feature_vector)\n",
    "                    batch_movie_ids.append(movie_id)\n",
    "                \n",
    "                # Convert to numpy array\n",
    "                batch_features = np.array(batch_features, dtype=np.float32)\n",
    "                \n",
    "                # Skip if empty\n",
    "                if len(batch_features) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Make predictions in batch\n",
    "                try:\n",
    "                    batch_predictions = dnn_model.predict(batch_features, verbose=0).flatten()\n",
    "                    \n",
    "                    # Ensure ratings are within bounds\n",
    "                    batch_predictions = np.clip(batch_predictions, 0.5, 5.0)\n",
    "                    \n",
    "                    # Add to predictions list\n",
    "                    for movie_id, pred in zip(batch_movie_ids, batch_predictions):\n",
    "                        predictions.append((movie_id, float(pred)))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error making predictions for user {user_id}, batch {j}: {e}\")\n",
    "            \n",
    "            # Sort predictions by rating and take top n\n",
    "            predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "            all_recommendations[user_id] = predictions[:n]\n",
    "            \n",
    "            # Log progress for every 10th user or the last one\n",
    "            if (user_idx + 1) % 10 == 0 or user_idx == len(batch_users) - 1:\n",
    "                elapsed_batch = time.time() - batch_start_time\n",
    "                avg_time_per_user = elapsed_batch / (user_idx + 1)\n",
    "                print(f\"  Processed {user_idx + 1}/{len(batch_users)} users in batch, avg time: {avg_time_per_user:.2f}s per user\")\n",
    "        \n",
    "        # Log batch completion\n",
    "        elapsed = time.time() - start_time\n",
    "        avg_time_per_batch = elapsed / ((batch_end - i) / batch_size)\n",
    "        progress = batch_end / total_users * 100\n",
    "        remaining = avg_time_per_batch * ((total_users - batch_end) / batch_size) if batch_end < total_users else 0\n",
    "        \n",
    "        print(f\"Completed batch {i//batch_size + 1}/{(total_users-1)//batch_size + 1}\")\n",
    "        print(f\"Progress: {progress:.1f}% - Elapsed: {elapsed:.2f}s - Est. remaining: {remaining:.2f}s\")\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"Generated recommendations for {len(all_recommendations)} users\")\n",
    "    return all_recommendations\n",
    "\n",
    "# Generate DNN recommendations for all users (limiting to a reasonable number for demonstration)\n",
    "max_users = 200  # Adjust based on your computational resources\n",
    "dnn_recommendations = generate_recommendations_for_all_users(\n",
    "    dnn_model,\n",
    "    user_genre_preferences,\n",
    "    movie_genre_features,\n",
    "    data['train_ratings'],\n",
    "    top_n,\n",
    "    batch_size=50,\n",
    "    max_users=max_users\n",
    ")\n",
    "\n",
    "# Save recommendations\n",
    "with open(os.path.join(output_path, 'dnn_recommendations.pkl'), 'wb') as f:\n",
    "    pickle.dump(dnn_recommendations, f)\n",
    "\n",
    "# Analyze the recommendations\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"RECOMMENDATION ANALYSIS: DNN RECOMMENDATIONS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "if dnn_recommendations:\n",
    "    # Create recommendations dataframe for analysis\n",
    "    rec_list = []\n",
    "    for user_id, recs in dnn_recommendations.items():\n",
    "        for rank, (movie_id, rating) in enumerate(recs, 1):\n",
    "            rec_list.append({\n",
    "                'userId': user_id,\n",
    "                'movieId': movie_id,\n",
    "                'predicted_rating': rating,\n",
    "                'rank': rank\n",
    "            })\n",
    "    \n",
    "    rec_df = pd.DataFrame(rec_list)\n",
    "    \n",
    "    # Save in CSV format\n",
    "    if not rec_df.empty:\n",
    "        # Add movie titles if available\n",
    "        if 'movie_features' in data:\n",
    "            movie_titles = data['movie_features'][['movieId', 'title']]\n",
    "            rec_df = pd.merge(rec_df, movie_titles, on='movieId', how='left')\n",
    "        \n",
    "        rec_df.to_csv(os.path.join(output_path, 'dnn_recommendations.csv'), index=False)\n",
    "        print(f\"Saved recommendations to {os.path.join(output_path, 'dnn_recommendations.csv')}\")\n",
    "    \n",
    "    # Basic recommendation statistics\n",
    "    print(f\"\\nRecommendation Statistics:\")\n",
    "    print(f\"- Users with recommendations: {len(dnn_recommendations)}\")\n",
    "    print(f\"- Total recommendation entries: {len(rec_df)}\")\n",
    "    print(f\"- Average recommendations per user: {len(rec_df)/len(dnn_recommendations):.2f}\")\n",
    "    \n",
    "    # Rating distribution\n",
    "    print(f\"\\nPredicted Rating Distribution:\")\n",
    "    rating_stats = rec_df['predicted_rating'].describe()\n",
    "    print(f\"- Min: {rating_stats['min']:.2f}\")\n",
    "    print(f\"- Max: {rating_stats['max']:.2f}\")\n",
    "    print(f\"- Mean: {rating_stats['mean']:.2f}\")\n",
    "    print(f\"- Median: {rating_stats['50%']:.2f}\")\n",
    "    print(f\"- Std Dev: {rating_stats['std']:.2f}\")\n",
    "    \n",
    "    # Plot recommendation rating distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(rec_df['predicted_rating'], bins=20, kde=True)\n",
    "    plt.title('Distribution of Predicted Ratings in Recommendations')\n",
    "    plt.xlabel('Predicted Rating')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig(os.path.join(output_path, 'recommendation_rating_distribution.png'))\n",
    "    print(f\"Recommendation rating distribution saved to {os.path.join(output_path, 'recommendation_rating_distribution.png')}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Analyze top recommended movies\n",
    "    if 'movie_features' in data:\n",
    "        print(\"\\nTop Recommended Movies:\")\n",
    "        top_movies = rec_df.groupby('movieId').size().reset_index(name='count')\n",
    "        top_movies = pd.merge(top_movies, data['movie_features'][['movieId', 'title']], on='movieId')\n",
    "        top_movies = top_movies.sort_values('count', ascending=False).head(10)\n",
    "        \n",
    "        for i, (_, row) in enumerate(top_movies.iterrows(), 1):\n",
    "            print(f\"{i}. '{row['title']}' - Recommended to {row['count']} users\")\n",
    "        \n",
    "        # Get genre distribution of top recommended movies\n",
    "        top_movie_ids = top_movies['movieId'].values\n",
    "        top_movie_genres = movie_genre_features[movie_genre_features['movieId'].isin(top_movie_ids)]\n",
    "        \n",
    "        genre_columns = [col for col in movie_genre_features.columns if col != 'movieId']\n",
    "        genre_counts = {}\n",
    "        \n",
    "        for genre in genre_columns:\n",
    "            count = top_movie_genres[genre].sum()\n",
    "            genre_counts[genre] = count\n",
    "        \n",
    "        # Plot genre distribution of top recommendations\n",
    "        if genre_counts:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sorted_genres = sorted(genre_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "            genres, counts = zip(*sorted_genres)\n",
    "            plt.bar(genres, counts)\n",
    "            plt.title('Genre Distribution of Top Recommended Movies')\n",
    "            plt.xlabel('Genre')\n",
    "            plt.ylabel('Count')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_path, 'top_recommendations_genre_distribution.png'))\n",
    "            print(f\"Top recommendations genre distribution saved to {os.path.join(output_path, 'top_recommendations_genre_distribution.png')}\")\n",
    "            plt.close()\n",
    "    \n",
    "    # Show sample recommendations for a few users\n",
    "    print(\"\\nSample Recommendations for 3 Users:\")\n",
    "    sample_users = list(dnn_recommendations.keys())[:3]\n",
    "    \n",
    "    for user_id in sample_users:\n",
    "        print(f\"\\nUser {user_id}:\")\n",
    "        user_recs = dnn_recommendations[user_id][:5]  # Show top 5\n",
    "        \n",
    "        for i, (movie_id, rating) in enumerate(user_recs, 1):\n",
    "            movie_info = f\"Movie ID: {movie_id}\"\n",
    "            \n",
    "            # Try to get movie title and genres if available\n",
    "            if 'movie_features' in data:\n",
    "                movie_row = data['movie_features'][data['movie_features']['movieId'] == movie_id]\n",
    "                if not movie_row.empty:\n",
    "                    movie_info = movie_row.iloc[0]['title']\n",
    "            \n",
    "            print(f\"{i}. {movie_info} - Predicted Rating: {rating:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 7: EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def evaluate_recommendations(recommendations, test_ratings, dnn_model, user_genre_preferences, movie_genre_features):\n",
    "    \"\"\"\n",
    "    Evaluate recommendations using RMSE and MAE metrics with expanded predictions\n",
    "    Modified to handle user-based train-test split\n",
    "    \"\"\"\n",
    "    print(\"Evaluating recommendations using RMSE and MAE...\")\n",
    "    \n",
    "    # Check if we have a user-based split by checking user overlap\n",
    "    train_users = set(user_genre_preferences['userId'].unique())\n",
    "    test_users = set(test_ratings['userId'].unique())\n",
    "    common_users = train_users.intersection(test_users)\n",
    "    \n",
    "    print(f\"Train users: {len(train_users)}, Test users: {len(test_users)}, Common users: {len(common_users)}\")\n",
    "    \n",
    "    # If no common users, use baseline evaluation with average rating\n",
    "    if len(common_users) == 0:\n",
    "        print(\"Using user-based split - no common users between train and test.\")\n",
    "        print(\"Evaluating using average rating for all predictions instead.\")\n",
    "        \n",
    "        # Calculate average rating from training data\n",
    "        avg_rating = 3.0  # Fallback value\n",
    "        if 'rating' in test_ratings.columns:\n",
    "            # Use the average rating from test data as baseline\n",
    "            avg_rating = test_ratings['rating'].mean()\n",
    "        \n",
    "        # Get test ratings\n",
    "        total_predictions = len(test_ratings)\n",
    "        \n",
    "        if total_predictions == 0:\n",
    "            print(\"No test ratings available for evaluation\")\n",
    "            return {\n",
    "                'rmse': float('inf'),\n",
    "                'mae': float('inf'),\n",
    "                'num_predictions': 0\n",
    "            }\n",
    "        \n",
    "        # Calculate RMSE and MAE using average rating as prediction\n",
    "        squared_errors_sum = ((test_ratings['rating'] - avg_rating) ** 2).sum()\n",
    "        absolute_errors_sum = (abs(test_ratings['rating'] - avg_rating)).sum()\n",
    "        \n",
    "        rmse = np.sqrt(squared_errors_sum / total_predictions)\n",
    "        mae = absolute_errors_sum / total_predictions\n",
    "        \n",
    "        print(f\"Baseline evaluation results (using avg rating {avg_rating:.2f}):\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"Number of predictions: {total_predictions}\")\n",
    "        \n",
    "        return {\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'num_predictions': total_predictions,\n",
    "            'method': 'baseline_average_rating'\n",
    "        }\n",
    "    \n",
    "    # Standard evaluation (original code) for when there are common users\n",
    "    # Initialize lists for predictions and actual ratings\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    # For each user in the test set\n",
    "    for user_id in test_ratings['userId'].unique():\n",
    "        # Skip users without genre preferences\n",
    "        if user_id not in user_genre_preferences['userId'].values:\n",
    "            continue\n",
    "        \n",
    "        # Get user's test ratings\n",
    "        user_test_ratings = test_ratings[test_ratings['userId'] == user_id]\n",
    "        \n",
    "        # Get user's recommendations (movie_id, predicted_rating) if available\n",
    "        user_recs = {}\n",
    "        if user_id in recommendations:\n",
    "            user_recs = dict(recommendations[user_id])\n",
    "        \n",
    "        # Match test ratings with predictions\n",
    "        for _, row in user_test_ratings.iterrows():\n",
    "            movie_id = row['movieId']\n",
    "            actual_rating = row['rating']\n",
    "            \n",
    "            # If the movie is in recommendations\n",
    "            if movie_id in user_recs:\n",
    "                predictions.append(user_recs[movie_id])\n",
    "                actuals.append(actual_rating)\n",
    "            # Otherwise, make a new prediction for this movie\n",
    "            elif movie_id in movie_genre_features['movieId'].values:\n",
    "                # Generate feature vector for this user-movie pair\n",
    "                feature_vector = generate_user_movie_features(\n",
    "                    user_id, \n",
    "                    movie_id, \n",
    "                    user_genre_preferences, \n",
    "                    movie_genre_features\n",
    "                )\n",
    "                \n",
    "                if feature_vector is not None:\n",
    "                    # Predict rating\n",
    "                    predicted_rating = dnn_model.predict(feature_vector, verbose=0)[0][0]\n",
    "                    # Ensure rating is within bounds\n",
    "                    predicted_rating = max(0.5, min(5.0, predicted_rating))\n",
    "                    \n",
    "                    predictions.append(predicted_rating)\n",
    "                    actuals.append(actual_rating)\n",
    "    \n",
    "    \n",
    "    # Check if we have predictions to evaluate\n",
    "    if not predictions:\n",
    "        print(\"No predictions available for evaluation using standard method\")\n",
    "        # Fall back to baseline if we have test ratings\n",
    "        if len(test_ratings) > 0:\n",
    "            print(\"Falling back to baseline evaluation method\")\n",
    "            avg_rating = test_ratings['rating'].mean() if 'rating' in test_ratings.columns else 3.0\n",
    "            total_predictions = len(test_ratings)\n",
    "            \n",
    "            # Calculate RMSE and MAE using average rating\n",
    "            squared_errors_sum = ((test_ratings['rating'] - avg_rating) ** 2).sum()\n",
    "            absolute_errors_sum = (abs(test_ratings['rating'] - avg_rating)).sum()\n",
    "            \n",
    "            rmse = np.sqrt(squared_errors_sum / total_predictions)\n",
    "            mae = absolute_errors_sum / total_predictions\n",
    "            \n",
    "            print(f\"Baseline evaluation results:\")\n",
    "            print(f\"RMSE: {rmse:.4f}\")\n",
    "            print(f\"MAE: {mae:.4f}\")\n",
    "            print(f\"Number of predictions: {total_predictions}\")\n",
    "            \n",
    "            return {\n",
    "                'rmse': rmse,\n",
    "                'mae': mae,\n",
    "                'num_predictions': total_predictions,\n",
    "                'method': 'baseline_average_rating'\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'rmse': float('inf'),\n",
    "            'mae': float('inf'),\n",
    "            'num_predictions': 0\n",
    "        }\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "    \n",
    "    # Calculate RMSE and MAE\n",
    "    rmse = np.sqrt(np.mean((predictions - actuals) ** 2))\n",
    "    mae = np.mean(np.abs(predictions - actuals))\n",
    "    \n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'num_predictions': len(predictions),\n",
    "        'method': 'standard'\n",
    "    }\n",
    "    \n",
    "    print(f\"Evaluation completed - RMSE: {rmse:.4f}, MAE: {mae:.4f}, Predictions: {len(predictions)}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def recommend_for_user(user_id, recommendations, movie_features=None, n=10):\n",
    "    \"\"\"\n",
    "    Print recommendations for a specific user\n",
    "    \n",
    "    Input:\n",
    "      - user_id: User ID to display recommendations for\n",
    "      - recommendations: Dictionary with recommendation lists\n",
    "      - movie_features: DataFrame with movie features (for titles)\n",
    "      - n: Number of recommendations to display\n",
    "    \n",
    "    Output:\n",
    "      - None (prints recommendations)\n",
    "    \"\"\"\n",
    "    # Check if user has recommendations\n",
    "    if user_id not in recommendations:\n",
    "        print(f\"No recommendations found for user {user_id}\")\n",
    "        return\n",
    "    \n",
    "    # Get recommendations\n",
    "    user_recs = recommendations[user_id][:n]\n",
    "    \n",
    "    if not user_recs:\n",
    "        print(f\"No recommendations found for user {user_id}\")\n",
    "        return\n",
    "    \n",
    "    # Print recommendations\n",
    "    print(f\"\\nTop {len(user_recs)} recommendations for user {user_id}:\")\n",
    "    \n",
    "    for i, (movie_id, predicted_rating) in enumerate(user_recs, 1):\n",
    "        movie_info = f\"Movie ID: {movie_id}\"\n",
    "        \n",
    "        # Try to get movie title if available\n",
    "        if movie_features is not None:\n",
    "            movie_row = movie_features[movie_features['movieId'] == movie_id]\n",
    "            if not movie_row.empty and 'title' in movie_row.columns:\n",
    "                movie_info = movie_row.iloc[0]['title']\n",
    "        \n",
    "        print(f\"{i}. {movie_info} - Predicted Rating: {predicted_rating:.2f}\")\n",
    "\n",
    "# Evaluate recommendations\n",
    "print(\"\\nEvaluating DNN recommendations...\")\n",
    "evaluation_metrics = evaluate_recommendations(\n",
    "    dnn_recommendations,\n",
    "    data['test_ratings'],\n",
    "    dnn_model,\n",
    "    user_genre_preferences,\n",
    "    movie_genre_features\n",
    ")\n",
    "\n",
    "# Save evaluation metrics\n",
    "if evaluation_metrics:\n",
    "    evaluation_results = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_results.to_csv(os.path.join(output_path, 'dnn_evaluation.csv'), index=False)\n",
    "    print(f\"Saved evaluation metrics to {os.path.join(output_path, 'dnn_evaluation.csv')}\")\n",
    "\n",
    "# Analyze the evaluation results\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"EVALUATION ANALYSIS: MODEL PERFORMANCE\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "if evaluation_metrics:\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    for key, value in evaluation_metrics.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"- {key}: {value:.4f}\" if isinstance(value, float) else f\"- {key}: {value}\")\n",
    "        else:\n",
    "            print(f\"- {key}: {value}\")\n",
    "    \n",
    "    # Compare with baseline if possible\n",
    "    if 'method' in evaluation_metrics and evaluation_metrics['method'] == 'standard':\n",
    "        # Calculate baseline (using average rating)\n",
    "        if 'train_ratings' in data:\n",
    "            avg_rating = data['train_ratings']['rating'].mean()\n",
    "            user_avg_ratings = data['train_ratings'].groupby('userId')['rating'].mean()\n",
    "            \n",
    "            # Get actual ratings used in evaluation\n",
    "            baseline_errors = []\n",
    "            personalized_baseline_errors = []\n",
    "            \n",
    "            for user_id in test_ratings['userId'].unique():\n",
    "                if user_id not in user_genre_preferences['userId'].values:\n",
    "                    continue\n",
    "                \n",
    "                user_test_ratings = test_ratings[test_ratings['userId'] == user_id]\n",
    "                \n",
    "                for _, row in user_test_ratings.iterrows():\n",
    "                    actual_rating = row['rating']\n",
    "                    \n",
    "                    # Global average baseline\n",
    "                    baseline_errors.append((avg_rating - actual_rating)**2)\n",
    "                    \n",
    "                    # Per-user average baseline\n",
    "                    if user_id in user_avg_ratings.index:\n",
    "                        user_avg = user_avg_ratings[user_id]\n",
    "                        personalized_baseline_errors.append((user_avg - actual_rating)**2)\n",
    "            \n",
    "            if baseline_errors:\n",
    "                baseline_rmse = np.sqrt(np.mean(baseline_errors))\n",
    "                print(f\"\\nBaseline RMSE (global average): {baseline_rmse:.4f}\")\n",
    "                print(f\"Improvement over baseline: {(baseline_rmse - evaluation_metrics['rmse'])/baseline_rmse*100:.2f}%\")\n",
    "            \n",
    "            if personalized_baseline_errors:\n",
    "                personalized_baseline_rmse = np.sqrt(np.mean(personalized_baseline_errors))\n",
    "                print(f\"\\nBaseline RMSE (user average): {personalized_baseline_rmse:.4f}\")\n",
    "                print(f\"Improvement over personalized baseline: {(personalized_baseline_rmse - evaluation_metrics['rmse'])/personalized_baseline_rmse*100:.2f}%\")\n",
    "\n",
    "# Display a sample recommendation for user exploration\n",
    "print(\"\\nSample recommendation for exploration:\")\n",
    "if dnn_recommendations:\n",
    "    # Pick a random user\n",
    "    sample_user_id = np.random.choice(list(dnn_recommendations.keys()))\n",
    "    \n",
    "    # Get user's genre preferences\n",
    "    if sample_user_id in user_genre_preferences['userId'].values:\n",
    "        user_prefs = user_genre_preferences[user_genre_preferences['userId'] == sample_user_id].iloc[0]\n",
    "        genre_columns = [col for col in user_genre_preferences.columns if col != 'userId']\n",
    "        \n",
    "        print(f\"\\nUser {sample_user_id} Genre Preferences:\")\n",
    "        # Show top 3 liked and disliked genres\n",
    "        user_prefs_list = [(genre, user_prefs[genre]) for genre in genre_columns]\n",
    "        liked_genres = sorted(user_prefs_list, key=lambda x: x[1], reverse=True)[:3]\n",
    "        disliked_genres = sorted(user_prefs_list, key=lambda x: x[1])[:3]\n",
    "        \n",
    "        print(f\"- Most liked genres: {', '.join([f'{g} ({v:.2f})' for g, v in liked_genres])}\")\n",
    "        print(f\"- Most disliked genres: {', '.join([f'{g} ({v:.2f})' for g, v in disliked_genres])}\")\n",
    "    \n",
    "    # Show recommendations\n",
    "    recommend_for_user(sample_user_id, dnn_recommendations, data['movie_features'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: COLLABORATIVE FILTERING WITH DNN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Final summary of model performance and characteristics\n",
    "print(\"\\nModel Characteristics:\")\n",
    "print(f\"- Hidden layer sizes: {dnn_hidden_layers}\")\n",
    "print(f\"- Dropout rate: {dnn_dropout_rate}\")\n",
    "print(f\"- Learning rate: {dnn_learning_rate}\")\n",
    "print(f\"- Batch size: {dnn_batch_size}\")\n",
    "\n",
    "# Display dataset statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"- Training samples: {len(X_train)}\")\n",
    "print(f\"- Validation samples: {len(X_val)}\")\n",
    "print(f\"- Feature dimensions: {X_train.shape[1]}\")\n",
    "print(f\"- Number of users with genre preferences: {len(user_genre_preferences)}\")\n",
    "print(f\"- Number of movies with genre features: {len(movie_genre_features)}\")\n",
    "\n",
    "# Show performance metrics\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "if evaluation_metrics:\n",
    "    print(f\"- RMSE: {evaluation_metrics['rmse']:.4f}\")\n",
    "    print(f\"- MAE: {evaluation_metrics['mae']:.4f}\")\n",
    "    print(f\"- Predictions evaluated: {evaluation_metrics['num_predictions']}\")\n",
    "\n",
    "# Display comparison table with other methods\n",
    "print(\"\\nComparison with Other Methods:\")\n",
    "headers = [\"Model\", \"RMSE\", \"MAE\", \"Predictions\"]\n",
    "rows = [\n",
    "    [\n",
    "        \"Collaborative Filtering (DNN)\",\n",
    "        f\"{evaluation_metrics['rmse']:.4f}\" if evaluation_metrics else \"N/A\",\n",
    "        f\"{evaluation_metrics['mae']:.4f}\" if evaluation_metrics else \"N/A\",\n",
    "        f\"{evaluation_metrics['num_predictions']}\" if evaluation_metrics else \"N/A\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Print table\n",
    "col_widths = [max(len(row[i]) for row in [headers] + rows) for i in range(len(headers))]\n",
    "print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")\n",
    "print(\"| \" + \" | \".join(headers[i].ljust(col_widths[i]) for i in range(len(headers))) + \" |\")\n",
    "print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")\n",
    "for row in rows:\n",
    "    print(\"| \" + \" | \".join(row[i].ljust(col_widths[i]) for i in range(len(row))) + \" |\")\n",
    "print(\"+\" + \"+\".join(\"-\" * (width + 2) for width in col_widths) + \"+\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
