{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONTENT-BASED MOVIE RECOMMENDATION SYSTEM WITH LOG-LIKELIHOOD AND WORD2VEC\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 1: DATA LOADING\n",
      "================================================================================\n",
      "Loading processed data from stage1.py...\n",
      "Loaded features for 10993 movies\n",
      "\n",
      "Sample of movie features data:\n",
      "   movieId                    title  \\\n",
      "0        1         Toy Story (1995)   \n",
      "1        2           Jumanji (1995)   \n",
      "2        3  Grumpier Old Men (1995)   \n",
      "\n",
      "                                      top_keywords  \n",
      "0        [woody, andy, buzz, animation, toy story]  \n",
      "1                [game, time, board, alan, travel]  \n",
      "2  [max, local, grumpier old men, family, wedding]  \n",
      "\n",
      "Average token count per movie: 29.74\n",
      "Min token count: 1, Max token count: 101\n",
      "\n",
      "Loaded 290944 normalized ratings\n",
      "\n",
      "Sample of normalized ratings data:\n",
      "   userId  movieId  rating  normalized_rating\n",
      "0       1        2     3.5           0.666667\n",
      "1       1       29     3.5           0.666667\n",
      "2       1       32     3.5           0.666667\n",
      "\n",
      "Number of unique users: 2000\n",
      "Number of unique movies: 10993\n",
      "Rating sparsity: 98.6767%\n",
      "\n",
      "Split ratings into 231967 training and 58977 testing samples\n",
      "Training set covers 2000 users and 8577 movies\n",
      "Testing set covers 2000 users and 8513 movies\n",
      "\n",
      "================================================================================\n",
      "STEP 2: CORPUS ANALYSIS\n",
      "================================================================================\n",
      "Building vocabulary and word frequency counts...\n",
      "Processed 1000/10993 movies (9.1%)\n",
      "Processed 2000/10993 movies (18.2%)\n",
      "Processed 3000/10993 movies (27.3%)\n",
      "Processed 4000/10993 movies (36.4%)\n",
      "Processed 5000/10993 movies (45.5%)\n",
      "Processed 6000/10993 movies (54.6%)\n",
      "Processed 7000/10993 movies (63.7%)\n",
      "Processed 8000/10993 movies (72.8%)\n",
      "Processed 9000/10993 movies (81.9%)\n",
      "Processed 10000/10993 movies (91.0%)\n",
      "Processed 10993/10993 movies (100.0%)\n",
      "Built vocabulary with 42589 unique words\n",
      "Total words in corpus: 326967\n",
      "\n",
      "Top 20 most common words in the corpus:\n",
      "'life': 2397\n",
      "'find': 1712\n",
      "'one': 1462\n",
      "'young': 1436\n",
      "'man': 1300\n",
      "'year': 1290\n",
      "'friend': 1156\n",
      "'family': 1134\n",
      "'love': 1129\n",
      "'woman': 1127\n",
      "'two': 1126\n",
      "'get': 1080\n",
      "'take': 1000\n",
      "'world': 925\n",
      "'time': 912\n",
      "'new': 899\n",
      "'story': 833\n",
      "'must': 806\n",
      "'father': 797\n",
      "'old': 790\n",
      "\n",
      "================================================================================\n",
      "STEP 3: LOG-LIKELIHOOD CALCULATION\n",
      "================================================================================\n",
      "Calculating Log-Likelihood values for all movies in batches...\n",
      "Total corpus size: 326967 words\n",
      "Processing batch 1: movies 1-100 of 10993\n",
      "Progress: 0.9% - Elapsed: 0.02s - Est. remaining: 2.15s\n",
      "Processing batch 2: movies 101-200 of 10993\n",
      "Progress: 1.8% - Elapsed: 0.74s - Est. remaining: 80.07s\n",
      "Processing batch 3: movies 201-300 of 10993\n",
      "Progress: 2.7% - Elapsed: 1.38s - Est. remaining: 147.63s\n",
      "Processing batch 4: movies 301-400 of 10993\n",
      "Progress: 3.6% - Elapsed: 1.97s - Est. remaining: 208.67s\n",
      "Processing batch 5: movies 401-500 of 10993\n",
      "Progress: 4.5% - Elapsed: 2.45s - Est. remaining: 257.35s\n",
      "Processing batch 6: movies 501-600 of 10993\n",
      "Progress: 5.5% - Elapsed: 2.91s - Est. remaining: 302.47s\n",
      "Processing batch 7: movies 601-700 of 10993\n",
      "Progress: 6.4% - Elapsed: 3.34s - Est. remaining: 344.22s\n",
      "Processing batch 8: movies 701-800 of 10993\n",
      "Progress: 7.3% - Elapsed: 3.80s - Est. remaining: 387.55s\n",
      "Processing batch 9: movies 801-900 of 10993\n",
      "Progress: 8.2% - Elapsed: 4.24s - Est. remaining: 428.16s\n",
      "Processing batch 10: movies 901-1000 of 10993\n",
      "Progress: 9.1% - Elapsed: 4.71s - Est. remaining: 470.69s\n",
      "Processing batch 11: movies 1001-1100 of 10993\n",
      "Progress: 10.0% - Elapsed: 5.22s - Est. remaining: 516.73s\n",
      "Processing batch 12: movies 1101-1200 of 10993\n",
      "Progress: 10.9% - Elapsed: 5.74s - Est. remaining: 561.63s\n",
      "Processing batch 13: movies 1201-1300 of 10993\n",
      "Progress: 11.8% - Elapsed: 6.26s - Est. remaining: 607.23s\n",
      "Processing batch 14: movies 1301-1400 of 10993\n",
      "Progress: 12.7% - Elapsed: 6.82s - Est. remaining: 653.83s\n",
      "Processing batch 15: movies 1401-1500 of 10993\n",
      "Progress: 13.6% - Elapsed: 7.25s - Est. remaining: 688.45s\n",
      "Processing batch 16: movies 1501-1600 of 10993\n",
      "Progress: 14.6% - Elapsed: 7.68s - Est. remaining: 721.27s\n",
      "Processing batch 17: movies 1601-1700 of 10993\n",
      "Progress: 15.5% - Elapsed: 8.12s - Est. remaining: 754.24s\n",
      "Processing batch 18: movies 1701-1800 of 10993\n",
      "Progress: 16.4% - Elapsed: 8.57s - Est. remaining: 787.85s\n",
      "Processing batch 19: movies 1801-1900 of 10993\n",
      "Progress: 17.3% - Elapsed: 9.13s - Est. remaining: 830.44s\n",
      "Processing batch 20: movies 1901-2000 of 10993\n",
      "Progress: 18.2% - Elapsed: 9.62s - Est. remaining: 865.05s\n",
      "Processing batch 21: movies 2001-2100 of 10993\n",
      "Progress: 19.1% - Elapsed: 10.06s - Est. remaining: 894.65s\n",
      "Processing batch 22: movies 2101-2200 of 10993\n",
      "Progress: 20.0% - Elapsed: 10.50s - Est. remaining: 923.54s\n",
      "Processing batch 23: movies 2201-2300 of 10993\n",
      "Progress: 20.9% - Elapsed: 10.97s - Est. remaining: 953.26s\n",
      "Processing batch 24: movies 2301-2400 of 10993\n",
      "Progress: 21.8% - Elapsed: 11.41s - Est. remaining: 980.50s\n",
      "Processing batch 25: movies 2401-2500 of 10993\n",
      "Progress: 22.7% - Elapsed: 11.84s - Est. remaining: 1005.63s\n",
      "Processing batch 26: movies 2501-2600 of 10993\n",
      "Progress: 23.7% - Elapsed: 12.30s - Est. remaining: 1032.09s\n",
      "Processing batch 27: movies 2601-2700 of 10993\n",
      "Progress: 24.6% - Elapsed: 12.75s - Est. remaining: 1057.18s\n",
      "Processing batch 28: movies 2701-2800 of 10993\n",
      "Progress: 25.5% - Elapsed: 13.20s - Est. remaining: 1081.72s\n",
      "Processing batch 29: movies 2801-2900 of 10993\n",
      "Progress: 26.4% - Elapsed: 13.69s - Est. remaining: 1107.74s\n",
      "Processing batch 30: movies 2901-3000 of 10993\n",
      "Progress: 27.3% - Elapsed: 14.12s - Est. remaining: 1128.66s\n",
      "Processing batch 31: movies 3001-3100 of 10993\n",
      "Progress: 28.2% - Elapsed: 14.56s - Est. remaining: 1148.89s\n",
      "Processing batch 32: movies 3101-3200 of 10993\n",
      "Progress: 29.1% - Elapsed: 14.98s - Est. remaining: 1167.59s\n",
      "Processing batch 33: movies 3201-3300 of 10993\n",
      "Progress: 30.0% - Elapsed: 15.50s - Est. remaining: 1192.17s\n",
      "Processing batch 34: movies 3301-3400 of 10993\n",
      "Progress: 30.9% - Elapsed: 15.96s - Est. remaining: 1212.02s\n",
      "Processing batch 35: movies 3401-3500 of 10993\n",
      "Progress: 31.8% - Elapsed: 16.43s - Est. remaining: 1230.95s\n",
      "Processing batch 36: movies 3501-3600 of 10993\n",
      "Progress: 32.7% - Elapsed: 16.87s - Est. remaining: 1247.19s\n",
      "Processing batch 37: movies 3601-3700 of 10993\n",
      "Progress: 33.7% - Elapsed: 17.32s - Est. remaining: 1263.10s\n",
      "Processing batch 38: movies 3701-3800 of 10993\n",
      "Progress: 34.6% - Elapsed: 17.85s - Est. remaining: 1283.63s\n",
      "Processing batch 39: movies 3801-3900 of 10993\n",
      "Progress: 35.5% - Elapsed: 18.31s - Est. remaining: 1298.69s\n",
      "Processing batch 40: movies 3901-4000 of 10993\n",
      "Progress: 36.4% - Elapsed: 18.90s - Est. remaining: 1321.92s\n",
      "Processing batch 41: movies 4001-4100 of 10993\n",
      "Progress: 37.3% - Elapsed: 19.34s - Est. remaining: 1333.38s\n",
      "Processing batch 42: movies 4101-4200 of 10993\n",
      "Progress: 38.2% - Elapsed: 19.82s - Est. remaining: 1346.43s\n",
      "Processing batch 43: movies 4201-4300 of 10993\n",
      "Progress: 39.1% - Elapsed: 20.27s - Est. remaining: 1356.90s\n",
      "Processing batch 44: movies 4301-4400 of 10993\n",
      "Progress: 40.0% - Elapsed: 20.69s - Est. remaining: 1364.07s\n",
      "Processing batch 45: movies 4401-4500 of 10993\n",
      "Progress: 40.9% - Elapsed: 21.23s - Est. remaining: 1378.52s\n",
      "Processing batch 46: movies 4501-4600 of 10993\n",
      "Progress: 41.8% - Elapsed: 21.75s - Est. remaining: 1390.38s\n",
      "Processing batch 47: movies 4601-4700 of 10993\n",
      "Progress: 42.8% - Elapsed: 22.20s - Est. remaining: 1397.25s\n",
      "Processing batch 48: movies 4701-4800 of 10993\n",
      "Progress: 43.7% - Elapsed: 22.71s - Est. remaining: 1406.46s\n",
      "Processing batch 49: movies 4801-4900 of 10993\n",
      "Progress: 44.6% - Elapsed: 23.16s - Est. remaining: 1410.96s\n",
      "Processing batch 50: movies 4901-5000 of 10993\n",
      "Progress: 45.5% - Elapsed: 23.59s - Est. remaining: 1413.68s\n",
      "Processing batch 51: movies 5001-5100 of 10993\n",
      "Progress: 46.4% - Elapsed: 24.03s - Est. remaining: 1416.13s\n",
      "Processing batch 52: movies 5101-5200 of 10993\n",
      "Progress: 47.3% - Elapsed: 24.49s - Est. remaining: 1418.78s\n",
      "Processing batch 53: movies 5201-5300 of 10993\n",
      "Progress: 48.2% - Elapsed: 24.97s - Est. remaining: 1421.81s\n",
      "Processing batch 54: movies 5301-5400 of 10993\n",
      "Progress: 49.1% - Elapsed: 25.41s - Est. remaining: 1421.20s\n",
      "Processing batch 55: movies 5401-5500 of 10993\n",
      "Progress: 50.0% - Elapsed: 25.85s - Est. remaining: 1420.04s\n",
      "Processing batch 56: movies 5501-5600 of 10993\n",
      "Progress: 50.9% - Elapsed: 26.31s - Est. remaining: 1418.88s\n",
      "Processing batch 57: movies 5601-5700 of 10993\n",
      "Progress: 51.9% - Elapsed: 26.76s - Est. remaining: 1416.37s\n",
      "Processing batch 58: movies 5701-5800 of 10993\n",
      "Progress: 52.8% - Elapsed: 27.21s - Est. remaining: 1413.23s\n",
      "Processing batch 59: movies 5801-5900 of 10993\n",
      "Progress: 53.7% - Elapsed: 27.72s - Est. remaining: 1412.01s\n",
      "Processing batch 60: movies 5901-6000 of 10993\n",
      "Progress: 54.6% - Elapsed: 28.16s - Est. remaining: 1406.17s\n",
      "Processing batch 61: movies 6001-6100 of 10993\n",
      "Progress: 55.5% - Elapsed: 28.60s - Est. remaining: 1399.40s\n",
      "Processing batch 62: movies 6101-6200 of 10993\n",
      "Progress: 56.4% - Elapsed: 29.05s - Est. remaining: 1392.23s\n",
      "Processing batch 63: movies 6201-6300 of 10993\n",
      "Progress: 57.3% - Elapsed: 29.50s - Est. remaining: 1384.26s\n",
      "Processing batch 64: movies 6301-6400 of 10993\n",
      "Progress: 58.2% - Elapsed: 29.95s - Est. remaining: 1375.74s\n",
      "Processing batch 65: movies 6401-6500 of 10993\n",
      "Progress: 59.1% - Elapsed: 30.39s - Est. remaining: 1365.64s\n",
      "Processing batch 66: movies 6501-6600 of 10993\n",
      "Progress: 60.0% - Elapsed: 30.81s - Est. remaining: 1353.54s\n",
      "Processing batch 67: movies 6601-6700 of 10993\n",
      "Progress: 60.9% - Elapsed: 31.29s - Est. remaining: 1343.21s\n",
      "Processing batch 68: movies 6701-6800 of 10993\n",
      "Progress: 61.9% - Elapsed: 31.73s - Est. remaining: 1330.25s\n",
      "Processing batch 69: movies 6801-6900 of 10993\n",
      "Progress: 62.8% - Elapsed: 32.19s - Est. remaining: 1317.35s\n",
      "Processing batch 70: movies 6901-7000 of 10993\n",
      "Progress: 63.7% - Elapsed: 32.65s - Est. remaining: 1303.81s\n",
      "Processing batch 71: movies 7001-7100 of 10993\n",
      "Progress: 64.6% - Elapsed: 33.09s - Est. remaining: 1288.11s\n",
      "Processing batch 72: movies 7101-7200 of 10993\n",
      "Progress: 65.5% - Elapsed: 33.51s - Est. remaining: 1271.00s\n",
      "Processing batch 73: movies 7201-7300 of 10993\n",
      "Progress: 66.4% - Elapsed: 33.95s - Est. remaining: 1253.77s\n",
      "Processing batch 74: movies 7301-7400 of 10993\n",
      "Progress: 67.3% - Elapsed: 34.39s - Est. remaining: 1235.67s\n",
      "Processing batch 75: movies 7401-7500 of 10993\n",
      "Progress: 68.2% - Elapsed: 34.81s - Est. remaining: 1216.01s\n",
      "Processing batch 76: movies 7501-7600 of 10993\n",
      "Progress: 69.1% - Elapsed: 35.28s - Est. remaining: 1197.12s\n",
      "Processing batch 77: movies 7601-7700 of 10993\n",
      "Progress: 70.0% - Elapsed: 35.79s - Est. remaining: 1178.49s\n",
      "Processing batch 78: movies 7701-7800 of 10993\n",
      "Progress: 71.0% - Elapsed: 36.37s - Est. remaining: 1161.18s\n",
      "Processing batch 79: movies 7801-7900 of 10993\n",
      "Progress: 71.9% - Elapsed: 36.91s - Est. remaining: 1141.55s\n",
      "Processing batch 80: movies 7901-8000 of 10993\n",
      "Progress: 72.8% - Elapsed: 37.43s - Est. remaining: 1120.22s\n",
      "Processing batch 81: movies 8001-8100 of 10993\n",
      "Progress: 73.7% - Elapsed: 38.00s - Est. remaining: 1099.34s\n",
      "Processing batch 82: movies 8101-8200 of 10993\n",
      "Progress: 74.6% - Elapsed: 38.73s - Est. remaining: 1081.81s\n",
      "Processing batch 83: movies 8201-8300 of 10993\n",
      "Progress: 75.5% - Elapsed: 39.30s - Est. remaining: 1058.26s\n",
      "Processing batch 84: movies 8301-8400 of 10993\n",
      "Progress: 76.4% - Elapsed: 39.85s - Est. remaining: 1033.29s\n",
      "Processing batch 85: movies 8401-8500 of 10993\n",
      "Progress: 77.3% - Elapsed: 40.41s - Est. remaining: 1007.44s\n",
      "Processing batch 86: movies 8501-8600 of 10993\n",
      "Progress: 78.2% - Elapsed: 40.91s - Est. remaining: 978.97s\n",
      "Processing batch 87: movies 8601-8700 of 10993\n",
      "Progress: 79.1% - Elapsed: 41.49s - Est. remaining: 951.34s\n",
      "Processing batch 88: movies 8701-8800 of 10993\n",
      "Progress: 80.1% - Elapsed: 42.02s - Est. remaining: 921.43s\n",
      "Processing batch 89: movies 8801-8900 of 10993\n",
      "Progress: 81.0% - Elapsed: 42.47s - Est. remaining: 888.80s\n",
      "Processing batch 90: movies 8901-9000 of 10993\n",
      "Progress: 81.9% - Elapsed: 42.95s - Est. remaining: 856.02s\n",
      "Processing batch 91: movies 9001-9100 of 10993\n",
      "Progress: 82.8% - Elapsed: 43.40s - Est. remaining: 821.65s\n",
      "Processing batch 92: movies 9101-9200 of 10993\n",
      "Progress: 83.7% - Elapsed: 43.87s - Est. remaining: 786.65s\n",
      "Processing batch 93: movies 9201-9300 of 10993\n",
      "Progress: 84.6% - Elapsed: 44.34s - Est. remaining: 750.76s\n",
      "Processing batch 94: movies 9301-9400 of 10993\n",
      "Progress: 85.5% - Elapsed: 44.84s - Est. remaining: 714.35s\n",
      "Processing batch 95: movies 9401-9500 of 10993\n",
      "Progress: 86.4% - Elapsed: 45.31s - Est. remaining: 676.52s\n",
      "Processing batch 96: movies 9501-9600 of 10993\n",
      "Progress: 87.3% - Elapsed: 45.94s - Est. remaining: 639.95s\n",
      "Processing batch 97: movies 9601-9700 of 10993\n",
      "Progress: 88.2% - Elapsed: 46.58s - Est. remaining: 602.30s\n",
      "Processing batch 98: movies 9701-9800 of 10993\n",
      "Progress: 89.1% - Elapsed: 47.22s - Est. remaining: 563.32s\n",
      "Processing batch 99: movies 9801-9900 of 10993\n",
      "Progress: 90.1% - Elapsed: 47.91s - Est. remaining: 523.63s\n",
      "Processing batch 100: movies 9901-10000 of 10993\n",
      "Progress: 91.0% - Elapsed: 48.56s - Est. remaining: 482.24s\n",
      "Processing batch 101: movies 10001-10100 of 10993\n",
      "Progress: 91.9% - Elapsed: 49.19s - Est. remaining: 439.30s\n",
      "Processing batch 102: movies 10101-10200 of 10993\n",
      "Progress: 92.8% - Elapsed: 49.69s - Est. remaining: 394.05s\n",
      "Processing batch 103: movies 10201-10300 of 10993\n",
      "Progress: 93.7% - Elapsed: 50.17s - Est. remaining: 347.71s\n",
      "Processing batch 104: movies 10301-10400 of 10993\n",
      "Progress: 94.6% - Elapsed: 50.71s - Est. remaining: 300.73s\n",
      "Processing batch 105: movies 10401-10500 of 10993\n",
      "Progress: 95.5% - Elapsed: 51.30s - Est. remaining: 252.89s\n",
      "Processing batch 106: movies 10501-10600 of 10993\n",
      "Progress: 96.4% - Elapsed: 52.07s - Est. remaining: 204.63s\n",
      "Processing batch 107: movies 10601-10700 of 10993\n",
      "Progress: 97.3% - Elapsed: 52.67s - Est. remaining: 154.33s\n",
      "Processing batch 108: movies 10701-10800 of 10993\n",
      "Progress: 98.2% - Elapsed: 53.54s - Est. remaining: 103.34s\n",
      "Processing batch 109: movies 10801-10900 of 10993\n",
      "Progress: 99.2% - Elapsed: 54.34s - Est. remaining: 50.54s\n",
      "Processing batch 110: movies 10901-10993 of 10993\n",
      "Progress: 100.0% - Elapsed: 55.06s - Est. remaining: 0.00s\n",
      "\n",
      "Sample Log-Likelihood values for movie 'Toy Story (1995)' (ID: 1):\n",
      "Word: 'woody', LL Value: 41.89\n",
      "Word: 'andy', LL Value: 34.28\n",
      "Word: 'buzz', LL Value: 26.57\n",
      "Word: 'animation', LL Value: 17.66\n",
      "Word: 'buzz lightyear', LL Value: 15.01\n",
      "Word: 'tã', LL Value: 15.01\n",
      "Word: 'toy story', LL Value: 13.28\n",
      "Word: 'aside', LL Value: 10.73\n",
      "Word: 'pixar', LL Value: 10.73\n",
      "Word: 'afraid', LL Value: 10.30\n",
      "Calculated Log-Likelihood values for 10993 movies\n",
      "Average number of words with LL > 10 per movie: 10.37\n",
      "Min: 0, Max: 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 16:10:27,518 : INFO : collecting all words and their counts\n",
      "2025-04-16 16:10:27,518 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-04-16 16:10:27,549 : INFO : PROGRESS: at sentence #10000, processed 298182 words, keeping 40237 word types\n",
      "2025-04-16 16:10:27,565 : INFO : collected 42589 word types from a corpus of 326967 raw words and 10993 sentences\n",
      "2025-04-16 16:10:27,565 : INFO : Creating a fresh vocabulary\n",
      "2025-04-16 16:10:27,580 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 8121 unique words (19.07% of original 42589, drops 34468)', 'datetime': '2025-04-16T16:10:27.580743', 'gensim': '4.3.3', 'python': '3.10.1 (tags/v3.10.1:2cd268a, Dec  6 2021, 19:10:37) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2025-04-16 16:10:27,580 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 277158 word corpus (84.77% of original 326967, drops 49809)', 'datetime': '2025-04-16T16:10:27.580743', 'gensim': '4.3.3', 'python': '3.10.1 (tags/v3.10.1:2cd268a, Dec  6 2021, 19:10:37) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2025-04-16 16:10:27,623 : INFO : deleting the raw counts dictionary of 42589 items\n",
      "2025-04-16 16:10:27,625 : INFO : sample=0.001 downsamples 21 most-common words\n",
      "2025-04-16 16:10:27,626 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 270627.5671597655 word corpus (97.6%% of prior 277158)', 'datetime': '2025-04-16T16:10:27.625724', 'gensim': '4.3.3', 'python': '3.10.1 (tags/v3.10.1:2cd268a, Dec  6 2021, 19:10:37) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2025-04-16 16:10:27,662 : INFO : estimated required memory for 8121 words and 300 dimensions: 23550900 bytes\n",
      "2025-04-16 16:10:27,662 : INFO : resetting layer weights\n",
      "2025-04-16 16:10:27,678 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-04-16T16:10:27.678284', 'gensim': '4.3.3', 'python': '3.10.1 (tags/v3.10.1:2cd268a, Dec  6 2021, 19:10:37) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'build_vocab'}\n",
      "2025-04-16 16:10:27,678 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 8121 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-04-16T16:10:27.678284', 'gensim': '4.3.3', 'python': '3.10.1 (tags/v3.10.1:2cd268a, Dec  6 2021, 19:10:37) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: WORD2VEC MODEL TRAINING\n",
      "================================================================================\n",
      "Training Word2Vec model with 300 dimensions...\n",
      "Loaded tokens from 1000/10993 movies (9.1%)\n",
      "Loaded tokens from 2000/10993 movies (18.2%)\n",
      "Loaded tokens from 3000/10993 movies (27.3%)\n",
      "Loaded tokens from 4000/10993 movies (36.4%)\n",
      "Loaded tokens from 5000/10993 movies (45.5%)\n",
      "Loaded tokens from 6000/10993 movies (54.6%)\n",
      "Loaded tokens from 7000/10993 movies (63.7%)\n",
      "Loaded tokens from 8000/10993 movies (72.8%)\n",
      "Loaded tokens from 9000/10993 movies (81.9%)\n",
      "Loaded tokens from 10000/10993 movies (91.0%)\n",
      "Loaded tokens from 10993/10993 movies (100.0%)\n",
      "Training corpus size: 326967 tokens from 10993 documents\n",
      "Starting Word2Vec training (this may take a few minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 16:10:27,874 : INFO : EPOCH 0: training on 326967 raw words (270565 effective words) took 0.2s, 1412660 effective words/s\n",
      "2025-04-16 16:10:28,070 : INFO : EPOCH 1: training on 326967 raw words (270756 effective words) took 0.2s, 1413463 effective words/s\n",
      "2025-04-16 16:10:28,276 : INFO : EPOCH 2: training on 326967 raw words (270577 effective words) took 0.2s, 1361481 effective words/s\n",
      "2025-04-16 16:10:28,508 : INFO : EPOCH 3: training on 326967 raw words (270702 effective words) took 0.2s, 1192815 effective words/s\n",
      "2025-04-16 16:10:28,780 : INFO : EPOCH 4: training on 326967 raw words (270759 effective words) took 0.3s, 1008357 effective words/s\n",
      "2025-04-16 16:10:28,991 : INFO : EPOCH 5: training on 326967 raw words (270495 effective words) took 0.2s, 1329320 effective words/s\n",
      "2025-04-16 16:10:29,210 : INFO : EPOCH 6: training on 326967 raw words (270621 effective words) took 0.2s, 1215417 effective words/s\n",
      "2025-04-16 16:10:29,444 : INFO : EPOCH 7: training on 326967 raw words (270578 effective words) took 0.2s, 1236588 effective words/s\n",
      "2025-04-16 16:10:29,715 : INFO : EPOCH 8: training on 326967 raw words (270677 effective words) took 0.3s, 1026091 effective words/s\n",
      "2025-04-16 16:10:29,928 : INFO : EPOCH 9: training on 326967 raw words (270534 effective words) took 0.2s, 1269406 effective words/s\n",
      "2025-04-16 16:10:30,162 : INFO : EPOCH 10: training on 326967 raw words (270568 effective words) took 0.2s, 1184304 effective words/s\n",
      "2025-04-16 16:10:30,397 : INFO : EPOCH 11: training on 326967 raw words (270699 effective words) took 0.2s, 1198008 effective words/s\n",
      "2025-04-16 16:10:30,653 : INFO : EPOCH 12: training on 326967 raw words (270626 effective words) took 0.3s, 1079145 effective words/s\n",
      "2025-04-16 16:10:30,876 : INFO : EPOCH 13: training on 326967 raw words (270638 effective words) took 0.2s, 1246020 effective words/s\n",
      "2025-04-16 16:10:31,053 : INFO : EPOCH 14: training on 326967 raw words (270728 effective words) took 0.2s, 1489117 effective words/s\n",
      "2025-04-16 16:10:31,225 : INFO : EPOCH 15: training on 326967 raw words (270600 effective words) took 0.2s, 1645960 effective words/s\n",
      "2025-04-16 16:10:31,398 : INFO : EPOCH 16: training on 326967 raw words (270456 effective words) took 0.2s, 1649432 effective words/s\n",
      "2025-04-16 16:10:31,569 : INFO : EPOCH 17: training on 326967 raw words (270640 effective words) took 0.2s, 1608298 effective words/s\n",
      "2025-04-16 16:10:31,769 : INFO : EPOCH 18: training on 326967 raw words (270573 effective words) took 0.2s, 1400136 effective words/s\n",
      "2025-04-16 16:10:31,945 : INFO : EPOCH 19: training on 326967 raw words (270532 effective words) took 0.2s, 1569770 effective words/s\n",
      "2025-04-16 16:10:32,129 : INFO : EPOCH 20: training on 326967 raw words (270640 effective words) took 0.2s, 1496181 effective words/s\n",
      "2025-04-16 16:10:32,289 : INFO : EPOCH 21: training on 326967 raw words (270574 effective words) took 0.2s, 1756037 effective words/s\n",
      "2025-04-16 16:10:32,484 : INFO : EPOCH 22: training on 326967 raw words (270706 effective words) took 0.2s, 1371003 effective words/s\n",
      "2025-04-16 16:10:32,694 : INFO : EPOCH 23: training on 326967 raw words (270601 effective words) took 0.2s, 1301653 effective words/s\n",
      "2025-04-16 16:10:32,898 : INFO : EPOCH 24: training on 326967 raw words (270679 effective words) took 0.2s, 1455875 effective words/s\n",
      "2025-04-16 16:10:33,085 : INFO : EPOCH 25: training on 326967 raw words (270603 effective words) took 0.2s, 1377408 effective words/s\n",
      "2025-04-16 16:10:33,268 : INFO : EPOCH 26: training on 326967 raw words (270510 effective words) took 0.2s, 1623329 effective words/s\n",
      "2025-04-16 16:10:33,427 : INFO : EPOCH 27: training on 326967 raw words (270520 effective words) took 0.2s, 1749813 effective words/s\n",
      "2025-04-16 16:10:33,650 : INFO : EPOCH 28: training on 326967 raw words (270702 effective words) took 0.2s, 1237735 effective words/s\n",
      "2025-04-16 16:10:33,936 : INFO : EPOCH 29: training on 326967 raw words (270674 effective words) took 0.3s, 972498 effective words/s\n",
      "2025-04-16 16:10:33,937 : INFO : Word2Vec lifecycle event {'msg': 'training on 9809010 raw words (8118533 effective words) took 6.3s, 1297284 effective words/s', 'datetime': '2025-04-16T16:10:33.937512', 'gensim': '4.3.3', 'python': '3.10.1 (tags/v3.10.1:2cd268a, Dec  6 2021, 19:10:37) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "2025-04-16 16:10:33,937 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=8121, vector_size=300, alpha=0.025>', 'datetime': '2025-04-16T16:10:33.937771', 'gensim': '4.3.3', 'python': '3.10.1 (tags/v3.10.1:2cd268a, Dec  6 2021, 19:10:37) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'created'}\n",
      "2025-04-16 16:10:34,831 : INFO : Word2Vec lifecycle event {'fname_or_handle': './rec/content-recommendations\\\\word2vec_model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-04-16T16:10:34.831215', 'gensim': '4.3.3', 'python': '3.10.1 (tags/v3.10.1:2cd268a, Dec  6 2021, 19:10:37) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'saving'}\n",
      "2025-04-16 16:10:34,832 : INFO : not storing attribute cum_table\n",
      "2025-04-16 16:10:34,851 : INFO : saved ./rec/content-recommendations\\word2vec_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec training completed in 7.31 seconds\n",
      "Word2Vec model vocabulary size: 8121 words\n",
      "\n",
      "Example word vectors from the trained model:\n",
      "'life': [ 0.4688385   0.8431155   0.12388313  0.99406224 -0.18825476]...\n",
      "'find': [ 0.32521227  1.0903789  -0.05281287 -0.02362921  0.7203941 ]...\n",
      "'one': [ 0.7176905  -1.6737013  -0.10925581 -0.04867217  0.8168447 ]...\n",
      "'young': [-0.5266023  -0.8684016  -0.24544564  0.9585796  -1.0504597 ]...\n",
      "'man': [-0.92798054  0.23561978  0.28354576 -0.41058534  0.8773406 ]...\n",
      "'year': [ 0.29281783  1.0929253  -0.03751885  0.9982206  -0.56150174]...\n",
      "'friend': [ 1.5992899  -1.1819743  -1.2093209   0.39266342  0.94145733]...\n",
      "'family': [ 0.7035913   0.47495857  2.2786355   0.33186027 -0.38537127]...\n",
      "'love': [ 0.3920567   0.25316107  0.2179615  -0.24928835 -0.58948773]...\n",
      "'woman': [0.10597842 1.3766848  0.7498807  0.58229446 1.9212137 ]...\n",
      "\n",
      "Example word similarities:\n",
      "Words similar to 'action': [('animation', 0.6525269746780396), ('puppetry', 0.6208895444869995), ('remade', 0.6187089085578918), ('packed', 0.614043653011322), ('interact', 0.544952392578125)]\n",
      "Words similar to 'love': [('asleep', 0.5271825790405273), ('the rise', 0.5169394612312317), ('astaire', 0.5110260844230652), ('alluring', 0.49818629026412964), ('spell', 0.48774030804634094)]\n",
      "Words similar to 'hero': [('fist', 0.4939144551753998), ('sherman', 0.4919359087944031), ('idol', 0.4665384888648987), ('wage', 0.46642962098121643), ('opposing', 0.4584980905056)]\n",
      "Words similar to 'villain': [('humankind', 0.6166484951972961), ('genie', 0.6075811982154846), ('superhuman', 0.6011646389961243), ('lex luthor', 0.5952045321464539), ('evil', 0.5885708332061768)]\n",
      "Trained and saved Word2Vec model with 8121 words\n",
      "\n",
      "================================================================================\n",
      "STEP 5: MOVIE VECTOR GENERATION\n",
      "================================================================================\n",
      "Generating movie feature vectors using Log-Likelihood + Word2Vec in batches...\n",
      "Processing batch 1: movies 1-100 of 10993\n",
      "Progress: 0.9% - Elapsed: 0.02s - Est. remaining: 2.58s\n",
      "Successfully created vectors: 100/100\n",
      "Processing batch 2: movies 101-200 of 10993\n",
      "Progress: 1.8% - Elapsed: 1.10s - Est. remaining: 118.32s\n",
      "Successfully created vectors: 200/200\n",
      "Processing batch 3: movies 201-300 of 10993\n",
      "Progress: 2.7% - Elapsed: 2.02s - Est. remaining: 215.89s\n",
      "Successfully created vectors: 300/300\n",
      "Processing batch 4: movies 301-400 of 10993\n",
      "Progress: 3.6% - Elapsed: 2.78s - Est. remaining: 295.00s\n",
      "Successfully created vectors: 400/400\n",
      "Processing batch 5: movies 401-500 of 10993\n",
      "Progress: 4.5% - Elapsed: 3.53s - Est. remaining: 370.92s\n",
      "Successfully created vectors: 500/500\n",
      "Processing batch 6: movies 501-600 of 10993\n",
      "Progress: 5.5% - Elapsed: 4.39s - Est. remaining: 456.69s\n",
      "Successfully created vectors: 600/600\n",
      "Processing batch 7: movies 601-700 of 10993\n",
      "Progress: 6.4% - Elapsed: 5.25s - Est. remaining: 540.29s\n",
      "Successfully created vectors: 698/700\n",
      "Processing batch 8: movies 701-800 of 10993\n",
      "Progress: 7.3% - Elapsed: 6.05s - Est. remaining: 616.43s\n",
      "Successfully created vectors: 796/800\n",
      "Processing batch 9: movies 801-900 of 10993\n",
      "Progress: 8.2% - Elapsed: 6.87s - Est. remaining: 693.05s\n",
      "Successfully created vectors: 896/900\n",
      "Processing batch 10: movies 901-1000 of 10993\n",
      "Progress: 9.1% - Elapsed: 7.54s - Est. remaining: 753.44s\n",
      "Successfully created vectors: 996/1000\n",
      "Processing batch 11: movies 1001-1100 of 10993\n",
      "Progress: 10.0% - Elapsed: 8.18s - Est. remaining: 809.13s\n",
      "Successfully created vectors: 1094/1100\n",
      "Processing batch 12: movies 1101-1200 of 10993\n",
      "Progress: 10.9% - Elapsed: 8.92s - Est. remaining: 873.52s\n",
      "Successfully created vectors: 1194/1200\n",
      "Processing batch 13: movies 1201-1300 of 10993\n",
      "Progress: 11.8% - Elapsed: 9.68s - Est. remaining: 938.30s\n",
      "Successfully created vectors: 1294/1300\n",
      "Processing batch 14: movies 1301-1400 of 10993\n",
      "Progress: 12.7% - Elapsed: 10.30s - Est. remaining: 988.09s\n",
      "Successfully created vectors: 1393/1400\n",
      "Processing batch 15: movies 1401-1500 of 10993\n",
      "Progress: 13.6% - Elapsed: 10.98s - Est. remaining: 1042.57s\n",
      "Successfully created vectors: 1493/1500\n",
      "Processing batch 16: movies 1501-1600 of 10993\n",
      "Progress: 14.6% - Elapsed: 11.60s - Est. remaining: 1089.91s\n",
      "Successfully created vectors: 1592/1600\n",
      "Processing batch 17: movies 1601-1700 of 10993\n",
      "Progress: 15.5% - Elapsed: 12.17s - Est. remaining: 1130.60s\n",
      "Successfully created vectors: 1692/1700\n",
      "Processing batch 18: movies 1701-1800 of 10993\n",
      "Progress: 16.4% - Elapsed: 12.69s - Est. remaining: 1166.51s\n",
      "Successfully created vectors: 1792/1800\n",
      "Processing batch 19: movies 1801-1900 of 10993\n",
      "Progress: 17.3% - Elapsed: 13.28s - Est. remaining: 1207.28s\n",
      "Successfully created vectors: 1892/1900\n",
      "Processing batch 20: movies 1901-2000 of 10993\n",
      "Progress: 18.2% - Elapsed: 13.81s - Est. remaining: 1241.55s\n",
      "Successfully created vectors: 1992/2000\n",
      "Processing batch 21: movies 2001-2100 of 10993\n",
      "Progress: 19.1% - Elapsed: 14.39s - Est. remaining: 1279.44s\n",
      "Successfully created vectors: 2091/2100\n",
      "Processing batch 22: movies 2101-2200 of 10993\n",
      "Progress: 20.0% - Elapsed: 15.19s - Est. remaining: 1335.75s\n",
      "Successfully created vectors: 2191/2200\n",
      "Processing batch 23: movies 2201-2300 of 10993\n",
      "Progress: 20.9% - Elapsed: 15.90s - Est. remaining: 1382.03s\n",
      "Successfully created vectors: 2291/2300\n",
      "Processing batch 24: movies 2301-2400 of 10993\n",
      "Progress: 21.8% - Elapsed: 16.87s - Est. remaining: 1449.87s\n",
      "Successfully created vectors: 2390/2400\n",
      "Processing batch 25: movies 2401-2500 of 10993\n",
      "Progress: 22.7% - Elapsed: 18.20s - Est. remaining: 1545.90s\n",
      "Successfully created vectors: 2490/2500\n",
      "Processing batch 26: movies 2501-2600 of 10993\n",
      "Progress: 23.7% - Elapsed: 19.87s - Est. remaining: 1667.78s\n",
      "Successfully created vectors: 2590/2600\n",
      "Processing batch 27: movies 2601-2700 of 10993\n",
      "Progress: 24.6% - Elapsed: 21.47s - Est. remaining: 1780.49s\n",
      "Successfully created vectors: 2689/2700\n",
      "Processing batch 28: movies 2701-2800 of 10993\n",
      "Progress: 25.5% - Elapsed: 22.73s - Est. remaining: 1862.15s\n",
      "Successfully created vectors: 2789/2800\n",
      "Processing batch 29: movies 2801-2900 of 10993\n",
      "Progress: 26.4% - Elapsed: 23.68s - Est. remaining: 1916.07s\n",
      "Successfully created vectors: 2889/2900\n",
      "Processing batch 30: movies 2901-3000 of 10993\n",
      "Progress: 27.3% - Elapsed: 25.09s - Est. remaining: 2005.46s\n",
      "Successfully created vectors: 2989/3000\n",
      "Processing batch 31: movies 3001-3100 of 10993\n",
      "Progress: 28.2% - Elapsed: 26.03s - Est. remaining: 2054.71s\n",
      "Successfully created vectors: 3089/3100\n",
      "Processing batch 32: movies 3101-3200 of 10993\n",
      "Progress: 29.1% - Elapsed: 26.99s - Est. remaining: 2102.96s\n",
      "Successfully created vectors: 3189/3200\n",
      "Processing batch 33: movies 3201-3300 of 10993\n",
      "Progress: 30.0% - Elapsed: 27.69s - Est. remaining: 2129.97s\n",
      "Successfully created vectors: 3289/3300\n",
      "Processing batch 34: movies 3301-3400 of 10993\n",
      "Progress: 30.9% - Elapsed: 28.30s - Est. remaining: 2148.82s\n",
      "Successfully created vectors: 3389/3400\n",
      "Processing batch 35: movies 3401-3500 of 10993\n",
      "Progress: 31.8% - Elapsed: 28.94s - Est. remaining: 2168.66s\n",
      "Successfully created vectors: 3489/3500\n",
      "Processing batch 36: movies 3501-3600 of 10993\n",
      "Progress: 32.7% - Elapsed: 29.54s - Est. remaining: 2183.78s\n",
      "Successfully created vectors: 3589/3600\n",
      "Processing batch 37: movies 3601-3700 of 10993\n",
      "Progress: 33.7% - Elapsed: 30.22s - Est. remaining: 2204.28s\n",
      "Successfully created vectors: 3689/3700\n",
      "Processing batch 38: movies 3701-3800 of 10993\n",
      "Progress: 34.6% - Elapsed: 30.83s - Est. remaining: 2217.32s\n",
      "Successfully created vectors: 3789/3800\n",
      "Processing batch 39: movies 3801-3900 of 10993\n",
      "Progress: 35.5% - Elapsed: 31.38s - Est. remaining: 2225.94s\n",
      "Successfully created vectors: 3888/3900\n",
      "Processing batch 40: movies 3901-4000 of 10993\n",
      "Progress: 36.4% - Elapsed: 32.02s - Est. remaining: 2239.04s\n",
      "Successfully created vectors: 3988/4000\n",
      "Processing batch 41: movies 4001-4100 of 10993\n",
      "Progress: 37.3% - Elapsed: 32.65s - Est. remaining: 2250.30s\n",
      "Successfully created vectors: 4088/4100\n",
      "Processing batch 42: movies 4101-4200 of 10993\n",
      "Progress: 38.2% - Elapsed: 33.35s - Est. remaining: 2265.28s\n",
      "Successfully created vectors: 4188/4200\n",
      "Processing batch 43: movies 4201-4300 of 10993\n",
      "Progress: 39.1% - Elapsed: 34.88s - Est. remaining: 2334.43s\n",
      "Successfully created vectors: 4288/4300\n",
      "Processing batch 44: movies 4301-4400 of 10993\n",
      "Progress: 40.0% - Elapsed: 35.88s - Est. remaining: 2365.59s\n",
      "Successfully created vectors: 4388/4400\n",
      "Processing batch 45: movies 4401-4500 of 10993\n",
      "Progress: 40.9% - Elapsed: 36.72s - Est. remaining: 2384.18s\n",
      "Successfully created vectors: 4488/4500\n",
      "Processing batch 46: movies 4501-4600 of 10993\n",
      "Progress: 41.8% - Elapsed: 37.49s - Est. remaining: 2396.60s\n",
      "Successfully created vectors: 4588/4600\n",
      "Processing batch 47: movies 4601-4700 of 10993\n",
      "Progress: 42.8% - Elapsed: 38.10s - Est. remaining: 2397.43s\n",
      "Successfully created vectors: 4688/4700\n",
      "Processing batch 48: movies 4701-4800 of 10993\n",
      "Progress: 43.7% - Elapsed: 38.79s - Est. remaining: 2402.30s\n",
      "Successfully created vectors: 4787/4800\n",
      "Processing batch 49: movies 4801-4900 of 10993\n",
      "Progress: 44.6% - Elapsed: 39.40s - Est. remaining: 2400.70s\n",
      "Successfully created vectors: 4887/4900\n",
      "Processing batch 50: movies 4901-5000 of 10993\n",
      "Progress: 45.5% - Elapsed: 40.25s - Est. remaining: 2412.01s\n",
      "Successfully created vectors: 4987/5000\n",
      "Processing batch 51: movies 5001-5100 of 10993\n",
      "Progress: 46.4% - Elapsed: 41.30s - Est. remaining: 2433.61s\n",
      "Successfully created vectors: 5087/5100\n",
      "Processing batch 52: movies 5101-5200 of 10993\n",
      "Progress: 47.3% - Elapsed: 42.41s - Est. remaining: 2456.79s\n",
      "Successfully created vectors: 5187/5200\n",
      "Processing batch 53: movies 5201-5300 of 10993\n",
      "Progress: 48.2% - Elapsed: 43.76s - Est. remaining: 2491.02s\n",
      "Successfully created vectors: 5287/5300\n",
      "Processing batch 54: movies 5301-5400 of 10993\n",
      "Progress: 49.1% - Elapsed: 45.76s - Est. remaining: 2559.19s\n",
      "Successfully created vectors: 5387/5400\n",
      "Processing batch 55: movies 5401-5500 of 10993\n",
      "Progress: 50.0% - Elapsed: 46.98s - Est. remaining: 2580.56s\n",
      "Successfully created vectors: 5487/5500\n",
      "Processing batch 56: movies 5501-5600 of 10993\n",
      "Progress: 50.9% - Elapsed: 47.76s - Est. remaining: 2575.61s\n",
      "Successfully created vectors: 5587/5600\n",
      "Processing batch 57: movies 5601-5700 of 10993\n",
      "Progress: 51.9% - Elapsed: 48.50s - Est. remaining: 2567.21s\n",
      "Successfully created vectors: 5687/5700\n",
      "Processing batch 58: movies 5701-5800 of 10993\n",
      "Progress: 52.8% - Elapsed: 49.21s - Est. remaining: 2555.67s\n",
      "Successfully created vectors: 5787/5800\n",
      "Processing batch 59: movies 5801-5900 of 10993\n",
      "Progress: 53.7% - Elapsed: 49.85s - Est. remaining: 2538.96s\n",
      "Successfully created vectors: 5887/5900\n",
      "Processing batch 60: movies 5901-6000 of 10993\n",
      "Progress: 54.6% - Elapsed: 50.64s - Est. remaining: 2528.64s\n",
      "Successfully created vectors: 5987/6000\n",
      "Processing batch 61: movies 6001-6100 of 10993\n",
      "Progress: 55.5% - Elapsed: 51.22s - Est. remaining: 2505.99s\n",
      "Successfully created vectors: 6087/6100\n",
      "Processing batch 62: movies 6101-6200 of 10993\n",
      "Progress: 56.4% - Elapsed: 51.76s - Est. remaining: 2480.89s\n",
      "Successfully created vectors: 6187/6200\n",
      "Processing batch 63: movies 6201-6300 of 10993\n",
      "Progress: 57.3% - Elapsed: 52.36s - Est. remaining: 2457.28s\n",
      "Successfully created vectors: 6287/6300\n",
      "Processing batch 64: movies 6301-6400 of 10993\n",
      "Progress: 58.2% - Elapsed: 52.93s - Est. remaining: 2431.06s\n",
      "Successfully created vectors: 6387/6400\n",
      "Processing batch 65: movies 6401-6500 of 10993\n",
      "Progress: 59.1% - Elapsed: 53.49s - Est. remaining: 2403.34s\n",
      "Successfully created vectors: 6486/6500\n",
      "Processing batch 66: movies 6501-6600 of 10993\n",
      "Progress: 60.0% - Elapsed: 54.05s - Est. remaining: 2374.44s\n",
      "Successfully created vectors: 6586/6600\n",
      "Processing batch 67: movies 6601-6700 of 10993\n",
      "Progress: 60.9% - Elapsed: 54.61s - Est. remaining: 2344.43s\n",
      "Successfully created vectors: 6686/6700\n",
      "Processing batch 68: movies 6701-6800 of 10993\n",
      "Progress: 61.9% - Elapsed: 55.18s - Est. remaining: 2313.78s\n",
      "Successfully created vectors: 6786/6800\n",
      "Processing batch 69: movies 6801-6900 of 10993\n",
      "Progress: 62.8% - Elapsed: 55.79s - Est. remaining: 2283.44s\n",
      "Successfully created vectors: 6886/6900\n",
      "Processing batch 70: movies 6901-7000 of 10993\n",
      "Progress: 63.7% - Elapsed: 56.40s - Est. remaining: 2252.02s\n",
      "Successfully created vectors: 6986/7000\n",
      "Processing batch 71: movies 7001-7100 of 10993\n",
      "Progress: 64.6% - Elapsed: 56.96s - Est. remaining: 2217.38s\n",
      "Successfully created vectors: 7086/7100\n",
      "Processing batch 72: movies 7101-7200 of 10993\n",
      "Progress: 65.5% - Elapsed: 57.73s - Est. remaining: 2189.78s\n",
      "Successfully created vectors: 7186/7200\n",
      "Processing batch 73: movies 7201-7300 of 10993\n",
      "Progress: 66.4% - Elapsed: 58.56s - Est. remaining: 2162.75s\n",
      "Successfully created vectors: 7286/7300\n",
      "Processing batch 74: movies 7301-7400 of 10993\n",
      "Progress: 67.3% - Elapsed: 59.13s - Est. remaining: 2124.67s\n",
      "Successfully created vectors: 7382/7400\n",
      "Processing batch 75: movies 7401-7500 of 10993\n",
      "Progress: 68.2% - Elapsed: 59.70s - Est. remaining: 2085.37s\n",
      "Successfully created vectors: 7481/7500\n",
      "Processing batch 76: movies 7501-7600 of 10993\n",
      "Progress: 69.1% - Elapsed: 60.28s - Est. remaining: 2045.23s\n",
      "Successfully created vectors: 7580/7600\n",
      "Processing batch 77: movies 7601-7700 of 10993\n",
      "Progress: 70.0% - Elapsed: 60.84s - Est. remaining: 2003.61s\n",
      "Successfully created vectors: 7680/7700\n",
      "Processing batch 78: movies 7701-7800 of 10993\n",
      "Progress: 71.0% - Elapsed: 61.43s - Est. remaining: 1961.37s\n",
      "Successfully created vectors: 7780/7800\n",
      "Processing batch 79: movies 7801-7900 of 10993\n",
      "Progress: 71.9% - Elapsed: 62.07s - Est. remaining: 1919.85s\n",
      "Successfully created vectors: 7880/7900\n",
      "Processing batch 80: movies 7901-8000 of 10993\n",
      "Progress: 72.8% - Elapsed: 62.81s - Est. remaining: 1879.83s\n",
      "Successfully created vectors: 7980/8000\n",
      "Processing batch 81: movies 8001-8100 of 10993\n",
      "Progress: 73.7% - Elapsed: 63.71s - Est. remaining: 1843.19s\n",
      "Successfully created vectors: 8080/8100\n",
      "Processing batch 82: movies 8101-8200 of 10993\n",
      "Progress: 74.6% - Elapsed: 64.55s - Est. remaining: 1803.00s\n",
      "Successfully created vectors: 8180/8200\n",
      "Processing batch 83: movies 8201-8300 of 10993\n",
      "Progress: 75.5% - Elapsed: 65.19s - Est. remaining: 1755.49s\n",
      "Successfully created vectors: 8280/8300\n",
      "Processing batch 84: movies 8301-8400 of 10993\n",
      "Progress: 76.4% - Elapsed: 66.16s - Est. remaining: 1715.45s\n",
      "Successfully created vectors: 8379/8400\n",
      "Processing batch 85: movies 8401-8500 of 10993\n",
      "Progress: 77.3% - Elapsed: 67.13s - Est. remaining: 1673.61s\n",
      "Successfully created vectors: 8479/8500\n",
      "Processing batch 86: movies 8501-8600 of 10993\n",
      "Progress: 78.2% - Elapsed: 67.88s - Est. remaining: 1624.29s\n",
      "Successfully created vectors: 8578/8600\n",
      "Processing batch 87: movies 8601-8700 of 10993\n",
      "Progress: 79.1% - Elapsed: 68.43s - Est. remaining: 1569.17s\n",
      "Successfully created vectors: 8677/8700\n",
      "Processing batch 88: movies 8701-8800 of 10993\n",
      "Progress: 80.1% - Elapsed: 68.96s - Est. remaining: 1512.39s\n",
      "Successfully created vectors: 8777/8800\n",
      "Processing batch 89: movies 8801-8900 of 10993\n",
      "Progress: 81.0% - Elapsed: 69.54s - Est. remaining: 1455.43s\n",
      "Successfully created vectors: 8877/8900\n",
      "Processing batch 90: movies 8901-9000 of 10993\n",
      "Progress: 81.9% - Elapsed: 70.07s - Est. remaining: 1396.58s\n",
      "Successfully created vectors: 8977/9000\n",
      "Processing batch 91: movies 9001-9100 of 10993\n",
      "Progress: 82.8% - Elapsed: 70.60s - Est. remaining: 1336.45s\n",
      "Successfully created vectors: 9073/9100\n",
      "Processing batch 92: movies 9101-9200 of 10993\n",
      "Progress: 83.7% - Elapsed: 71.19s - Est. remaining: 1276.50s\n",
      "Successfully created vectors: 9172/9200\n",
      "Processing batch 93: movies 9201-9300 of 10993\n",
      "Progress: 84.6% - Elapsed: 71.85s - Est. remaining: 1216.41s\n",
      "Successfully created vectors: 9270/9300\n",
      "Processing batch 94: movies 9301-9400 of 10993\n",
      "Progress: 85.5% - Elapsed: 72.68s - Est. remaining: 1157.83s\n",
      "Successfully created vectors: 9370/9400\n",
      "Processing batch 95: movies 9401-9500 of 10993\n",
      "Progress: 86.4% - Elapsed: 73.58s - Est. remaining: 1098.53s\n",
      "Successfully created vectors: 9470/9500\n",
      "Processing batch 96: movies 9501-9600 of 10993\n",
      "Progress: 87.3% - Elapsed: 74.25s - Est. remaining: 1034.35s\n",
      "Successfully created vectors: 9570/9600\n",
      "Processing batch 97: movies 9601-9700 of 10993\n",
      "Progress: 88.2% - Elapsed: 75.07s - Est. remaining: 970.60s\n",
      "Successfully created vectors: 9668/9700\n",
      "Processing batch 98: movies 9701-9800 of 10993\n",
      "Progress: 89.1% - Elapsed: 75.98s - Est. remaining: 906.41s\n",
      "Successfully created vectors: 9768/9800\n",
      "Processing batch 99: movies 9801-9900 of 10993\n",
      "Progress: 90.1% - Elapsed: 76.80s - Est. remaining: 839.45s\n",
      "Successfully created vectors: 9868/9900\n",
      "Processing batch 100: movies 9901-10000 of 10993\n",
      "Progress: 91.0% - Elapsed: 77.46s - Est. remaining: 769.15s\n",
      "Successfully created vectors: 9968/10000\n",
      "Processing batch 101: movies 10001-10100 of 10993\n",
      "Progress: 91.9% - Elapsed: 78.14s - Est. remaining: 697.77s\n",
      "Successfully created vectors: 10068/10100\n",
      "Processing batch 102: movies 10101-10200 of 10993\n",
      "Progress: 92.8% - Elapsed: 78.86s - Est. remaining: 625.38s\n",
      "Successfully created vectors: 10168/10200\n",
      "Processing batch 103: movies 10201-10300 of 10993\n",
      "Progress: 93.7% - Elapsed: 79.52s - Est. remaining: 551.06s\n",
      "Successfully created vectors: 10268/10300\n",
      "Processing batch 104: movies 10301-10400 of 10993\n",
      "Progress: 94.6% - Elapsed: 80.18s - Est. remaining: 475.47s\n",
      "Successfully created vectors: 10368/10400\n",
      "Processing batch 105: movies 10401-10500 of 10993\n",
      "Progress: 95.5% - Elapsed: 80.83s - Est. remaining: 398.48s\n",
      "Successfully created vectors: 10467/10500\n",
      "Processing batch 106: movies 10501-10600 of 10993\n",
      "Progress: 96.4% - Elapsed: 81.50s - Est. remaining: 320.28s\n",
      "Successfully created vectors: 10565/10600\n",
      "Processing batch 107: movies 10601-10700 of 10993\n",
      "Progress: 97.3% - Elapsed: 82.19s - Est. remaining: 240.82s\n",
      "Successfully created vectors: 10665/10700\n",
      "Processing batch 108: movies 10701-10800 of 10993\n",
      "Progress: 98.2% - Elapsed: 82.89s - Est. remaining: 159.98s\n",
      "Successfully created vectors: 10761/10800\n",
      "Processing batch 109: movies 10801-10900 of 10993\n",
      "Progress: 99.2% - Elapsed: 83.58s - Est. remaining: 77.73s\n",
      "Successfully created vectors: 10859/10900\n",
      "Processing batch 110: movies 10901-10993 of 10993\n",
      "Progress: 100.0% - Elapsed: 84.24s - Est. remaining: 0.00s\n",
      "Successfully created vectors: 10949/10993\n",
      "\n",
      "Vector generation complete:\n",
      "Successfully created vectors: 10949/10993 (99.6%)\n",
      "Movies with no words found: 0\n",
      "Movies with too low LL sum: 44\n",
      "\n",
      "Sample movie vectors:\n",
      "Movie: 'Toy Story (1995)' (ID: 1)\n",
      "Vector shape: (300,)\n",
      "Vector norm: 1.0000\n",
      "First 5 dimensions: [ 0.01129066  0.08608983  0.01140525 -0.03385416  0.06061957]\n",
      "---\n",
      "Movie: 'Jumanji (1995)' (ID: 2)\n",
      "Vector shape: (300,)\n",
      "Vector norm: 1.0000\n",
      "First 5 dimensions: [ 0.05441351  0.04365786 -0.07128056 -0.04911663  0.03076783]\n",
      "---\n",
      "Movie: 'Grumpier Old Men (1995)' (ID: 3)\n",
      "Vector shape: (300,)\n",
      "Vector norm: 1.0000\n",
      "First 5 dimensions: [ 0.03766501 -0.0126873   0.01842736  0.01184421 -0.01289673]\n",
      "---\n",
      "Generated and saved feature vectors for 10949 movies\n",
      "\n",
      "Vector statistics:\n",
      "Average vector norm: 1.0000\n",
      "Vector dimensionality: 300\n",
      "\n",
      "================================================================================\n",
      "STEP 6: USER VECTOR GENERATION\n",
      "================================================================================\n",
      "Generating user feature vectors based on movie ratings in batches...\n",
      "Creating user ratings lookup dictionary...\n",
      "Processing 2000 users in batches...\n",
      "Processing batch 1: users 1-100 of 2000\n",
      "Progress: 5.0% - Elapsed: 4.46s - Est. remaining: 84.72s\n",
      "Successfully created vectors: 100\n",
      "Processing batch 2: users 101-200 of 2000\n",
      "Progress: 10.0% - Elapsed: 5.24s - Est. remaining: 94.36s\n",
      "Successfully created vectors: 200\n",
      "Processing batch 3: users 201-300 of 2000\n",
      "Progress: 15.0% - Elapsed: 5.93s - Est. remaining: 100.78s\n",
      "Successfully created vectors: 300\n",
      "Processing batch 4: users 301-400 of 2000\n",
      "Progress: 20.0% - Elapsed: 6.64s - Est. remaining: 106.23s\n",
      "Successfully created vectors: 400\n",
      "Processing batch 5: users 401-500 of 2000\n",
      "Progress: 25.0% - Elapsed: 7.32s - Est. remaining: 109.82s\n",
      "Successfully created vectors: 500\n",
      "Processing batch 6: users 501-600 of 2000\n",
      "Progress: 30.0% - Elapsed: 7.91s - Est. remaining: 110.72s\n",
      "Successfully created vectors: 600\n",
      "Processing batch 7: users 601-700 of 2000\n",
      "Progress: 35.0% - Elapsed: 8.50s - Est. remaining: 110.55s\n",
      "Successfully created vectors: 700\n",
      "Processing batch 8: users 701-800 of 2000\n",
      "Progress: 40.0% - Elapsed: 9.14s - Est. remaining: 109.67s\n",
      "Successfully created vectors: 800\n",
      "Processing batch 9: users 801-900 of 2000\n",
      "Progress: 45.0% - Elapsed: 9.70s - Est. remaining: 106.72s\n",
      "Successfully created vectors: 900\n",
      "Processing batch 10: users 901-1000 of 2000\n",
      "Progress: 50.0% - Elapsed: 10.30s - Est. remaining: 102.99s\n",
      "Successfully created vectors: 1000\n",
      "Processing batch 11: users 1001-1100 of 2000\n",
      "Progress: 55.0% - Elapsed: 10.95s - Est. remaining: 98.57s\n",
      "Successfully created vectors: 1100\n",
      "Processing batch 12: users 1101-1200 of 2000\n",
      "Progress: 60.0% - Elapsed: 11.89s - Est. remaining: 95.10s\n",
      "Successfully created vectors: 1200\n",
      "Processing batch 13: users 1201-1300 of 2000\n",
      "Progress: 65.0% - Elapsed: 12.89s - Est. remaining: 90.21s\n",
      "Successfully created vectors: 1300\n",
      "Processing batch 14: users 1301-1400 of 2000\n",
      "Progress: 70.0% - Elapsed: 13.75s - Est. remaining: 82.49s\n",
      "Successfully created vectors: 1400\n",
      "Processing batch 15: users 1401-1500 of 2000\n",
      "Progress: 75.0% - Elapsed: 14.69s - Est. remaining: 73.44s\n",
      "Successfully created vectors: 1500\n",
      "Processing batch 16: users 1501-1600 of 2000\n",
      "Progress: 80.0% - Elapsed: 15.79s - Est. remaining: 63.16s\n",
      "Successfully created vectors: 1600\n",
      "Processing batch 17: users 1601-1700 of 2000\n",
      "Progress: 85.0% - Elapsed: 17.15s - Est. remaining: 51.44s\n",
      "Successfully created vectors: 1700\n",
      "Processing batch 18: users 1701-1800 of 2000\n",
      "Progress: 90.0% - Elapsed: 18.25s - Est. remaining: 36.50s\n",
      "Successfully created vectors: 1800\n",
      "Processing batch 19: users 1801-1900 of 2000\n",
      "Progress: 95.0% - Elapsed: 19.21s - Est. remaining: 19.21s\n",
      "Successfully created vectors: 1900\n",
      "Processing batch 20: users 1901-2000 of 2000\n",
      "Progress: 100.0% - Elapsed: 20.45s - Est. remaining: 0.00s\n",
      "Successfully created vectors: 2000\n",
      "\n",
      "User vector generation complete:\n",
      "Successfully created vectors: 2000/2000 (100.0%)\n",
      "Users with no ratings: 0\n",
      "Users with no vectorized movies: 0\n",
      "Users with too low weight sum: 0\n",
      "\n",
      "Sample user vectors:\n",
      "User ID: 1.0\n",
      "Number of ratings: 4\n",
      "Vector shape: (300,)\n",
      "Vector norm: 1.0000\n",
      "First 5 dimensions: [-0.00056513  0.07494874 -0.00291258 -0.01814397  0.06341664]\n",
      "---\n",
      "User ID: 2.0\n",
      "Number of ratings: 4\n",
      "Vector shape: (300,)\n",
      "Vector norm: 1.0000\n",
      "First 5 dimensions: [-0.00511118  0.08598979  0.02235005 -0.00095008  0.08129885]\n",
      "---\n",
      "User ID: 3.0\n",
      "Number of ratings: 4\n",
      "Vector shape: (300,)\n",
      "Vector norm: 1.0000\n",
      "First 5 dimensions: [0.00339176 0.09366546 0.01195544 0.00198307 0.06234701]\n",
      "---\n",
      "Generated and saved feature vectors for 2000 users\n",
      "\n",
      "Vector statistics:\n",
      "Average vector norm: 1.0000\n",
      "Vector dimensionality: 300\n",
      "\n",
      "================================================================================\n",
      "STEP 7: USER-MOVIE SIMILARITY CALCULATION\n",
      "================================================================================\n",
      "Calculating user-movie similarity with threshold 0.5 in batches...\n",
      "Processing batch 1: users 1-50 of 2000\n",
      "User 50.0: 4194/10949 movies above threshold (38.30%)\n",
      "Processed 50/2000 users (2.5%) - Elapsed: 1.42s - Est. remaining: 55.50s\n",
      "Processing batch 2: users 51-100 of 2000\n",
      "User 100.0: 1942/10949 movies above threshold (17.74%)\n",
      "Processed 100/2000 users (5.0%) - Elapsed: 4.05s - Est. remaining: 153.97s\n",
      "Processing batch 3: users 101-150 of 2000\n",
      "User 150.0: 0/10949 movies above threshold (0.00%)\n",
      "Processed 150/2000 users (7.5%) - Elapsed: 6.76s - Est. remaining: 250.21s\n",
      "Processing batch 4: users 151-200 of 2000\n",
      "User 200.0: 5373/10949 movies above threshold (49.07%)\n",
      "Processed 200/2000 users (10.0%) - Elapsed: 8.84s - Est. remaining: 318.33s\n",
      "Processing batch 5: users 201-250 of 2000\n",
      "User 250.0: 12/10949 movies above threshold (0.11%)\n",
      "Processed 250/2000 users (12.5%) - Elapsed: 10.91s - Est. remaining: 381.68s\n",
      "Processing batch 6: users 251-300 of 2000\n",
      "User 300.0: 3158/10949 movies above threshold (28.84%)\n",
      "Processed 300/2000 users (15.0%) - Elapsed: 12.89s - Est. remaining: 438.36s\n",
      "Processing batch 7: users 301-350 of 2000\n",
      "User 350.0: 507/10949 movies above threshold (4.63%)\n",
      "Processed 350/2000 users (17.5%) - Elapsed: 14.98s - Est. remaining: 494.48s\n",
      "Processing batch 8: users 351-400 of 2000\n",
      "User 400.0: 2031/10949 movies above threshold (18.55%)\n",
      "Processed 400/2000 users (20.0%) - Elapsed: 16.95s - Est. remaining: 542.38s\n",
      "Processing batch 9: users 401-450 of 2000\n",
      "User 450.0: 2597/10949 movies above threshold (23.72%)\n",
      "Processed 450/2000 users (22.5%) - Elapsed: 18.85s - Est. remaining: 584.45s\n",
      "Processing batch 10: users 451-500 of 2000\n",
      "User 500.0: 3426/10949 movies above threshold (31.29%)\n",
      "Processed 500/2000 users (25.0%) - Elapsed: 20.49s - Est. remaining: 614.84s\n",
      "Processing batch 11: users 501-550 of 2000\n",
      "User 550.0: 2418/10949 movies above threshold (22.08%)\n",
      "Processed 550/2000 users (27.5%) - Elapsed: 22.40s - Est. remaining: 649.53s\n",
      "Processing batch 12: users 551-600 of 2000\n",
      "User 600.0: 4178/10949 movies above threshold (38.16%)\n",
      "Processed 600/2000 users (30.0%) - Elapsed: 24.61s - Est. remaining: 689.21s\n",
      "Processing batch 13: users 601-650 of 2000\n",
      "User 650.0: 2768/10949 movies above threshold (25.28%)\n",
      "Processed 650/2000 users (32.5%) - Elapsed: 26.44s - Est. remaining: 713.79s\n",
      "Processing batch 14: users 651-700 of 2000\n",
      "User 700.0: 2026/10949 movies above threshold (18.50%)\n",
      "Processed 700/2000 users (35.0%) - Elapsed: 28.35s - Est. remaining: 736.99s\n",
      "Processing batch 15: users 701-750 of 2000\n",
      "User 750.0: 0/10949 movies above threshold (0.00%)\n",
      "Processed 750/2000 users (37.5%) - Elapsed: 30.34s - Est. remaining: 758.62s\n",
      "Processing batch 16: users 751-800 of 2000\n",
      "User 800.0: 4269/10949 movies above threshold (38.99%)\n",
      "Processed 800/2000 users (40.0%) - Elapsed: 32.36s - Est. remaining: 776.67s\n",
      "Processing batch 17: users 801-850 of 2000\n",
      "User 850.0: 2962/10949 movies above threshold (27.05%)\n",
      "Processed 850/2000 users (42.5%) - Elapsed: 34.31s - Est. remaining: 789.14s\n",
      "Processing batch 18: users 851-900 of 2000\n",
      "User 900.0: 4438/10949 movies above threshold (40.53%)\n",
      "Processed 900/2000 users (45.0%) - Elapsed: 36.60s - Est. remaining: 805.15s\n",
      "Processing batch 19: users 901-950 of 2000\n",
      "User 950.0: 0/10949 movies above threshold (0.00%)\n",
      "Processed 950/2000 users (47.5%) - Elapsed: 38.47s - Est. remaining: 807.78s\n",
      "Processing batch 20: users 951-1000 of 2000\n",
      "User 1000.0: 2815/10949 movies above threshold (25.71%)\n",
      "Processed 1000/2000 users (50.0%) - Elapsed: 40.55s - Est. remaining: 810.90s\n",
      "Processing batch 21: users 1001-1050 of 2000\n",
      "User 1050.0: 0/10949 movies above threshold (0.00%)\n",
      "Processed 1050/2000 users (52.5%) - Elapsed: 42.59s - Est. remaining: 809.25s\n",
      "Processing batch 22: users 1051-1100 of 2000\n",
      "User 1100.0: 229/10949 movies above threshold (2.09%)\n",
      "Processed 1100/2000 users (55.0%) - Elapsed: 44.53s - Est. remaining: 801.56s\n",
      "Processing batch 23: users 1101-1150 of 2000\n",
      "User 1150.0: 4205/10949 movies above threshold (38.41%)\n",
      "Processed 1150/2000 users (57.5%) - Elapsed: 46.32s - Est. remaining: 787.47s\n",
      "Processing batch 24: users 1151-1200 of 2000\n",
      "User 1200.0: 4510/10949 movies above threshold (41.19%)\n",
      "Processed 1200/2000 users (60.0%) - Elapsed: 48.14s - Est. remaining: 770.25s\n",
      "Processing batch 25: users 1201-1250 of 2000\n",
      "User 1250.0: 5643/10949 movies above threshold (51.54%)\n",
      "Processed 1250/2000 users (62.5%) - Elapsed: 50.34s - Est. remaining: 755.03s\n",
      "Processing batch 26: users 1251-1300 of 2000\n",
      "User 1300.0: 2561/10949 movies above threshold (23.39%)\n",
      "Processed 1300/2000 users (65.0%) - Elapsed: 52.65s - Est. remaining: 737.12s\n",
      "Processing batch 27: users 1301-1350 of 2000\n",
      "User 1350.0: 5057/10949 movies above threshold (46.19%)\n",
      "Processed 1350/2000 users (67.5%) - Elapsed: 54.45s - Est. remaining: 707.86s\n",
      "Processing batch 28: users 1351-1400 of 2000\n",
      "User 1400.0: 4403/10949 movies above threshold (40.21%)\n",
      "Processed 1400/2000 users (70.0%) - Elapsed: 56.28s - Est. remaining: 675.32s\n",
      "Processing batch 29: users 1401-1450 of 2000\n",
      "User 1450.0: 6770/10949 movies above threshold (61.83%)\n",
      "Processed 1450/2000 users (72.5%) - Elapsed: 58.52s - Est. remaining: 643.73s\n",
      "Processing batch 30: users 1451-1500 of 2000\n",
      "User 1500.0: 5310/10949 movies above threshold (48.50%)\n",
      "Processed 1500/2000 users (75.0%) - Elapsed: 60.35s - Est. remaining: 603.45s\n",
      "Processing batch 31: users 1501-1550 of 2000\n",
      "User 1550.0: 507/10949 movies above threshold (4.63%)\n",
      "Processed 1550/2000 users (77.5%) - Elapsed: 62.51s - Est. remaining: 562.62s\n",
      "Processing batch 32: users 1551-1600 of 2000\n",
      "User 1600.0: 4494/10949 movies above threshold (41.04%)\n",
      "Processed 1600/2000 users (80.0%) - Elapsed: 64.74s - Est. remaining: 517.89s\n",
      "Processing batch 33: users 1601-1650 of 2000\n",
      "User 1650.0: 3010/10949 movies above threshold (27.49%)\n",
      "Processed 1650/2000 users (82.5%) - Elapsed: 67.06s - Est. remaining: 469.41s\n",
      "Processing batch 34: users 1651-1700 of 2000\n",
      "User 1700.0: 1985/10949 movies above threshold (18.13%)\n",
      "Processed 1700/2000 users (85.0%) - Elapsed: 70.03s - Est. remaining: 420.21s\n",
      "Processing batch 35: users 1701-1750 of 2000\n",
      "User 1750.0: 4187/10949 movies above threshold (38.24%)\n",
      "Processed 1750/2000 users (87.5%) - Elapsed: 72.48s - Est. remaining: 362.41s\n",
      "Processing batch 36: users 1751-1800 of 2000\n",
      "User 1800.0: 5313/10949 movies above threshold (48.52%)\n",
      "Processed 1800/2000 users (90.0%) - Elapsed: 74.64s - Est. remaining: 298.58s\n",
      "Processing batch 37: users 1801-1850 of 2000\n",
      "User 1850.0: 0/10949 movies above threshold (0.00%)\n",
      "Processed 1850/2000 users (92.5%) - Elapsed: 76.91s - Est. remaining: 230.72s\n",
      "Processing batch 38: users 1851-1900 of 2000\n",
      "User 1900.0: 0/10949 movies above threshold (0.00%)\n",
      "Processed 1900/2000 users (95.0%) - Elapsed: 79.14s - Est. remaining: 158.27s\n",
      "Processing batch 39: users 1901-1950 of 2000\n",
      "User 1950.0: 1808/10949 movies above threshold (16.51%)\n",
      "Processed 1950/2000 users (97.5%) - Elapsed: 81.12s - Est. remaining: 81.12s\n",
      "Processing batch 40: users 1951-2000 of 2000\n",
      "User 2000.0: 2448/10949 movies above threshold (22.36%)\n",
      "Processed 2000/2000 users (100.0%) - Elapsed: 83.73s - Est. remaining: 0.00s\n",
      "\n",
      "Similarity calculation complete:\n",
      "Total users processed: 2000\n",
      "Total movies per user: 10949\n",
      "Average movies above threshold per user: 3252.50\n",
      "\n",
      "Sample user-movie similarities:\n",
      "User ID: 1.0\n",
      "Number of movies above threshold: 4031\n",
      "Top 5 most similar movies:\n",
      "  'Hellboy II: The Golden Army (2008)' (ID: 57640): 0.8434\n",
      "  'Star Trek (2009)' (ID: 68358): 0.8361\n",
      "  'Ghostbusters II (1989)' (ID: 2717): 0.8361\n",
      "  'Serpent and the Rainbow, The (1988)' (ID: 4541): 0.8221\n",
      "  'Three Stooges in Orbit, The (1962)' (ID: 7252): 0.8146\n",
      "---\n",
      "User ID: 2.0\n",
      "Number of movies above threshold: 3686\n",
      "Top 5 most similar movies:\n",
      "  'Hunger Games, The (2012)' (ID: 91500): 0.8699\n",
      "  'Star Trek Into Darkness (2013)' (ID: 102445): 0.8591\n",
      "  'Matrix Reloaded, The (2003)' (ID: 6365): 0.8443\n",
      "  'Lost World: Jurassic Park, The (1997)' (ID: 1544): 0.8394\n",
      "  'Star Trek (2009)' (ID: 68358): 0.8368\n",
      "---\n",
      "User ID: 3.0\n",
      "Number of movies above threshold: 4491\n",
      "Top 5 most similar movies:\n",
      "  'Star Trek (2009)' (ID: 68358): 0.8512\n",
      "  'Hunger Games, The (2012)' (ID: 91500): 0.8407\n",
      "  'Ghostbusters II (1989)' (ID: 2717): 0.8367\n",
      "  'Matrix Reloaded, The (2003)' (ID: 6365): 0.8354\n",
      "  'Star Trek Into Darkness (2013)' (ID: 102445): 0.8245\n",
      "---\n",
      "Calculated and saved similarities for 2000 users\n",
      "\n",
      "Similarity statistics:\n",
      "Average number of similar movies per user: 3252.50\n",
      "Min: 0, Max: 7238\n",
      "\n",
      "================================================================================\n",
      "STEP 8: RECOMMENDATION GENERATION\n",
      "================================================================================\n",
      "Generating top-20 recommendations for all users in batches...\n",
      "Processing batch 1: users 1-100 of 2000\n",
      "Processed 100/2000 users (5.0%) - Elapsed: 0.23s - Est. remaining: 4.42s\n",
      "Users with recommendations so far: 93\n",
      "Processing batch 2: users 101-200 of 2000\n",
      "Processed 200/2000 users (10.0%) - Elapsed: 1.41s - Est. remaining: 3.68s\n",
      "Users with recommendations so far: 180\n",
      "Processing batch 3: users 201-300 of 2000\n",
      "Processed 300/2000 users (15.0%) - Elapsed: 2.78s - Est. remaining: 4.49s\n",
      "Users with recommendations so far: 274\n",
      "Processing batch 4: users 301-400 of 2000\n",
      "Processed 400/2000 users (20.0%) - Elapsed: 4.00s - Est. remaining: 4.04s\n",
      "Users with recommendations so far: 367\n",
      "Processing batch 5: users 401-500 of 2000\n",
      "Processed 500/2000 users (25.0%) - Elapsed: 4.90s - Est. remaining: 2.59s\n",
      "Users with recommendations so far: 454\n",
      "Processing batch 6: users 501-600 of 2000\n",
      "Processed 600/2000 users (30.0%) - Elapsed: 5.77s - Est. remaining: 2.46s\n",
      "Users with recommendations so far: 548\n",
      "Processing batch 7: users 601-700 of 2000\n",
      "Processed 700/2000 users (35.0%) - Elapsed: 6.65s - Est. remaining: 2.75s\n",
      "Users with recommendations so far: 645\n",
      "Processing batch 8: users 701-800 of 2000\n",
      "Processed 800/2000 users (40.0%) - Elapsed: 7.50s - Est. remaining: 2.07s\n",
      "Users with recommendations so far: 737\n",
      "Processing batch 9: users 801-900 of 2000\n",
      "Processed 900/2000 users (45.0%) - Elapsed: 8.44s - Est. remaining: 2.88s\n",
      "Users with recommendations so far: 827\n",
      "Processing batch 10: users 901-1000 of 2000\n",
      "Processed 1000/2000 users (50.0%) - Elapsed: 9.71s - Est. remaining: 2.20s\n",
      "Users with recommendations so far: 915\n",
      "Processing batch 11: users 1001-1100 of 2000\n",
      "Processed 1100/2000 users (55.0%) - Elapsed: 10.93s - Est. remaining: 2.48s\n",
      "Users with recommendations so far: 1010\n",
      "Processing batch 12: users 1101-1200 of 2000\n",
      "Processed 1200/2000 users (60.0%) - Elapsed: 11.98s - Est. remaining: 2.30s\n",
      "Users with recommendations so far: 1107\n",
      "Processing batch 13: users 1201-1300 of 2000\n",
      "Processed 1300/2000 users (65.0%) - Elapsed: 12.93s - Est. remaining: 1.45s\n",
      "Users with recommendations so far: 1203\n",
      "Processing batch 14: users 1301-1400 of 2000\n",
      "Processed 1400/2000 users (70.0%) - Elapsed: 14.07s - Est. remaining: 1.75s\n",
      "Users with recommendations so far: 1297\n",
      "Processing batch 15: users 1401-1500 of 2000\n",
      "Processed 1500/2000 users (75.0%) - Elapsed: 14.92s - Est. remaining: 0.84s\n",
      "Users with recommendations so far: 1388\n",
      "Processing batch 16: users 1501-1600 of 2000\n",
      "Processed 1600/2000 users (80.0%) - Elapsed: 15.89s - Est. remaining: 0.72s\n",
      "Users with recommendations so far: 1483\n",
      "Processing batch 17: users 1601-1700 of 2000\n",
      "Processed 1700/2000 users (85.0%) - Elapsed: 16.93s - Est. remaining: 1.06s\n",
      "Users with recommendations so far: 1570\n",
      "Processing batch 18: users 1701-1800 of 2000\n",
      "Processed 1800/2000 users (90.0%) - Elapsed: 18.41s - Est. remaining: 0.63s\n",
      "Users with recommendations so far: 1666\n",
      "Processing batch 19: users 1801-1900 of 2000\n",
      "Processed 1900/2000 users (95.0%) - Elapsed: 19.69s - Est. remaining: 0.37s\n",
      "Users with recommendations so far: 1760\n",
      "Processing batch 20: users 1901-2000 of 2000\n",
      "Processed 2000/2000 users (100.0%) - Elapsed: 20.83s - Est. remaining: 0.00s\n",
      "Users with recommendations so far: 1854\n",
      "\n",
      "Recommendation generation complete:\n",
      "Users with recommendations: 1854/2000 (92.7%)\n",
      "Total recommendations generated: 36735\n",
      "Average recommendations per user: 19.81\n",
      "\n",
      "Sample recommendations for 3 users:\n",
      "User ID: 1.0\n",
      "Top 5 recommended movies:\n",
      "  1. 'Hellboy II: The Golden Army (2008)' (ID: 57640): 0.8434\n",
      "  2. 'Star Trek (2009)' (ID: 68358): 0.8361\n",
      "  3. 'Ghostbusters II (1989)' (ID: 2717): 0.8361\n",
      "  4. 'Serpent and the Rainbow, The (1988)' (ID: 4541): 0.8221\n",
      "  5. 'Three Stooges in Orbit, The (1962)' (ID: 7252): 0.8146\n",
      "---\n",
      "User ID: 2.0\n",
      "Top 5 recommended movies:\n",
      "  1. 'Hunger Games, The (2012)' (ID: 91500): 0.8699\n",
      "  2. 'Star Trek Into Darkness (2013)' (ID: 102445): 0.8591\n",
      "  3. 'Matrix Reloaded, The (2003)' (ID: 6365): 0.8443\n",
      "  4. 'Star Trek (2009)' (ID: 68358): 0.8368\n",
      "  5. 'Running Man, The (1987)' (ID: 3698): 0.8264\n",
      "---\n",
      "User ID: 3.0\n",
      "Top 5 recommended movies:\n",
      "  1. 'Star Trek (2009)' (ID: 68358): 0.8512\n",
      "  2. 'Hunger Games, The (2012)' (ID: 91500): 0.8407\n",
      "  3. 'Ghostbusters II (1989)' (ID: 2717): 0.8367\n",
      "  4. 'Matrix Reloaded, The (2003)' (ID: 6365): 0.8354\n",
      "  5. 'Star Trek Into Darkness (2013)' (ID: 102445): 0.8245\n",
      "---\n",
      "Processed recommendation chunk 1: users 1-1000 of 1854\n",
      "Processed recommendation chunk 2: users 1001-1854 of 1854\n",
      "Saved recommendation chunk 1: recommendations 1-10000 of 36735\n",
      "Saved recommendation chunk 2: recommendations 10001-20000 of 36735\n",
      "Saved recommendation chunk 3: recommendations 20001-30000 of 36735\n",
      "Saved recommendation chunk 4: recommendations 30001-36735 of 36735\n",
      "Saved recommendations to CSV file with 36735 entries\n",
      "\n",
      "================================================================================\n",
      "STEP 9: MODEL EVALUATION\n",
      "================================================================================\n",
      "Running evaluation with RMSE and MAE metrics...\n",
      "Evaluating recommendation model using RMSE and MAE with batching...\n",
      "Users in test set with similarity data: 2000/2000 (100.0%)\n",
      "Evaluating batch 1: users 1-100 of 2000\n",
      "Batch metrics - RMSE: 1.0602, MAE: 0.8531, Predictions: 2259\n",
      "Processed 100/2000 users (5.0%) - Elapsed: 0.19s - Est. remaining: 3.61s\n",
      "Evaluating batch 2: users 101-200 of 2000\n",
      "Batch metrics - RMSE: 1.0745, MAE: 0.8643, Predictions: 2842\n",
      "Processed 200/2000 users (10.0%) - Elapsed: 1.25s - Est. remaining: 3.66s\n",
      "Evaluating batch 3: users 201-300 of 2000\n",
      "Batch metrics - RMSE: 1.0205, MAE: 0.8144, Predictions: 2950\n",
      "Processed 300/2000 users (15.0%) - Elapsed: 2.29s - Est. remaining: 3.22s\n",
      "Evaluating batch 4: users 301-400 of 2000\n",
      "Batch metrics - RMSE: 1.0831, MAE: 0.8657, Predictions: 3199\n",
      "Processed 400/2000 users (20.0%) - Elapsed: 3.73s - Est. remaining: 5.06s\n",
      "Evaluating batch 5: users 401-500 of 2000\n",
      "Batch metrics - RMSE: 1.0719, MAE: 0.8531, Predictions: 3253\n",
      "Processed 500/2000 users (25.0%) - Elapsed: 5.22s - Est. remaining: 5.25s\n",
      "Evaluating batch 6: users 501-600 of 2000\n",
      "Batch metrics - RMSE: 1.0288, MAE: 0.8262, Predictions: 2682\n",
      "Processed 600/2000 users (30.0%) - Elapsed: 6.55s - Est. remaining: 2.96s\n",
      "Evaluating batch 7: users 601-700 of 2000\n",
      "Batch metrics - RMSE: 1.0152, MAE: 0.8072, Predictions: 3038\n",
      "Processed 700/2000 users (35.0%) - Elapsed: 8.10s - Est. remaining: 4.55s\n",
      "Evaluating batch 8: users 701-800 of 2000\n",
      "Batch metrics - RMSE: 1.1997, MAE: 0.9567, Predictions: 3915\n",
      "Processed 800/2000 users (40.0%) - Elapsed: 9.66s - Est. remaining: 4.19s\n",
      "Evaluating batch 9: users 801-900 of 2000\n",
      "Batch metrics - RMSE: 1.1236, MAE: 0.8975, Predictions: 2575\n",
      "Processed 900/2000 users (45.0%) - Elapsed: 10.95s - Est. remaining: 2.39s\n",
      "Evaluating batch 10: users 901-1000 of 2000\n",
      "Batch metrics - RMSE: 1.0274, MAE: 0.8252, Predictions: 3806\n",
      "Processed 1000/2000 users (50.0%) - Elapsed: 12.49s - Est. remaining: 3.30s\n",
      "Evaluating batch 11: users 1001-1100 of 2000\n",
      "Batch metrics - RMSE: 1.0854, MAE: 0.8828, Predictions: 2532\n",
      "Processed 1100/2000 users (55.0%) - Elapsed: 13.98s - Est. remaining: 2.11s\n",
      "Evaluating batch 12: users 1101-1200 of 2000\n",
      "Batch metrics - RMSE: 1.0039, MAE: 0.8008, Predictions: 3066\n",
      "Processed 1200/2000 users (60.0%) - Elapsed: 15.67s - Est. remaining: 1.91s\n",
      "Evaluating batch 13: users 1201-1300 of 2000\n",
      "Batch metrics - RMSE: 1.0048, MAE: 0.8050, Predictions: 2571\n",
      "Processed 1300/2000 users (65.0%) - Elapsed: 17.12s - Est. remaining: 1.65s\n",
      "Evaluating batch 14: users 1301-1400 of 2000\n",
      "Batch metrics - RMSE: 1.0530, MAE: 0.8361, Predictions: 2623\n",
      "Processed 1400/2000 users (70.0%) - Elapsed: 18.66s - Est. remaining: 1.55s\n",
      "Evaluating batch 15: users 1401-1500 of 2000\n",
      "Batch metrics - RMSE: 1.0376, MAE: 0.8198, Predictions: 2522\n",
      "Processed 1500/2000 users (75.0%) - Elapsed: 20.09s - Est. remaining: 1.38s\n",
      "Evaluating batch 16: users 1501-1600 of 2000\n",
      "Batch metrics - RMSE: 1.0605, MAE: 0.8505, Predictions: 3134\n",
      "Processed 1600/2000 users (80.0%) - Elapsed: 21.61s - Est. remaining: 1.74s\n",
      "Evaluating batch 17: users 1601-1700 of 2000\n",
      "Batch metrics - RMSE: 1.0476, MAE: 0.8413, Predictions: 2757\n",
      "Processed 1700/2000 users (85.0%) - Elapsed: 22.76s - Est. remaining: 0.57s\n",
      "Evaluating batch 18: users 1701-1800 of 2000\n",
      "Batch metrics - RMSE: 1.1434, MAE: 0.9044, Predictions: 3037\n",
      "Processed 1800/2000 users (90.0%) - Elapsed: 23.76s - Est. remaining: 0.32s\n",
      "Evaluating batch 19: users 1801-1900 of 2000\n",
      "Batch metrics - RMSE: 1.0135, MAE: 0.8094, Predictions: 3258\n",
      "Processed 1900/2000 users (95.0%) - Elapsed: 24.71s - Est. remaining: 0.20s\n",
      "Evaluating batch 20: users 1901-2000 of 2000\n",
      "Batch metrics - RMSE: 1.0361, MAE: 0.8299, Predictions: 2958\n",
      "Processed 2000/2000 users (100.0%) - Elapsed: 25.74s - Est. remaining: 0.00s\n",
      "\n",
      "Evaluation results:\n",
      "Users evaluated: 2000\n",
      "Total predictions: 58977\n",
      "RMSE: 1.0626\n",
      "MAE: 0.8485\n",
      "\n",
      "Stored evaluation metrics:\n",
      "  rmse: 1.0625890294276914\n",
      "  mae: 0.8484826394104481\n",
      "  num_users_evaluated: 2000\n",
      "  num_predictions: 58977\n",
      "Saved evaluation metrics to CSV files\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF CONTENT-BASED RECOMMENDATION SYSTEM\n",
      "================================================================================\n",
      "\n",
      "Data Information:\n",
      "- Processed 10993 movie feature records\n",
      "- Vocabulary size: 42589 unique words\n",
      "- Generated feature vectors for 10949 movies\n",
      "- Generated feature vectors for 2000 users\n",
      "- Average similar movies per user: 3252.50\n",
      "- Average recommendations per user: 19.81\n",
      "\n",
      "Performance Metrics:\n",
      "- RMSE: 1.0626\n",
      "- MAE: 0.8485\n",
      "- Users evaluated: 2000\n",
      "- Total predictions: 58977\n",
      "\n",
      "Advantages of this approach:\n",
      "- Log-Likelihood identifies more meaningful words compared to TF-IDF\n",
      "- Word2Vec captures semantic relationships between words\n",
      "- Handles new movies effectively (cold start for items)\n",
      "- Generates personalized recommendations based on content preferences\n",
      "- Doesn't require item-item similarity calculations\n",
      "- Memory-optimized batch processing prevents RAM overflow during long runs\n",
      "\n",
      "Saved Files:\n",
      "- content_based_evaluation.csv (0.00 MB)\n",
      "- content_based_recommendations.csv (2.17 MB)\n",
      "- content_based_recommendations.pkl (0.55 MB)\n",
      "- corpus_word_counts.pkl (0.44 MB)\n",
      "- movie_id_to_idx.pkl (0.07 MB)\n",
      "- movie_ll_values.pkl (3.54 MB)\n",
      "- movie_vectors.pkl (12.92 MB)\n",
      "- user_id_to_idx.pkl (0.04 MB)\n",
      "- user_movie_similarities.pkl (76.55 MB)\n",
      "- user_vectors.pkl (2.39 MB)\n",
      "- word2vec_model (4.86 MB)\n",
      "\n",
      "Final memory usage: 1336.45 MB\n",
      "\n",
      "Content-Based Filtering Model Successfully Implemented!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import heapq\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import math\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import ast  # To safely evaluate string representations of lists\n",
    "import sys\n",
    "import gc  # Add garbage collector for memory management\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONTENT-BASED MOVIE RECOMMENDATION SYSTEM WITH LOG-LIKELIHOOD AND WORD2VEC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Set paths\n",
    "input_path = \"./\"  # Current directory where stage1.py saved the files\n",
    "output_path = \"./rec/content-recommendations\"\n",
    "top_n = 20\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# # Initialize NLTK tools\n",
    "# nltk.download('punkt', quiet=True)\n",
    "# nltk.download('stopwords', quiet=True)\n",
    "# nltk.download('wordnet', quiet=True)\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Model parameters/\n",
    "similarity_threshold = 0.5  # Minimum similarity to consider\n",
    "word2vec_dim = 300  # Dimensionality of Word2Vec embeddings\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: DATA LOADING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load processed data from stage1.py\"\"\"\n",
    "    print(\"Loading processed data from stage1.py...\")\n",
    "    \n",
    "    # Data containers\n",
    "    data = {}\n",
    "    \n",
    "    # Load movie features\n",
    "    movie_features_path = os.path.join(input_path, './processed/processed_movie_features.csv')\n",
    "    if os.path.exists(movie_features_path):\n",
    "        # First, load without the tokens column to save memory\n",
    "        data['movie_features'] = pd.read_csv(movie_features_path)\n",
    "        # Convert string representation of tokens and top_keywords back to lists\n",
    "        data['movie_features']['tokens'] = data['movie_features']['tokens'].apply(\n",
    "            lambda x: ast.literal_eval(x) if isinstance(x, str) else []\n",
    "        )\n",
    "        data['movie_features']['top_keywords'] = data['movie_features']['top_keywords'].apply(\n",
    "            lambda x: ast.literal_eval(x) if isinstance(x, str) else []\n",
    "        )\n",
    "        print(f\"Loaded features for {len(data['movie_features'])} movies\")\n",
    "        print(\"\\nSample of movie features data:\")\n",
    "        print(data['movie_features'][['movieId', 'title', 'top_keywords']].head(3))\n",
    "        \n",
    "        # Print token statistics\n",
    "        token_lengths = [len(tokens) for tokens in data['movie_features']['tokens']]\n",
    "        print(f\"\\nAverage token count per movie: {np.mean(token_lengths):.2f}\")\n",
    "        print(f\"Min token count: {min(token_lengths)}, Max token count: {max(token_lengths)}\")\n",
    "    else:\n",
    "        print(f\"Error: Movie features not found at {movie_features_path}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Load normalized ratings\n",
    "    ratings_path = os.path.join(input_path, './processed/normalized_ratings.csv')\n",
    "    if os.path.exists(ratings_path):\n",
    "        # Read in chunks to save memory\n",
    "        chunk_size = 100000  # Adjust based on dataset size\n",
    "        chunks = []\n",
    "        for chunk in pd.read_csv(ratings_path, chunksize=chunk_size):\n",
    "            chunks.append(chunk)\n",
    "        data['ratings'] = pd.concat(chunks)\n",
    "        print(f\"\\nLoaded {len(data['ratings'])} normalized ratings\")\n",
    "        print(\"\\nSample of normalized ratings data:\")\n",
    "        print(data['ratings'].head(3))\n",
    "        \n",
    "        # Print rating statistics\n",
    "        print(f\"\\nNumber of unique users: {data['ratings']['userId'].nunique()}\")\n",
    "        print(f\"Number of unique movies: {data['ratings']['movieId'].nunique()}\")\n",
    "        print(f\"Rating sparsity: {(1 - len(data['ratings']) / (data['ratings']['userId'].nunique() * data['ratings']['movieId'].nunique())) * 100:.4f}%\")\n",
    "    else:\n",
    "        print(f\"Error: Normalized ratings not found at {ratings_path}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Create training and testing sets with 80-20 split\n",
    "    if 'ratings' in data:\n",
    "        # Sort by timestamp if available to ensure reproducibility\n",
    "        if 'timestamp' in data['ratings'].columns:\n",
    "            data['ratings'] = data['ratings'].sort_values('timestamp')\n",
    "        \n",
    "        # Group by user to ensure each user has both training and testing data\n",
    "        user_groups = data['ratings'].groupby('userId')\n",
    "        train_chunks = []\n",
    "        test_chunks = []\n",
    "        \n",
    "        user_count = 0\n",
    "        total_users = len(user_groups)\n",
    "        \n",
    "        for user_id, group in user_groups:\n",
    "            n = len(group)\n",
    "            split_idx = int(n * 0.8)\n",
    "            train_chunks.append(group.iloc[:split_idx])\n",
    "            test_chunks.append(group.iloc[split_idx:])\n",
    "            \n",
    "            user_count += 1\n",
    "            # Process in batches to avoid excessive memory usage\n",
    "            if len(train_chunks) >= 1000 or user_count == total_users:\n",
    "                gc.collect()  # Force garbage collection\n",
    "        \n",
    "        # Concatenate all chunks\n",
    "        data['train_ratings'] = pd.concat(train_chunks).reset_index(drop=True)\n",
    "        data['test_ratings'] = pd.concat(test_chunks).reset_index(drop=True)\n",
    "        \n",
    "        # Free memory\n",
    "        del train_chunks, test_chunks\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"\\nSplit ratings into {len(data['train_ratings'])} training and {len(data['test_ratings'])} testing samples\")\n",
    "        print(f\"Training set covers {data['train_ratings']['userId'].nunique()} users and {data['train_ratings']['movieId'].nunique()} movies\")\n",
    "        print(f\"Testing set covers {data['test_ratings']['userId'].nunique()} users and {data['test_ratings']['movieId'].nunique()} movies\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load the data\n",
    "data = load_data()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: CORPUS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Build corpus word counts from movie features\n",
    "if 'movie_features' in data:\n",
    "    print(\"Building vocabulary and word frequency counts...\")\n",
    "    \n",
    "    corpus_word_counts = Counter()\n",
    "    \n",
    "    # Process in batches to avoid memory spikes\n",
    "    batch_size = 1000\n",
    "    total_movies = len(data['movie_features'])\n",
    "    \n",
    "    for i in range(0, total_movies, batch_size):\n",
    "        batch_end = min(i + batch_size, total_movies)\n",
    "        batch = data['movie_features'].iloc[i:batch_end]\n",
    "        \n",
    "        for tokens in batch['tokens']:\n",
    "            corpus_word_counts.update(tokens)\n",
    "        \n",
    "        # Log progress\n",
    "        print(f\"Processed {batch_end}/{total_movies} movies ({batch_end/total_movies*100:.1f}%)\")\n",
    "    \n",
    "    data['corpus_word_counts'] = corpus_word_counts\n",
    "    \n",
    "    # Save corpus word counts\n",
    "    with open(os.path.join(output_path, 'corpus_word_counts.pkl'), 'wb') as f:\n",
    "        pickle.dump(corpus_word_counts, f)\n",
    "    \n",
    "    print(f\"Built vocabulary with {len(corpus_word_counts)} unique words\")\n",
    "    print(f\"Total words in corpus: {sum(corpus_word_counts.values())}\")\n",
    "    \n",
    "    # Display top 20 most common words\n",
    "    print(\"\\nTop 20 most common words in the corpus:\")\n",
    "    for word, count in corpus_word_counts.most_common(20):\n",
    "        print(f\"'{word}': {count}\")\n",
    "\n",
    "    # Clear memory\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: LOG-LIKELIHOOD CALCULATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def calculate_log_likelihood(movie_features, corpus_word_counts, batch_size=100):\n",
    "    \"\"\"Calculate Log-Likelihood values for words in each movie in batches\"\"\"\n",
    "    print(\"Calculating Log-Likelihood values for all movies in batches...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Calculate total corpus size\n",
    "    total_corpus_size = sum(corpus_word_counts.values())\n",
    "    print(f\"Total corpus size: {total_corpus_size} words\")\n",
    "    \n",
    "    # Initialize container for movie features\n",
    "    movie_ll_values = {}\n",
    "    \n",
    "    # Process each movie document in batches\n",
    "    total_movies = len(movie_features)\n",
    "    \n",
    "    for batch_start in range(0, total_movies, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, total_movies)\n",
    "        print(f\"Processing batch {batch_start//batch_size + 1}: movies {batch_start+1}-{batch_end} of {total_movies}\")\n",
    "        \n",
    "        # Get batch of movies\n",
    "        batch = movie_features.iloc[batch_start:batch_end]\n",
    "        \n",
    "        for _, row in batch.iterrows():\n",
    "            movie_id = row['movieId']\n",
    "            tokens = row['tokens']\n",
    "            \n",
    "            if not tokens:\n",
    "                continue\n",
    "            \n",
    "            # Count word occurrences in this movie\n",
    "            movie_word_counts = Counter(tokens)\n",
    "            movie_size = sum(movie_word_counts.values())\n",
    "            \n",
    "            # Calculate Log-Likelihood for each word\n",
    "            movie_ll_values[movie_id] = {}\n",
    "            \n",
    "            for word, count in movie_word_counts.items():\n",
    "                # Observed frequencies\n",
    "                a = count  # Occurrences in this movie\n",
    "                b = corpus_word_counts[word] - count  # Occurrences in other movies\n",
    "                c = movie_size  # Total words in this movie\n",
    "                d = total_corpus_size - movie_size  # Total words in other movies\n",
    "                \n",
    "                # Expected counts based on corpus distribution\n",
    "                e1 = c * (a + b) / (c + d)\n",
    "                e2 = d * (a + b) / (c + d)\n",
    "                \n",
    "                # Log-Likelihood calculation\n",
    "                ll = 0\n",
    "                if a > 0 and e1 > 0:\n",
    "                    ll += a * math.log(a / e1)\n",
    "                if b > 0 and e2 > 0:\n",
    "                    ll += b * math.log(b / e2)\n",
    "                \n",
    "                ll = 2 * ll\n",
    "                movie_ll_values[movie_id][word] = ll\n",
    "        \n",
    "        # Log progress and elapsed time\n",
    "        elapsed = time.time() - start_time\n",
    "        progress = batch_end / total_movies * 100\n",
    "        remaining = elapsed / (batch_end - batch_start) * (total_movies - batch_end) if batch_end < total_movies else 0\n",
    "        print(f\"Progress: {progress:.1f}% - Elapsed: {elapsed:.2f}s - Est. remaining: {remaining:.2f}s\")\n",
    "        \n",
    "        # Force garbage collection after each batch\n",
    "        gc.collect()\n",
    "    \n",
    "    # Show sample LL values for a movie\n",
    "    if movie_ll_values:\n",
    "        sample_movie_id = next(iter(movie_ll_values.keys()))\n",
    "        sample_movie_title = movie_features[movie_features['movieId'] == sample_movie_id]['title'].values[0]\n",
    "        print(f\"\\nSample Log-Likelihood values for movie '{sample_movie_title}' (ID: {sample_movie_id}):\")\n",
    "        \n",
    "        # Get top 10 words by LL value\n",
    "        top_ll_words = sorted(movie_ll_values[sample_movie_id].items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        for word, ll_value in top_ll_words:\n",
    "            print(f\"Word: '{word}', LL Value: {ll_value:.2f}\")\n",
    "    \n",
    "    return movie_ll_values\n",
    "\n",
    "# Calculate Log-Likelihood if movie features are available\n",
    "if 'movie_features' in data and 'corpus_word_counts' in data:\n",
    "    movie_ll_values = calculate_log_likelihood(data['movie_features'], data['corpus_word_counts'], batch_size=100)\n",
    "    data['movie_ll_values'] = movie_ll_values\n",
    "    \n",
    "    # Save Log-Likelihood values\n",
    "    with open(os.path.join(output_path, 'movie_ll_values.pkl'), 'wb') as f:\n",
    "        pickle.dump(movie_ll_values, f)\n",
    "    \n",
    "    print(f\"Calculated Log-Likelihood values for {len(movie_ll_values)} movies\")\n",
    "    \n",
    "    # Calculate average number of words with high LL values\n",
    "    high_ll_counts = []\n",
    "    for movie_id, ll_dict in movie_ll_values.items():\n",
    "        high_ll_words = [word for word, value in ll_dict.items() if value > 10]  # Threshold of 10\n",
    "        high_ll_counts.append(len(high_ll_words))\n",
    "    \n",
    "    print(f\"Average number of words with LL > 10 per movie: {np.mean(high_ll_counts):.2f}\")\n",
    "    print(f\"Min: {min(high_ll_counts)}, Max: {max(high_ll_counts)}\")\n",
    "    \n",
    "    # Free memory\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: WORD2VEC MODEL TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def train_word2vec(movie_features, vector_size=100, batch_size=1000):\n",
    "    \"\"\"Train Word2Vec model on movie tokens with memory optimization\"\"\"\n",
    "    print(f\"Training Word2Vec model with {vector_size} dimensions...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Extract token lists from movie features in batches\n",
    "    tokenized_corpus = []\n",
    "    total_movies = len(movie_features)\n",
    "    \n",
    "    for i in range(0, total_movies, batch_size):\n",
    "        batch_end = min(i + batch_size, total_movies)\n",
    "        batch = movie_features.iloc[i:batch_end]\n",
    "        \n",
    "        batch_tokens = list(batch['tokens'])\n",
    "        tokenized_corpus.extend(batch_tokens)\n",
    "        \n",
    "        # Log progress\n",
    "        print(f\"Loaded tokens from {batch_end}/{total_movies} movies ({batch_end/total_movies*100:.1f}%)\")\n",
    "    \n",
    "    # Print corpus statistics\n",
    "    total_tokens = sum(len(tokens) for tokens in tokenized_corpus)\n",
    "    print(f\"Training corpus size: {total_tokens} tokens from {len(tokenized_corpus)} documents\")\n",
    "    \n",
    "    # Train Word2Vec model using CBOW approach with memory optimization\n",
    "    print(\"Starting Word2Vec training (this may take a few minutes)...\")\n",
    "    word2vec_model = Word2Vec(\n",
    "        sentences=tokenized_corpus,\n",
    "        vector_size=vector_size,\n",
    "        window=5,\n",
    "        min_count=5,\n",
    "        workers=4,\n",
    "        epochs=30,  # Reduced from 50 to save memory\n",
    "        sg=0  # CBOW model\n",
    "    )\n",
    "    \n",
    "    # Free memory - no longer need the full corpus\n",
    "    del tokenized_corpus\n",
    "    gc.collect()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Word2Vec training completed in {elapsed:.2f} seconds\")\n",
    "    \n",
    "    # Print model statistics\n",
    "    vocab_size = len(word2vec_model.wv)\n",
    "    print(f\"Word2Vec model vocabulary size: {vocab_size} words\")\n",
    "    \n",
    "    # Show some example vectors for common words\n",
    "    print(\"\\nExample word vectors from the trained model:\")\n",
    "    common_words = [word for word, _ in data['corpus_word_counts'].most_common(10)]\n",
    "    for word in common_words:\n",
    "        if word in word2vec_model.wv:\n",
    "            # Show just the first 5 dimensions of the vector\n",
    "            print(f\"'{word}': {word2vec_model.wv[word][:5]}...\")\n",
    "    \n",
    "    # Show some word similarities\n",
    "    if len(word2vec_model.wv) > 0:\n",
    "        print(\"\\nExample word similarities:\")\n",
    "        try:\n",
    "            # Try some movie-related terms\n",
    "            for word in ['action', 'love', 'hero', 'villain']:\n",
    "                if word in word2vec_model.wv:\n",
    "                    similar_words = word2vec_model.wv.most_similar(word, topn=5)\n",
    "                    print(f\"Words similar to '{word}': {similar_words}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not compute word similarities: {str(e)}\")\n",
    "    \n",
    "    return word2vec_model\n",
    "\n",
    "# Train Word2Vec if movie features are available\n",
    "if 'movie_features' in data:\n",
    "    word2vec_model = train_word2vec(data['movie_features'], word2vec_dim)\n",
    "    data['word2vec_model'] = word2vec_model\n",
    "    \n",
    "    # Save Word2Vec model\n",
    "    word2vec_path = os.path.join(output_path, 'word2vec_model')\n",
    "    word2vec_model.save(word2vec_path)\n",
    "    \n",
    "    print(f\"Trained and saved Word2Vec model with {len(word2vec_model.wv)} words\")\n",
    "    \n",
    "    # Free memory\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: MOVIE VECTOR GENERATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def generate_movie_vectors(movie_ll_values, word2vec_model, movie_features, batch_size=100):\n",
    "    \"\"\"Generate movie feature vectors using Log-Likelihood and Word2Vec in batches\"\"\"\n",
    "    print(\"Generating movie feature vectors using Log-Likelihood + Word2Vec in batches...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    movie_vectors = {}\n",
    "    successful_vectors = 0\n",
    "    no_words_found = 0\n",
    "    low_ll_sum = 0\n",
    "    \n",
    "    # Get movie IDs from LL values\n",
    "    movie_ids = list(movie_ll_values.keys())\n",
    "    total_movies = len(movie_ids)\n",
    "    \n",
    "    # Process movies in batches\n",
    "    for batch_start in range(0, total_movies, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, total_movies)\n",
    "        batch_movie_ids = movie_ids[batch_start:batch_end]\n",
    "        \n",
    "        print(f\"Processing batch {batch_start//batch_size + 1}: movies {batch_start+1}-{batch_end} of {total_movies}\")\n",
    "        \n",
    "        for movie_id in batch_movie_ids:\n",
    "            # Sort words by LL value and select top 200\n",
    "            ll_values = movie_ll_values[movie_id]\n",
    "            top_words = sorted(ll_values.items(), key=lambda x: x[1], reverse=True)[:200]\n",
    "            \n",
    "            if not top_words:\n",
    "                no_words_found += 1\n",
    "                continue\n",
    "            \n",
    "            # Combine Word2Vec vectors weighted by Log-Likelihood values\n",
    "            weighted_vectors = []\n",
    "            ll_sum = 0\n",
    "            words_used = 0\n",
    "            \n",
    "            for word, ll_value in top_words:\n",
    "                if ll_value <= 0:\n",
    "                    continue\n",
    "                \n",
    "                if word in word2vec_model.wv:\n",
    "                    weighted_vectors.append(word2vec_model.wv[word] * ll_value)\n",
    "                    ll_sum += ll_value\n",
    "                    words_used += 1\n",
    "            \n",
    "            if weighted_vectors and ll_sum > 0:\n",
    "                # Calculate the weighted average vector\n",
    "                movie_vector = np.sum(weighted_vectors, axis=0) / ll_sum\n",
    "                \n",
    "                # Normalize to unit length\n",
    "                norm = np.linalg.norm(movie_vector)\n",
    "                if norm > 0:\n",
    "                    movie_vector = movie_vector / norm\n",
    "                    movie_vectors[movie_id] = movie_vector\n",
    "                    successful_vectors += 1\n",
    "            else:\n",
    "                low_ll_sum += 1\n",
    "        \n",
    "        # Log progress and elapsed time\n",
    "        elapsed = time.time() - start_time\n",
    "        progress = batch_end / total_movies * 100\n",
    "        remaining = elapsed / (batch_end - batch_start) * (total_movies - batch_end) if batch_end < total_movies else 0\n",
    "        print(f\"Progress: {progress:.1f}% - Elapsed: {elapsed:.2f}s - Est. remaining: {remaining:.2f}s\")\n",
    "        print(f\"Successfully created vectors: {successful_vectors}/{batch_end}\")\n",
    "        \n",
    "        # Force garbage collection after each batch\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"\\nVector generation complete:\")\n",
    "    print(f\"Successfully created vectors: {successful_vectors}/{total_movies} ({successful_vectors/total_movies*100:.1f}%)\")\n",
    "    print(f\"Movies with no words found: {no_words_found}\")\n",
    "    print(f\"Movies with too low LL sum: {low_ll_sum}\")\n",
    "    \n",
    "    # Display sample movie vectors\n",
    "    if movie_vectors:\n",
    "        print(\"\\nSample movie vectors:\")\n",
    "        for movie_id in list(movie_vectors.keys())[:3]:\n",
    "            movie_title = movie_features[movie_features['movieId'] == movie_id]['title'].values[0]\n",
    "            vector = movie_vectors[movie_id]\n",
    "            print(f\"Movie: '{movie_title}' (ID: {movie_id})\")\n",
    "            print(f\"Vector shape: {vector.shape}\")\n",
    "            print(f\"Vector norm: {np.linalg.norm(vector):.4f}\")\n",
    "            print(f\"First 5 dimensions: {vector[:5]}\")\n",
    "            print(\"---\")\n",
    "    \n",
    "    return movie_vectors\n",
    "\n",
    "# Generate movie vectors if Word2Vec and LL values are available\n",
    "if 'word2vec_model' in data and 'movie_ll_values' in data:\n",
    "    movie_vectors = generate_movie_vectors(\n",
    "        data['movie_ll_values'], \n",
    "        data['word2vec_model'],\n",
    "        data['movie_features'],\n",
    "        batch_size=100\n",
    "    )\n",
    "    data['movie_vectors'] = movie_vectors\n",
    "    \n",
    "    # Save movie vectors\n",
    "    with open(os.path.join(output_path, 'movie_vectors.pkl'), 'wb') as f:\n",
    "        pickle.dump(movie_vectors, f)\n",
    "    \n",
    "    # Create movie ID to index mapping\n",
    "    movie_id_to_idx = {movie_id: i for i, movie_id in enumerate(movie_vectors.keys())}\n",
    "    data['movie_id_to_idx'] = movie_id_to_idx\n",
    "    \n",
    "    # Save the mapping\n",
    "    with open(os.path.join(output_path, 'movie_id_to_idx.pkl'), 'wb') as f:\n",
    "        pickle.dump(movie_id_to_idx, f)\n",
    "    \n",
    "    print(f\"Generated and saved feature vectors for {len(movie_vectors)} movies\")\n",
    "    \n",
    "    # Calculate and display vector statistics\n",
    "    vector_norms = [np.linalg.norm(v) for v in movie_vectors.values()]\n",
    "    print(f\"\\nVector statistics:\")\n",
    "    print(f\"Average vector norm: {np.mean(vector_norms):.4f}\")\n",
    "    print(f\"Vector dimensionality: {word2vec_dim}\")\n",
    "    \n",
    "    # Free memory\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: USER VECTOR GENERATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def generate_user_vectors(movie_vectors, train_ratings, batch_size=100):\n",
    "    \"\"\"Generate user feature vectors based on rated movies and their content in batches\"\"\"\n",
    "    print(\"Generating user feature vectors based on movie ratings in batches...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    user_vectors = {}\n",
    "    successful_vectors = 0\n",
    "    no_ratings_found = 0\n",
    "    no_vectors_for_movies = 0\n",
    "    low_weight_sum = 0\n",
    "    \n",
    "    # Create a rating cache for quick lookups\n",
    "    # This can be memory intensive for large datasets, but speeds up processing\n",
    "    user_ratings_dict = {}\n",
    "    print(\"Creating user ratings lookup dictionary...\")\n",
    "    \n",
    "    # Process in chunks to avoid memory issues\n",
    "    for _, row in train_ratings.iterrows():\n",
    "        user_id = row['userId']\n",
    "        movie_id = row['movieId']\n",
    "        rating = row['rating']\n",
    "        \n",
    "        if user_id not in user_ratings_dict:\n",
    "            user_ratings_dict[user_id] = {}\n",
    "        \n",
    "        user_ratings_dict[user_id][movie_id] = rating\n",
    "    \n",
    "    # Process each user\n",
    "    user_ids = list(user_ratings_dict.keys())\n",
    "    total_users = len(user_ids)\n",
    "    print(f\"Processing {total_users} users in batches...\")\n",
    "    \n",
    "    # Process users in batches\n",
    "    for batch_start in range(0, total_users, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, total_users)\n",
    "        batch_user_ids = user_ids[batch_start:batch_end]\n",
    "        \n",
    "        print(f\"Processing batch {batch_start//batch_size + 1}: users {batch_start+1}-{batch_end} of {total_users}\")\n",
    "        \n",
    "        for user_id in batch_user_ids:\n",
    "            # Get user ratings\n",
    "            user_ratings = user_ratings_dict[user_id]\n",
    "            \n",
    "            if len(user_ratings) == 0:\n",
    "                no_ratings_found += 1\n",
    "                continue\n",
    "            \n",
    "            weighted_vectors = []\n",
    "            weight_sum = 0\n",
    "            movies_with_vectors = 0\n",
    "            movies_without_vectors = 0\n",
    "            \n",
    "            for movie_id, rating in user_ratings.items():\n",
    "                # Center rating at 3.0 as described in the papers\n",
    "                weight = rating - 3.0\n",
    "                \n",
    "                # Skip if movie vector is not available\n",
    "                if movie_id not in movie_vectors:\n",
    "                    movies_without_vectors += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    movies_with_vectors += 1\n",
    "                \n",
    "                if weight != 0:\n",
    "                    weighted_vectors.append(movie_vectors[movie_id] * weight)\n",
    "                    weight_sum += abs(weight)\n",
    "            \n",
    "            if weighted_vectors and weight_sum > 0:\n",
    "                # Calculate the weighted average vector\n",
    "                user_vector = np.sum(weighted_vectors, axis=0) / weight_sum\n",
    "                \n",
    "                # Normalize to unit length\n",
    "                norm = np.linalg.norm(user_vector)\n",
    "                if norm > 0:\n",
    "                    user_vector = user_vector / norm\n",
    "                    user_vectors[user_id] = user_vector\n",
    "                    successful_vectors += 1\n",
    "            else:\n",
    "                low_weight_sum += 1\n",
    "        \n",
    "        # Log progress\n",
    "        elapsed = time.time() - start_time\n",
    "        progress = batch_end / total_users * 100\n",
    "        remaining = elapsed / (batch_end - batch_start) * (total_users - batch_end) if batch_end < total_users else 0\n",
    "        print(f\"Progress: {progress:.1f}% - Elapsed: {elapsed:.2f}s - Est. remaining: {remaining:.2f}s\")\n",
    "        print(f\"Successfully created vectors: {successful_vectors}\")\n",
    "        \n",
    "        # Force garbage collection after each batch\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"\\nUser vector generation complete:\")\n",
    "    print(f\"Successfully created vectors: {successful_vectors}/{total_users} ({successful_vectors/total_users*100:.1f}%)\")\n",
    "    print(f\"Users with no ratings: {no_ratings_found}\")\n",
    "    print(f\"Users with no vectorized movies: {no_vectors_for_movies}\")\n",
    "    print(f\"Users with too low weight sum: {low_weight_sum}\")\n",
    "    \n",
    "    # Free memory\n",
    "    del user_ratings_dict\n",
    "    gc.collect()\n",
    "    \n",
    "    # Display sample user vectors\n",
    "    if user_vectors:\n",
    "        print(\"\\nSample user vectors:\")\n",
    "        for user_id in list(user_vectors.keys())[:3]:\n",
    "            vector = user_vectors[user_id]\n",
    "            user_rating_count = len([r for r in train_ratings[train_ratings['userId'] == user_id]])\n",
    "            print(f\"User ID: {user_id}\")\n",
    "            print(f\"Number of ratings: {user_rating_count}\")\n",
    "            print(f\"Vector shape: {vector.shape}\")\n",
    "            print(f\"Vector norm: {np.linalg.norm(vector):.4f}\")\n",
    "            print(f\"First 5 dimensions: {vector[:5]}\")\n",
    "            print(\"---\")\n",
    "    \n",
    "    return user_vectors\n",
    "\n",
    "# Generate user vectors if movie vectors and training ratings are available\n",
    "if 'movie_vectors' in data and 'train_ratings' in data:\n",
    "    user_vectors = generate_user_vectors(data['movie_vectors'], data['train_ratings'], batch_size=100)\n",
    "    data['user_vectors'] = user_vectors\n",
    "    \n",
    "    # Save user vectors\n",
    "    with open(os.path.join(output_path, 'user_vectors.pkl'), 'wb') as f:\n",
    "        pickle.dump(user_vectors, f)\n",
    "    \n",
    "    # Create user ID to index mapping\n",
    "    user_id_to_idx = {user_id: i for i, user_id in enumerate(user_vectors.keys())}\n",
    "    data['user_id_to_idx'] = user_id_to_idx\n",
    "    \n",
    "    # Save the mapping\n",
    "    with open(os.path.join(output_path, 'user_id_to_idx.pkl'), 'wb') as f:\n",
    "        pickle.dump(user_id_to_idx, f)\n",
    "    \n",
    "    print(f\"Generated and saved feature vectors for {len(user_vectors)} users\")\n",
    "    \n",
    "    # Calculate and display vector statistics\n",
    "    vector_norms = [np.linalg.norm(v) for v in user_vectors.values()]\n",
    "    print(f\"\\nVector statistics:\")\n",
    "    print(f\"Average vector norm: {np.mean(vector_norms):.4f}\")\n",
    "    print(f\"Vector dimensionality: {word2vec_dim}\")\n",
    "    \n",
    "    # Free memory\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 7: USER-MOVIE SIMILARITY CALCULATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def calculate_user_movie_similarity(user_vectors, movie_vectors, threshold=0.3, batch_size=50):\n",
    "    \"\"\"Calculate similarity between users and movies in batches\"\"\"\n",
    "    print(f\"Calculating user-movie similarity with threshold {threshold} in batches...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Store similarities in a dictionary of dictionaries\n",
    "    # {user_id: {movie_id: similarity_score}}\n",
    "    user_movie_similarities = {}\n",
    "    \n",
    "    # Get all user IDs\n",
    "    user_ids = list(user_vectors.keys())\n",
    "    total_users = len(user_ids)\n",
    "    total_movies = len(movie_vectors)\n",
    "    \n",
    "    # Process users in batches\n",
    "    for batch_start in range(0, total_users, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, total_users)\n",
    "        batch_user_ids = user_ids[batch_start:batch_end]\n",
    "        \n",
    "        print(f\"Processing batch {batch_start//batch_size + 1}: users {batch_start+1}-{batch_end} of {total_users}\")\n",
    "        \n",
    "        for user_id in batch_user_ids:\n",
    "            user_vector = user_vectors[user_id]\n",
    "            user_sims = {}\n",
    "            user_similarities = 0\n",
    "            user_above_threshold = 0\n",
    "            \n",
    "            # Calculate similarity for all movies at once (vectorized)\n",
    "            # Convert both user and movie vectors to arrays for faster computation\n",
    "            user_vector_array = np.array(user_vector).reshape(1, -1)\n",
    "            \n",
    "            # Process movies in chunks to avoid memory issues\n",
    "            movie_ids = list(movie_vectors.keys())\n",
    "            movie_chunk_size = 1000  # Adjust based on memory availability\n",
    "            \n",
    "            for movie_chunk_start in range(0, len(movie_ids), movie_chunk_size):\n",
    "                movie_chunk_end = min(movie_chunk_start + movie_chunk_size, len(movie_ids))\n",
    "                chunk_movie_ids = movie_ids[movie_chunk_start:movie_chunk_end]\n",
    "                \n",
    "                # Create array of movie vectors for this chunk\n",
    "                movie_vectors_array = np.array([movie_vectors[mid] for mid in chunk_movie_ids])\n",
    "                \n",
    "                # Calculate cosine similarity in a vectorized way\n",
    "                similarities = np.dot(user_vector_array, movie_vectors_array.T)[0]\n",
    "                \n",
    "                # Filter by threshold and store\n",
    "                for i, sim in enumerate(similarities):\n",
    "                    if sim > threshold:\n",
    "                        movie_id = chunk_movie_ids[i]\n",
    "                        user_sims[movie_id] = float(sim)  # Convert to native Python float\n",
    "                        user_above_threshold += 1\n",
    "                    user_similarities += 1\n",
    "            \n",
    "            user_movie_similarities[user_id] = user_sims\n",
    "            \n",
    "            # Log progress for this user\n",
    "            if len(batch_user_ids) <= 10 or (user_id == batch_user_ids[-1]):\n",
    "                print(f\"User {user_id}: {user_above_threshold}/{total_movies} movies above threshold ({user_above_threshold/total_movies*100:.2f}%)\")\n",
    "        \n",
    "        # Log progress for this batch\n",
    "        elapsed = time.time() - start_time\n",
    "        progress = batch_end / total_users * 100\n",
    "        remaining = (elapsed / (batch_end - batch_start)) * (total_users - batch_end) if batch_end < total_users else 0\n",
    "        print(f\"Processed {batch_end}/{total_users} users ({progress:.1f}%) - Elapsed: {elapsed:.2f}s - Est. remaining: {remaining:.2f}s\")\n",
    "        \n",
    "        # Force garbage collection after each batch\n",
    "        gc.collect()\n",
    "    \n",
    "    avg_above_threshold = sum(len(sims) for sims in user_movie_similarities.values()) / len(user_movie_similarities) if user_movie_similarities else 0\n",
    "    \n",
    "    print(f\"\\nSimilarity calculation complete:\")\n",
    "    print(f\"Total users processed: {len(user_movie_similarities)}\")\n",
    "    print(f\"Total movies per user: {total_movies}\")\n",
    "    print(f\"Average movies above threshold per user: {avg_above_threshold:.2f}\")\n",
    "    \n",
    "    # Display sample user similarities\n",
    "    if user_movie_similarities:\n",
    "        print(\"\\nSample user-movie similarities:\")\n",
    "        for user_id in list(user_movie_similarities.keys())[:3]:\n",
    "            sims = user_movie_similarities[user_id]\n",
    "            print(f\"User ID: {user_id}\")\n",
    "            print(f\"Number of movies above threshold: {len(sims)}\")\n",
    "            if sims:\n",
    "                top_movies = sorted(sims.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "                print(\"Top 5 most similar movies:\")\n",
    "                for movie_id, sim in top_movies:\n",
    "                    movie_title = data['movie_features'][data['movie_features']['movieId'] == movie_id]['title'].values[0] if 'movie_features' in data else f\"Movie {movie_id}\"\n",
    "                    print(f\"  '{movie_title}' (ID: {movie_id}): {sim:.4f}\")\n",
    "            print(\"---\")\n",
    "    \n",
    "    return user_movie_similarities\n",
    "# Calculate similarities if user and movie vectors are available\n",
    "if 'user_vectors' in data and 'movie_vectors' in data:\n",
    "    user_movie_similarities = calculate_user_movie_similarity(\n",
    "        data['user_vectors'], \n",
    "        data['movie_vectors'], \n",
    "        threshold=similarity_threshold,\n",
    "        batch_size=50\n",
    "    )\n",
    "    data['user_movie_similarities'] = user_movie_similarities\n",
    "    \n",
    "    # Save the similarities\n",
    "    with open(os.path.join(output_path, 'user_movie_similarities.pkl'), 'wb') as f:\n",
    "        pickle.dump(user_movie_similarities, f)\n",
    "    \n",
    "    print(f\"Calculated and saved similarities for {len(user_movie_similarities)} users\")\n",
    "    \n",
    "    # Calculate and display similarity statistics\n",
    "    similarity_counts = [len(sims) for sims in user_movie_similarities.values()]\n",
    "    print(f\"\\nSimilarity statistics:\")\n",
    "    print(f\"Average number of similar movies per user: {np.mean(similarity_counts):.2f}\")\n",
    "    print(f\"Min: {min(similarity_counts)}, Max: {max(similarity_counts)}\")\n",
    "    \n",
    "    # Free memory\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 8: RECOMMENDATION GENERATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def get_user_rated_movies(user_id, train_ratings, cached_rated_movies=None):\n",
    "    \"\"\"Get the set of movies already rated by a user with caching for efficiency\"\"\"\n",
    "    # Initialize cache if not provided\n",
    "    if cached_rated_movies is None:\n",
    "        cached_rated_movies = {}\n",
    "        \n",
    "    # Return from cache if available\n",
    "    if user_id in cached_rated_movies:\n",
    "        return cached_rated_movies[user_id]\n",
    "    \n",
    "    # Get from ratings dataframe\n",
    "    if train_ratings is None:\n",
    "        return set()\n",
    "    \n",
    "    user_data = train_ratings[train_ratings['userId'] == user_id]\n",
    "    rated_movies = set(user_data['movieId'].values)\n",
    "    \n",
    "    # Cache for future use\n",
    "    cached_rated_movies[user_id] = rated_movies\n",
    "    \n",
    "    return rated_movies\n",
    "\n",
    "def get_top_n_recommendations(user_id, user_movie_similarities, train_ratings, cached_rated_movies=None, n=10):\n",
    "    \"\"\"\n",
    "    Generate top-N recommendations for a specific user\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    user_id : int\n",
    "        The user ID to generate recommendations for\n",
    "    user_movie_similarities : dict\n",
    "        Dictionary of user-movie similarities\n",
    "    train_ratings : pd.DataFrame\n",
    "        DataFrame of user ratings\n",
    "    cached_rated_movies : dict, optional\n",
    "        Cache of user rated movies for efficiency\n",
    "    n : int, optional\n",
    "        Number of recommendations to generate\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list of tuples\n",
    "        (movie_id, similarity_score) pairs sorted by similarity in descending order\n",
    "    \"\"\"\n",
    "    if user_id not in user_movie_similarities:\n",
    "        return []\n",
    "    \n",
    "    # Get movies already rated by the user (using cache)\n",
    "    rated_movies = get_user_rated_movies(user_id, train_ratings, cached_rated_movies)\n",
    "    \n",
    "    # Get user's similarities\n",
    "    user_sims = user_movie_similarities[user_id]\n",
    "    \n",
    "    # Filter out already rated movies and sort by similarity\n",
    "    candidates = [(movie_id, sim) for movie_id, sim in user_sims.items() \n",
    "                 if movie_id not in rated_movies]\n",
    "    \n",
    "    # Sort by similarity (descending)\n",
    "    recommendations = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top N\n",
    "    return recommendations[:n]\n",
    "\n",
    "def predict_rating(user_id, movie_id, user_movie_similarities, train_ratings):\n",
    "    \"\"\"\n",
    "    Predict a user's rating for a movie\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    user_id : int\n",
    "        The user ID\n",
    "    movie_id : int\n",
    "        The movie ID\n",
    "    user_movie_similarities : dict\n",
    "        Dictionary of user-movie similarities\n",
    "    train_ratings : pd.DataFrame\n",
    "        DataFrame of user ratings\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Predicted rating (0.5-5.0 scale)\n",
    "    \"\"\"\n",
    "    # If user not in similarity matrix, return average rating\n",
    "    if user_id not in user_movie_similarities:\n",
    "        return 3.0\n",
    "    \n",
    "    # Get user's average rating from training data\n",
    "    user_train = train_ratings[train_ratings['userId'] == user_id]\n",
    "    user_avg_rating = user_train['rating'].mean() if len(user_train) > 0 else 3.0\n",
    "    \n",
    "    # If movie not in similarity matrix, return user's average rating\n",
    "    if movie_id not in user_movie_similarities[user_id]:\n",
    "        return user_avg_rating\n",
    "    \n",
    "    # Convert similarity score to rating prediction\n",
    "    # Similarity is in range [0,1], convert to rating range [0.5,5]\n",
    "    sim_score = user_movie_similarities[user_id][movie_id]\n",
    "    predicted_rating = 0.5 + 4.5 * sim_score\n",
    "    \n",
    "    return predicted_rating\n",
    "\n",
    "def generate_recommendations_for_all_users(user_movie_similarities, train_ratings, movie_features, n=10, batch_size=100):\n",
    "    \"\"\"Generate recommendations for all users with memory efficiency in mind\"\"\"\n",
    "    print(f\"Generating top-{n} recommendations for all users in batches...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create a shared cache for rated movies\n",
    "    cached_rated_movies = {}\n",
    "    \n",
    "    # Get all user IDs\n",
    "    user_ids = list(user_movie_similarities.keys())\n",
    "    total_users = len(user_ids)\n",
    "    \n",
    "    all_recommendations = {}\n",
    "    users_with_recommendations = 0\n",
    "    \n",
    "    # Process users in batches\n",
    "    for batch_start in range(0, total_users, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, total_users)\n",
    "        batch_user_ids = user_ids[batch_start:batch_end]\n",
    "        \n",
    "        print(f\"Processing batch {batch_start//batch_size + 1}: users {batch_start+1}-{batch_end} of {total_users}\")\n",
    "        batch_start_time = time.time()\n",
    "        \n",
    "        for user_id in batch_user_ids:\n",
    "            recommendations = get_top_n_recommendations(\n",
    "                user_id, \n",
    "                user_movie_similarities, \n",
    "                train_ratings, \n",
    "                cached_rated_movies,\n",
    "                n\n",
    "            )\n",
    "            \n",
    "            if recommendations:\n",
    "                all_recommendations[user_id] = recommendations\n",
    "                users_with_recommendations += 1\n",
    "        \n",
    "        # Log progress after each batch\n",
    "        batch_time = time.time() - batch_start_time\n",
    "        elapsed = time.time() - start_time\n",
    "        progress = batch_end / total_users * 100\n",
    "        remaining = batch_time * ((total_users - batch_end) / len(batch_user_ids)) if batch_end < total_users else 0\n",
    "        \n",
    "        print(f\"Processed {batch_end}/{total_users} users ({progress:.1f}%) - Elapsed: {elapsed:.2f}s - Est. remaining: {remaining:.2f}s\")\n",
    "        print(f\"Users with recommendations so far: {users_with_recommendations}\")\n",
    "        \n",
    "        # Force garbage collection after each batch\n",
    "        gc.collect()\n",
    "    \n",
    "    # Calculate statistics\n",
    "    if all_recommendations:\n",
    "        total_recommendations = sum(len(recs) for recs in all_recommendations.values())\n",
    "        avg_recommendations = total_recommendations / users_with_recommendations if users_with_recommendations > 0 else 0\n",
    "        \n",
    "        print(f\"\\nRecommendation generation complete:\")\n",
    "        print(f\"Users with recommendations: {users_with_recommendations}/{total_users} ({users_with_recommendations/total_users*100:.1f}%)\")\n",
    "        print(f\"Total recommendations generated: {total_recommendations}\")\n",
    "        print(f\"Average recommendations per user: {avg_recommendations:.2f}\")\n",
    "    \n",
    "    # Display sample recommendations for a few users\n",
    "    if all_recommendations:\n",
    "        print(\"\\nSample recommendations for 3 users:\")\n",
    "        for user_id in list(all_recommendations.keys())[:3]:\n",
    "            print(f\"User ID: {user_id}\")\n",
    "            print(\"Top 5 recommended movies:\")\n",
    "            \n",
    "            for rank, (movie_id, score) in enumerate(all_recommendations[user_id][:5], 1):\n",
    "                movie_title = \"Unknown\"\n",
    "                if movie_features is not None:\n",
    "                    movie_row = movie_features[movie_features['movieId'] == movie_id]\n",
    "                    if not movie_row.empty and 'title' in movie_row.columns:\n",
    "                        movie_title = movie_row.iloc[0]['title']\n",
    "                print(f\"  {rank}. '{movie_title}' (ID: {movie_id}): {score:.4f}\")\n",
    "            print(\"---\")\n",
    "    \n",
    "    return all_recommendations\n",
    "\n",
    "# Generate recommendations if similarities are available\n",
    "if 'user_movie_similarities' in data and 'train_ratings' in data:\n",
    "    all_recommendations = generate_recommendations_for_all_users(\n",
    "        data['user_movie_similarities'], \n",
    "        data['train_ratings'],\n",
    "        data['movie_features'],\n",
    "        n=top_n,\n",
    "        batch_size=100\n",
    "    )\n",
    "    data['all_recommendations'] = all_recommendations\n",
    "    \n",
    "    # Save recommendations\n",
    "    with open(os.path.join(output_path, 'content_based_recommendations.pkl'), 'wb') as f:\n",
    "        pickle.dump(all_recommendations, f)\n",
    "    \n",
    "    # Also save in a more readable CSV format\n",
    "    recommendations_list = []\n",
    "    \n",
    "    # Process in chunks to avoid memory issues\n",
    "    chunk_size = 1000\n",
    "    user_ids = list(all_recommendations.keys())\n",
    "    total_users = len(user_ids)\n",
    "    \n",
    "    for chunk_start in range(0, total_users, chunk_size):\n",
    "        chunk_end = min(chunk_start + chunk_size, total_users)\n",
    "        user_chunk = user_ids[chunk_start:chunk_end]\n",
    "        \n",
    "        chunk_recommendations = []\n",
    "        for user_id in user_chunk:\n",
    "            for rank, (movie_id, score) in enumerate(all_recommendations[user_id], 1):\n",
    "                movie_title = \"Unknown\"\n",
    "                if 'movie_features' in data:\n",
    "                    movie_row = data['movie_features'][data['movie_features']['movieId'] == movie_id]\n",
    "                    if not movie_row.empty and 'title' in movie_row.columns:\n",
    "                        movie_title = movie_row.iloc[0]['title']\n",
    "                        \n",
    "                chunk_recommendations.append({\n",
    "                    'userId': user_id,\n",
    "                    'movieId': movie_id,\n",
    "                    'title': movie_title,\n",
    "                    'rank': rank,\n",
    "                    'similarity_score': score\n",
    "                })\n",
    "        \n",
    "        recommendations_list.extend(chunk_recommendations)\n",
    "        print(f\"Processed recommendation chunk {chunk_start//chunk_size + 1}: users {chunk_start+1}-{chunk_end} of {total_users}\")\n",
    "        gc.collect()\n",
    "    \n",
    "    if recommendations_list:\n",
    "        # Write CSV in chunks to avoid memory issues\n",
    "        chunk_size = 10000\n",
    "        total_recs = len(recommendations_list)\n",
    "        \n",
    "        for chunk_start in range(0, total_recs, chunk_size):\n",
    "            chunk_end = min(chunk_start + chunk_size, total_recs)\n",
    "            chunk = recommendations_list[chunk_start:chunk_end]\n",
    "            \n",
    "            chunk_df = pd.DataFrame(chunk)\n",
    "            \n",
    "            # For first chunk, write with header\n",
    "            if chunk_start == 0:\n",
    "                chunk_df.to_csv(os.path.join(output_path, 'content_based_recommendations.csv'), index=False, mode='w')\n",
    "            else:\n",
    "                # For subsequent chunks, append without header\n",
    "                chunk_df.to_csv(os.path.join(output_path, 'content_based_recommendations.csv'), index=False, mode='a', header=False)\n",
    "            \n",
    "            print(f\"Saved recommendation chunk {chunk_start//chunk_size + 1}: recommendations {chunk_start+1}-{chunk_end} of {total_recs}\")\n",
    "            \n",
    "        print(f\"Saved recommendations to CSV file with {total_recs} entries\")\n",
    "    \n",
    "    # Free memory\n",
    "    del recommendations_list\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 9: MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def evaluate_with_rmse_mae(user_movie_similarities, train_ratings, test_ratings, batch_size=100):\n",
    "    \"\"\"\n",
    "    Evaluate the recommendations using RMSE and MAE with batching for memory efficiency\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    user_movie_similarities : dict\n",
    "        Dictionary of user-movie similarities\n",
    "    train_ratings : pd.DataFrame\n",
    "        DataFrame of training ratings\n",
    "    test_ratings : pd.DataFrame\n",
    "        DataFrame of test ratings\n",
    "    batch_size : int\n",
    "        Size of user batches to process\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    print(\"Evaluating recommendation model using RMSE and MAE with batching...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Users with similarity data\n",
    "    users_with_similarity = set(user_movie_similarities.keys())\n",
    "    test_users = test_ratings['userId'].unique()\n",
    "    users_in_test_with_similarity = set(test_users).intersection(users_with_similarity)\n",
    "    \n",
    "    print(f\"Users in test set with similarity data: {len(users_in_test_with_similarity)}/{len(test_users)} ({len(users_in_test_with_similarity)/len(test_users)*100:.1f}%)\")\n",
    "    \n",
    "    # Track metrics in chunks instead of storing all predictions\n",
    "    squared_errors_sum = 0\n",
    "    absolute_errors_sum = 0\n",
    "    total_predictions = 0\n",
    "    users_evaluated = 0\n",
    "    \n",
    "    # Process users in batches\n",
    "    user_list = list(users_in_test_with_similarity)\n",
    "    for batch_start in range(0, len(user_list), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(user_list))\n",
    "        batch_users = user_list[batch_start:batch_end]\n",
    "        \n",
    "        print(f\"Evaluating batch {batch_start//batch_size + 1}: users {batch_start+1}-{batch_end} of {len(user_list)}\")\n",
    "        batch_start_time = time.time()\n",
    "        \n",
    "        batch_squared_errors = 0\n",
    "        batch_absolute_errors = 0\n",
    "        batch_predictions = 0\n",
    "        \n",
    "        for user_id in batch_users:\n",
    "            # Get user test ratings\n",
    "            user_test = test_ratings[test_ratings['userId'] == user_id]\n",
    "            \n",
    "            if len(user_test) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Get user's average rating from training data\n",
    "            user_train = train_ratings[train_ratings['userId'] == user_id]\n",
    "            user_avg_rating = user_train['rating'].mean() if len(user_train) > 0 else 3.0\n",
    "            \n",
    "            # Predict ratings for test items\n",
    "            for _, row in user_test.iterrows():\n",
    "                movie_id = row['movieId']\n",
    "                true_rating = row['rating']\n",
    "                \n",
    "                # Get similarity-based prediction\n",
    "                if movie_id in user_movie_similarities.get(user_id, {}):\n",
    "                    # Convert similarity score to rating prediction\n",
    "                    sim_score = user_movie_similarities[user_id][movie_id]\n",
    "                    predicted_rating = 0.5 + 4.5 * sim_score\n",
    "                else:\n",
    "                    # Use user's average rating as fallback\n",
    "                    predicted_rating = user_avg_rating\n",
    "                \n",
    "                # Calculate error\n",
    "                squared_error = (predicted_rating - true_rating) ** 2\n",
    "                absolute_error = abs(predicted_rating - true_rating)\n",
    "                \n",
    "                batch_squared_errors += squared_error\n",
    "                batch_absolute_errors += absolute_error\n",
    "                batch_predictions += 1\n",
    "            \n",
    "            users_evaluated += 1\n",
    "        \n",
    "        # Accumulate batch metrics\n",
    "        squared_errors_sum += batch_squared_errors\n",
    "        absolute_errors_sum += batch_absolute_errors\n",
    "        total_predictions += batch_predictions\n",
    "        \n",
    "        # Log progress\n",
    "        batch_time = time.time() - batch_start_time\n",
    "        elapsed = time.time() - start_time\n",
    "        progress = batch_end / len(user_list) * 100\n",
    "        remaining = batch_time * ((len(user_list) - batch_end) / len(batch_users)) if batch_end < len(user_list) else 0\n",
    "        \n",
    "        # Periodically calculate and log intermediate metrics\n",
    "        if batch_predictions > 0:\n",
    "            batch_rmse = np.sqrt(batch_squared_errors / batch_predictions)\n",
    "            batch_mae = batch_absolute_errors / batch_predictions\n",
    "            print(f\"Batch metrics - RMSE: {batch_rmse:.4f}, MAE: {batch_mae:.4f}, Predictions: {batch_predictions}\")\n",
    "        \n",
    "        print(f\"Processed {batch_end}/{len(user_list)} users ({progress:.1f}%) - Elapsed: {elapsed:.2f}s - Est. remaining: {remaining:.2f}s\")\n",
    "        \n",
    "        # Force garbage collection after each batch\n",
    "        gc.collect()\n",
    "    \n",
    "    # Calculate overall RMSE and MAE\n",
    "    if total_predictions > 0:\n",
    "        overall_rmse = np.sqrt(squared_errors_sum / total_predictions)\n",
    "        overall_mae = absolute_errors_sum / total_predictions\n",
    "    else:\n",
    "        overall_rmse = 0.0\n",
    "        overall_mae = 0.0\n",
    "    \n",
    "    print(\"\\nEvaluation results:\")\n",
    "    print(f\"Users evaluated: {users_evaluated}\")\n",
    "    print(f\"Total predictions: {total_predictions}\")\n",
    "    print(f\"RMSE: {overall_rmse:.4f}\")\n",
    "    print(f\"MAE: {overall_mae:.4f}\")\n",
    "    \n",
    "    # Create metrics dictionary\n",
    "    metrics = {\n",
    "        'rmse': overall_rmse,\n",
    "        'mae': overall_mae,\n",
    "        'num_users_evaluated': users_evaluated,\n",
    "        'num_predictions': total_predictions\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "# Evaluate recommendations if test ratings are available\n",
    "if 'user_movie_similarities' in data and 'train_ratings' in data and 'test_ratings' in data:\n",
    "    # Call the new evaluation function\n",
    "    print(\"Running evaluation with RMSE and MAE metrics...\")\n",
    "    evaluation_metrics = evaluate_with_rmse_mae(\n",
    "        data['user_movie_similarities'],\n",
    "        data['train_ratings'],\n",
    "        data['test_ratings'],\n",
    "        batch_size=100\n",
    "    )\n",
    "    \n",
    "    # Store the metrics in the data dictionary\n",
    "    data['evaluation_metrics'] = evaluation_metrics\n",
    "    \n",
    "    # Print the metrics to confirm they're stored correctly\n",
    "    print(\"\\nStored evaluation metrics:\")\n",
    "    for key, value in evaluation_metrics.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    evaluation_results = pd.DataFrame([evaluation_metrics])\n",
    "    evaluation_results.to_csv(os.path.join(output_path, 'content_based_evaluation.csv'), index=False)\n",
    "    \n",
    "    # Also save user metrics\n",
    "    if 'user_metrics' in data:\n",
    "        user_metrics_df = pd.DataFrame.from_dict(data['user_metrics'], orient='index')\n",
    "        user_metrics_df.reset_index(inplace=True)\n",
    "        user_metrics_df.rename(columns={'index': 'userId'}, inplace=True)\n",
    "        user_metrics_df.to_csv(os.path.join(output_path, 'user_metrics.csv'), index=False)\n",
    "    \n",
    "    print(f\"Saved evaluation metrics to CSV files\")\n",
    "    \n",
    "    # Free memory\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY OF CONTENT-BASED RECOMMENDATION SYSTEM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Data information\n",
    "print(\"\\nData Information:\")\n",
    "if 'movie_features' in data:\n",
    "    print(f\"- Processed {len(data['movie_features'])} movie feature records\")\n",
    "if 'corpus_word_counts' in data:\n",
    "    print(f\"- Vocabulary size: {len(data['corpus_word_counts'])} unique words\")\n",
    "if 'movie_vectors' in data:\n",
    "    print(f\"- Generated feature vectors for {len(data['movie_vectors'])} movies\")\n",
    "if 'user_vectors' in data:\n",
    "    print(f\"- Generated feature vectors for {len(data['user_vectors'])} users\")\n",
    "if 'user_movie_similarities' in data:\n",
    "    avg_similar_movies = sum(len(sims) for sims in data['user_movie_similarities'].values()) / len(data['user_movie_similarities'])\n",
    "    print(f\"- Average similar movies per user: {avg_similar_movies:.2f}\")\n",
    "if 'all_recommendations' in data:\n",
    "    avg_recommendations = sum(len(recs) for recs in data['all_recommendations'].values()) / len(data['all_recommendations'])\n",
    "    print(f\"- Average recommendations per user: {avg_recommendations:.2f}\")\n",
    "\n",
    "# Safely display evaluation metrics without assuming specific keys\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "if 'evaluation_metrics' in data:\n",
    "    # Safely check for each expected metric\n",
    "    if 'rmse' in data['evaluation_metrics']:\n",
    "        print(f\"- RMSE: {data['evaluation_metrics']['rmse']:.4f}\")\n",
    "    if 'mae' in data['evaluation_metrics']:\n",
    "        print(f\"- MAE: {data['evaluation_metrics']['mae']:.4f}\")\n",
    "    if 'num_users_evaluated' in data['evaluation_metrics']:\n",
    "        print(f\"- Users evaluated: {data['evaluation_metrics']['num_users_evaluated']}\")\n",
    "    if 'num_predictions' in data['evaluation_metrics']:\n",
    "        print(f\"- Total predictions: {data['evaluation_metrics']['num_predictions']}\")\n",
    "else:\n",
    "    print(\"- No evaluation metrics available\")\n",
    "\n",
    "# Model advantages\n",
    "print(\"\\nAdvantages of this approach:\")\n",
    "print(\"- Log-Likelihood identifies more meaningful words compared to TF-IDF\")\n",
    "print(\"- Word2Vec captures semantic relationships between words\")\n",
    "print(\"- Handles new movies effectively (cold start for items)\")\n",
    "print(\"- Generates personalized recommendations based on content preferences\")\n",
    "print(\"- Doesn't require item-item similarity calculations\")\n",
    "print(\"- Memory-optimized batch processing prevents RAM overflow during long runs\")\n",
    "\n",
    "# Saved files\n",
    "print(\"\\nSaved Files:\")\n",
    "for file in os.listdir(output_path):\n",
    "    file_path = os.path.join(output_path, file)\n",
    "    file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
    "    print(f\"- {file} ({file_size:.2f} MB)\")\n",
    "\n",
    "# Memory usage information\n",
    "import psutil\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_usage = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "print(f\"\\nFinal memory usage: {memory_usage:.2f} MB\")\n",
    "\n",
    "print(\"\\nContent-Based Filtering Model Successfully Implemented!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
